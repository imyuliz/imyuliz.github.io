<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on yulibaozi</title>
		<link>https://yulibaozi.com/posts/</link>
		<description>Recent content in Posts on yulibaozi</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>zh-hans</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Mon, 02 Sep 2019 12:00:00 +0000</lastBuildDate>
		<atom:link href="https://yulibaozi.com/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>用 GODEBUG 看 GC</title>
			<link>https://yulibaozi.com/posts/go/tools/2019-09-02-godebug-gc/</link>
			<pubDate>Mon, 02 Sep 2019 12:00:00 +0000</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/tools/2019-09-02-godebug-gc/</guid>
			<description>什么是 GC 在计算机科学中，垃圾回收（GC）是一种自动管理内存的机制，垃圾回收器会去尝试回收程序不再使用的对象及其占用的内存。而最早 John McCarthy 在 1959 年左右发明了垃圾回收，以简化 Lisp 中的手动内存管理的机制（来自 wikipedia）。
为什么要 GC 手动管理内存挺麻烦，管错或者管漏内存也很糟糕，将会直接导致程序不稳定（持续泄露）甚至直接崩溃。
GC 带来的问题 硬要说会带来什么问题的话，也就数大家最关注的 Stop The World（STW），STW 代指在执行某个垃圾回收算法的某个阶段时，需要将整个应用程序暂停去处理 GC 相关的工作事项。例如：
   行为 会不会 STW 为什么     标记开始 会 在开始标记时，准备根对象的扫描，会打开写屏障（Write Barrier） 和 辅助GC（mutator assist），而回收器和应用程序是并发运行的，因此会暂停当前正在运行的所有 Goroutine。   并发标记中 不会 标记阶段，主要目的是标记堆内存中仍在使用的值。   标记结束 会 在完成标记任务后，将重新扫描部分根对象，这时候会禁用写屏障（Write Barrier）和辅助GC（mutator assist），而标记阶段和应用程序是并发运行的，所以在标记阶段可能会有新的对象产生，因此在重新扫描时需要进行 STW。    如何调整 GC 频率 可以通过 GOGC 变量设置初始垃圾收集器的目标百分比值，对比的规则为当新分配的数值与上一次收集后剩余的实时数值的比例达到设置的目标百分比时，就会触发 GC，默认值为 GOGC=100。如果将其设置为 GOGC=off 可以完全禁用垃圾回收器，要不试试？
简单来讲就是，GOGC 的值设置的越大，GC 的频率越低，但每次最终所触发到 GC 的堆内存也会更大。</description>
			<content type="html"><![CDATA[

<p><img src="https://image.eddycjy.com/b07f55c7fd136392763729b9782f7776.png" alt="image" /></p>

<h2 id="什么是-gc">什么是 GC</h2>

<p>在计算机科学中，垃圾回收（GC）是一种自动管理内存的机制，垃圾回收器会去尝试回收程序不再使用的对象及其占用的内存。而最早 John McCarthy 在 1959 年左右发明了垃圾回收，以简化 Lisp 中的手动内存管理的机制（来自 wikipedia）。</p>

<h2 id="为什么要-gc">为什么要 GC</h2>

<p>手动管理内存挺麻烦，管错或者管漏内存也很糟糕，将会直接导致程序不稳定（持续泄露）甚至直接崩溃。</p>

<h2 id="gc-带来的问题">GC 带来的问题</h2>

<p>硬要说会带来什么问题的话，也就数大家最关注的 Stop The World（STW），STW 代指在执行某个垃圾回收算法的某个阶段时，需要将整个应用程序暂停去处理 GC 相关的工作事项。例如：</p>

<table>
<thead>
<tr>
<th>行为</th>
<th>会不会 STW</th>
<th>为什么</th>
</tr>
</thead>

<tbody>
<tr>
<td>标记开始</td>
<td>会</td>
<td>在开始标记时，准备根对象的扫描，会打开写屏障（Write Barrier） 和 辅助GC（mutator assist），而回收器和应用程序是并发运行的，因此会暂停当前正在运行的所有 Goroutine。</td>
</tr>

<tr>
<td>并发标记中</td>
<td>不会</td>
<td>标记阶段，主要目的是标记堆内存中仍在使用的值。</td>
</tr>

<tr>
<td>标记结束</td>
<td>会</td>
<td>在完成标记任务后，将重新扫描部分根对象，这时候会禁用写屏障（Write Barrier）和辅助GC（mutator assist），而标记阶段和应用程序是并发运行的，所以在标记阶段可能会有新的对象产生，因此在重新扫描时需要进行 STW。</td>
</tr>
</tbody>
</table>

<h2 id="如何调整-gc-频率">如何调整 GC 频率</h2>

<p>可以通过 GOGC 变量设置初始垃圾收集器的目标百分比值，对比的规则为当新分配的数值与上一次收集后剩余的实时数值的比例达到设置的目标百分比时，就会触发 GC，默认值为 GOGC=100。如果将其设置为 GOGC=off 可以完全禁用垃圾回收器，要不试试？</p>

<p>简单来讲就是，GOGC 的值设置的越大，GC 的频率越低，但每次最终所触发到 GC 的堆内存也会更大。</p>

<h2 id="各版本-gc-情况">各版本 GC 情况</h2>

<table>
<thead>
<tr>
<th>版本</th>
<th>GC 算法</th>
<th>STW 时间</th>
</tr>
</thead>

<tbody>
<tr>
<td>Go 1.0</td>
<td>STW（强依赖 tcmalloc）</td>
<td>百ms到秒级别</td>
</tr>

<tr>
<td>Go 1.3</td>
<td>Mark STW, Sweep 并行</td>
<td>百ms级别</td>
</tr>

<tr>
<td>Go 1.5</td>
<td>三色标记法, 并发标记清除。同时运行时从 C 和少量汇编，改为 Go 和少量汇编实现</td>
<td>10-50ms级别</td>
</tr>

<tr>
<td>Go 1.6</td>
<td>1.5 中一些与并发 GC 不协调的地方更改，集中式的 GC 协调协程，改为状态机实现</td>
<td>5ms级别</td>
</tr>

<tr>
<td>Go 1.7</td>
<td>GC 时由 mark 栈收缩改为并发，span 对象分配机制由 freelist 改为 bitmap 模式，SSA引入</td>
<td>ms级别</td>
</tr>

<tr>
<td>Go 1.8</td>
<td>混合写屏障（hybrid write barrier）, 消除 re-scanning stack</td>
<td>sub ms</td>
</tr>

<tr>
<td>Go 1.12</td>
<td>Mark Termination 流程优化</td>
<td>sub ms, 但几乎减少一半</td>
</tr>
</tbody>
</table>

<p>注：资料来源于 @boya 在深圳 Gopher Meetup 的分享。</p>

<h2 id="godebug">GODEBUG</h2>

<p>GODEBUG 变量可以控制运行时内的调试变量，参数以逗号分隔，格式为：<code>name=val</code>。本文着重点在 GC 的观察上，主要涉及 gctrace 参数，我们通过设置 <code>gctrace=1</code> 后就可以使得垃圾收集器向标准错误流发出 GC 运行信息。</p>

<h2 id="涉及术语">涉及术语</h2>

<ul>
<li>mark：标记阶段。</li>
<li>markTermination：标记结束阶段。</li>
<li>mutator assist：辅助 GC，是指在 GC 过程中 mutator 线程会并发运行，而 mutator assist 机制会协助 GC 做一部分的工作。</li>
<li>heap_live：在 Go 的内存管理中，span 是内存页的基本单元，每页大小为 8kb，同时 Go 会根据对象的大小不同而分配不同页数的 span，而 heap_live 就代表着所有 span 的总大小。</li>
<li>dedicated / fractional / idle：在标记阶段会分为三种不同的 mark worker 模式，分别是 dedicated、fractional 和 idle，它们代表着不同的专注程度，其中 dedicated 模式最专注，是完整的 GC 回收行为，fractional 只会干部分的 GC 行为，idle 最轻松。这里你只需要了解它是不同专注程度的 mark worker 就好了，详细介绍我们可以等后续的文章。</li>
</ul>

<h2 id="演示代码">演示代码</h2>

<pre><code>func main() {
    wg := sync.WaitGroup{}
    wg.Add(10)
    for i := 0; i &lt; 10; i++ {
        go func(wg *sync.WaitGroup) {
            var counter int
            for i := 0; i &lt; 1e10; i++ {
                counter++
            }
            wg.Done()
        }(&amp;wg)
    }

    wg.Wait()
}
</code></pre>

<h2 id="gctrace">gctrace</h2>

<pre><code>$ GODEBUG=gctrace=1 go run main.go    
gc 1 @0.032s 0%: 0.019+0.45+0.003 ms clock, 0.076+0.22/0.40/0.80+0.012 ms cpu, 4-&gt;4-&gt;0 MB, 5 MB goal, 4 P
gc 2 @0.046s 0%: 0.004+0.40+0.008 ms clock, 0.017+0.32/0.25/0.81+0.034 ms cpu, 4-&gt;4-&gt;0 MB, 5 MB goal, 4 P
gc 3 @0.063s 0%: 0.004+0.40+0.008 ms clock, 0.018+0.056/0.32/0.64+0.033 ms cpu, 4-&gt;4-&gt;0 MB, 5 MB goal, 4 P
gc 4 @0.080s 0%: 0.004+0.45+0.016 ms clock, 0.018+0.15/0.34/0.77+0.065 ms cpu, 4-&gt;4-&gt;1 MB, 5 MB goal, 4 P
gc 5 @0.095s 0%: 0.015+0.87+0.005 ms clock, 0.061+0.27/0.74/1.8+0.023 ms cpu, 4-&gt;4-&gt;1 MB, 5 MB goal, 4 P
gc 6 @0.113s 0%: 0.014+0.69+0.002 ms clock, 0.056+0.23/0.48/1.4+0.011 ms cpu, 4-&gt;4-&gt;1 MB, 5 MB goal, 4 P
gc 7 @0.140s 1%: 0.031+2.0+0.042 ms clock, 0.12+0.43/1.8/0.049+0.17 ms cpu, 4-&gt;4-&gt;1 MB, 5 MB goal, 4 P
...
</code></pre>

<h3 id="格式">格式</h3>

<pre><code>gc # @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #-&gt;#-&gt;# MB, # MB goal, # P
</code></pre>

<h3 id="含义">含义</h3>

<ul>
<li><code>gc#</code>：GC 执行次数的编号，每次叠加。</li>
<li><code>@#s</code>：自程序启动后到当前的具体秒数。</li>
<li><code>#%</code>：自程序启动以来在GC中花费的时间百分比。</li>
<li><code>#+...+#</code>：GC 的标记工作共使用的 CPU 时间占总 CPU 时间的百分比。</li>
<li><code>#-&gt;#-&gt;# MB</code>：分别表示 GC 启动时, GC 结束时, GC 活动时的堆大小.</li>
<li><code>#MB goal</code>：下一次触发 GC 的内存占用阈值。</li>
<li><code>#P</code>：当前使用的处理器 P 的数量。</li>
</ul>

<h3 id="案例">案例</h3>

<pre><code>gc 7 @0.140s 1%: 0.031+2.0+0.042 ms clock, 0.12+0.43/1.8/0.049+0.17 ms cpu, 4-&gt;4-&gt;1 MB, 5 MB goal, 4 P
</code></pre>

<ul>
<li>gc 7：第 7 次 GC。</li>
<li>@0.140s：当前是程序启动后的 0.140s。</li>
<li>1%：程序启动后到现在共花费 1% 的时间在 GC 上。</li>
<li>0.031+2.0+0.042 ms clock：

<ul>
<li>0.031：表示单个 P 在 mark 阶段的 STW 时间。</li>
<li>2.0：表示所有 P 的 mark concurrent（并发标记）所使用的时间。</li>
<li>0.042：表示单个 P 的 markTermination 阶段的 STW 时间。</li>
</ul></li>
<li>0.12+0.<sup>43</sup>&frasl;<sub>1</sub>.<sup>8</sup>&frasl;<sub>0</sub>.049+0.17 ms cpu：

<ul>
<li>0.12：表示整个进程在 mark 阶段 STW 停顿的时间。</li>
<li>0.<sup>43</sup>&frasl;<sub>1</sub>.<sup>8</sup>&frasl;<sub>0</sub>.049：0.43 表示 mutator assist 占用的时间，1.8 表示 dedicated + fractional 占用的时间，0.049 表示 idle 占用的时间。</li>
<li>0.17ms：0.17 表示整个进程在 markTermination 阶段 STW 时间。</li>
</ul></li>
<li>4-&gt;4-&gt;1 MB：

<ul>
<li>4：表示开始 mark 阶段前的 heap_live 大小。</li>
<li>4：表示开始 markTermination 阶段前的 heap_live 大小。</li>
<li>1：表示被标记对象的大小。</li>
</ul></li>
<li>5 MB goal：表示下一次触发 GC 回收的阈值是 5 MB。</li>
<li>4 P：本次 GC 一共涉及多少个 P。</li>
</ul>

<h2 id="总结">总结</h2>

<p>通过本章节我们掌握了使用 GODEBUG 查看应用程序 GC 运行情况的方法，只要用这种方法我们就可以观测不同情况下 GC 的情况了，甚至可以做出非常直观的对比图，大家不妨尝试一下。</p>

<h2 id="关联文章">关联文章</h2>

<ul>
<li><a href="https://mp.weixin.qq.com/s/Brby6D7d1szUIBjcD_8kfg">用 GODEBUG 看调度跟踪</a></li>
</ul>

<h2 id="参考">参考</h2>

<ul>
<li><a href="https://gocn.vip/question/310">Go GC打印出来的这些信息都是什么含义？</a></li>
<li><a href="http://cbsheng.github.io/posts/godebug%E4%B9%8Bgctrace%E8%A7%A3%E6%9E%90/">GODEBUG之gctrace解析</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/77943973">关于Golang GC的一些误解&ndash;真的比Java GC更领先吗？</a></li>
<li>@boya 深入浅出Golang Runtime PPT</li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>用 GODEBUG 看调度跟踪</title>
			<link>https://yulibaozi.com/posts/go/tools/2019-08-19-godebug-sched/</link>
			<pubDate>Mon, 19 Aug 2019 12:00:00 +0000</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/tools/2019-08-19-godebug-sched/</guid>
			<description>让 Go 更强大的原因之一莫过于它的 GODEBUG 工具，GODEBUG 的设置可以让 Go 程序在运行时输出调试信息，可以根据你的要求很直观的看到你想要的调度器或垃圾回收等详细信息，并且还不需要加装其它的插件，非常方便，今天我们将先讲解 GODEBUG 的调度器相关内容，希望对你有所帮助。
不过在开始前，没接触过的小伙伴得先补补如下前置知识，便于更好的了解调试器输出的信息内容。
前置知识 Go scheduler 的主要功能是针对在处理器上运行的 OS 线程分发可运行的 Goroutine，而我们一提到调度器，就离不开三个经常被提到的缩写，分别是：
 G：Goroutine，实际上我们每次调用 go func 就是生成了一个 G。 P：处理器，一般为处理器的核数，可以通过 GOMAXPROCS 进行修改。 M：OS 线程  这三者交互实际来源于 Go 的 M: N 调度模型，也就是 M 必须与 P 进行绑定，然后不断地在 M 上循环寻找可运行的 G 来执行相应的任务，如果想具体了解可以详细阅读 《Go Runtime Scheduler》，我们抽其中的工作流程图进行简单分析，如下:
 当我们执行 go func() 时，实际上就是创建一个全新的 Goroutine，我们称它为 G。 新创建的 G 会被放入 P 的本地队列（Local Queue）或全局队列（Global Queue）中，准备下一步的动作。 唤醒或创建 M 以便执行 G。 不断地进行事件循环 寻找在可用状态下的 G 进行执行任务 清除后，重新进入事件循环  而在描述中有提到全局和本地这两类队列，其实在功能上来讲都是用于存放正在等待运行的 G，但是不同点在于，本地队列有数量限制，不允许超过 256 个。并且在新建 G 时，会优先选择 P 的本地队列，如果本地队列满了，则将 P 的本地队列的一半的 G 移动到全局队列，这其实可以理解为调度资源的共享和再平衡。</description>
			<content type="html"><![CDATA[

<p><img src="https://image.eddycjy.com/b01c2ce25e34f80d499f0488d034b00b.png" alt="image" /></p>

<p>让 Go 更强大的原因之一莫过于它的 GODEBUG 工具，GODEBUG 的设置可以让 Go 程序在运行时输出调试信息，可以根据你的要求很直观的看到你想要的调度器或垃圾回收等详细信息，并且还不需要加装其它的插件，非常方便，今天我们将先讲解 GODEBUG 的调度器相关内容，希望对你有所帮助。</p>

<p>不过在开始前，没接触过的小伙伴得先补补如下前置知识，便于更好的了解调试器输出的信息内容。</p>

<h2 id="前置知识">前置知识</h2>

<p>Go scheduler 的主要功能是针对在处理器上运行的 OS 线程分发可运行的 Goroutine，而我们一提到调度器，就离不开三个经常被提到的缩写，分别是：</p>

<ul>
<li>G：Goroutine，实际上我们每次调用 <code>go func</code> 就是生成了一个 G。</li>
<li>P：处理器，一般为处理器的核数，可以通过 <code>GOMAXPROCS</code> 进行修改。</li>
<li>M：OS 线程</li>
</ul>

<p>这三者交互实际来源于 Go 的 M: N 调度模型，也就是 M 必须与 P 进行绑定，然后不断地在 M 上循环寻找可运行的 G 来执行相应的任务，如果想具体了解可以详细阅读 <a href="https://speakerdeck.com/retervision/go-runtime-scheduler">《Go Runtime Scheduler》</a>，我们抽其中的工作流程图进行简单分析，如下:</p>

<p><img src="https://image.eddycjy.com/fb4c6c92c93af3bc2dfc4f13dc167cdf.png" alt="image" /></p>

<ol>
<li>当我们执行 <code>go func()</code> 时，实际上就是创建一个全新的 Goroutine，我们称它为 G。</li>
<li>新创建的 G 会被放入 P 的本地队列（Local Queue）或全局队列（Global Queue）中，准备下一步的动作。</li>
<li>唤醒或创建 M 以便执行 G。</li>
<li>不断地进行事件循环</li>
<li>寻找在可用状态下的 G 进行执行任务</li>
<li>清除后，重新进入事件循环</li>
</ol>

<p>而在描述中有提到全局和本地这两类队列，其实在功能上来讲都是用于存放正在等待运行的 G，但是不同点在于，本地队列有数量限制，不允许超过 256 个。并且在新建 G 时，会优先选择 P 的本地队列，如果本地队列满了，则将 P 的本地队列的一半的 G 移动到全局队列，这其实可以理解为调度资源的共享和再平衡。</p>

<p>另外我们可以看到图上有 steal 行为，这是用来做什么的呢，我们都知道当你创建新的 G 或者 G 变成可运行状态时，它会被推送加入到当前 P 的本地队列中。但其实当 P 执行 G 完毕后，它也会 “干活”，它会将其从本地队列中弹出 G，同时会检查当前本地队列是否为空，如果为空会随机的从其他 P 的本地队列中尝试窃取一半可运行的 G 到自己的名下。例子如下：</p>

<p><img src="https://image.eddycjy.com/e7ca8f212466d8c15ec0f60b69a1ce4d.png" alt="image" /></p>

<p>在这个例子中，P2 在本地队列中找不到可以运行的 G，它会执行 <code>work-stealing</code> 调度算法，随机选择其它的处理器 P1，并从 P1 的本地队列中窃取了三个 G 到它自己的本地队列中去。至此，P1、P2 都拥有了可运行的 G，P1 多余的 G 也不会被浪费，调度资源将会更加平均的在多个处理器中流转。</p>

<h2 id="godebug">GODEBUG</h2>

<p>GODEBUG 变量可以控制运行时内的调试变量，参数以逗号分隔，格式为：<code>name=val</code>。本文着重点在调度器观察上，将会使用如下两个参数：</p>

<ul>
<li>schedtrace：设置 <code>schedtrace=X</code> 参数可以使运行时在每 X 毫秒发出一行调度器的摘要信息到标准 err 输出中。</li>
<li>scheddetail：设置 <code>schedtrace=X</code> 和 <code>scheddetail=1</code> 可以使运行时在每 X 毫秒发出一次详细的多行信息，信息内容主要包括调度程序、处理器、OS 线程 和 Goroutine 的状态。</li>
</ul>

<h3 id="演示代码">演示代码</h3>

<pre><code>func main() {
	wg := sync.WaitGroup{}
	wg.Add(10)
	for i := 0; i &lt; 10; i++ {
		go func(wg *sync.WaitGroup) {
			var counter int
			for i := 0; i &lt; 1e10; i++ {
				counter++
			}
			wg.Done()
		}(&amp;wg)
	}

	wg.Wait()
}
</code></pre>

<h3 id="schedtrace">schedtrace</h3>

<pre><code>$ GODEBUG=schedtrace=1000 ./awesomeProject 
SCHED 0ms: gomaxprocs=4 idleprocs=1 threads=5 spinningthreads=1 idlethreads=0 runqueue=0 [0 0 0 0]
SCHED 1000ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=0 [1 2 2 1]
SCHED 2000ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=0 [1 2 2 1]
SCHED 3001ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=0 [1 2 2 1]
SCHED 4010ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=0 [1 2 2 1]
SCHED 5011ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=0 [1 2 2 1]
SCHED 6012ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=0 [1 2 2 1]
SCHED 7021ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=4 [0 1 1 0]
SCHED 8023ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=4 [0 1 1 0]
SCHED 9031ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=4 [0 1 1 0]
SCHED 10033ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=4 [0 1 1 0]
SCHED 11038ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=4 [0 1 1 0]
SCHED 12044ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=4 [0 1 1 0]
SCHED 13051ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=4 [0 1 1 0]
SCHED 14052ms: gomaxprocs=4 idleprocs=2 threads=5 
...
</code></pre>

<ul>
<li>sched：每一行都代表调度器的调试信息，后面提示的毫秒数表示启动到现在的运行时间，输出的时间间隔受 <code>schedtrace</code> 的值影响。</li>
<li>gomaxprocs：当前的 CPU 核心数（GOMAXPROCS 的当前值）。</li>
<li>idleprocs：空闲的处理器数量，后面的数字表示当前的空闲数量。</li>
<li>threads：OS 线程数量，后面的数字表示当前正在运行的线程数量。</li>
<li>spinningthreads：自旋状态的 OS 线程数量。</li>
<li>idlethreads：空闲的线程数量。</li>
<li>runqueue：全局队列中中的 Goroutine 数量，而后面的 [0 0 1 1] 则分别代表这 4 个 P 的本地队列正在运行的 Goroutine 数量。</li>
</ul>

<p>在上面我们有提到 “自旋线程” 这个概念，如果你之前没有了解过相关概念，一听 “自旋” 肯定会比较懵，我们引用 《Head First of Golang Scheduler》 的内容来说明：</p>

<blockquote>
<p>自旋线程的这个说法，是因为 Go Scheduler 的设计者在考虑了 “OS 的资源利用率” 以及 “频繁的线程抢占给 OS 带来的负载” 之后，提出了 “Spinning Thread” 的概念。也就是当 “自旋线程” 没有找到可供其调度执行的 Goroutine 时，并不会销毁该线程 ，而是采取 “自旋” 的操作保存了下来。虽然看起来这是浪费了一些资源，但是考虑一下 syscall 的情景就可以知道，比起 “自旋&rdquo;，线程间频繁的抢占以及频繁的创建和销毁操作可能带来的危害会更大。</p>
</blockquote>

<h3 id="scheddetail">scheddetail</h3>

<p>如果我们想要更详细的看到调度器的完整信息时，我们可以增加 <code>scheddetail</code> 参数，就能够更进一步的查看调度的细节逻辑，如下：</p>

<pre><code>$ GODEBUG=scheddetail=1,schedtrace=1000 ./awesomeProject
SCHED 1000ms: gomaxprocs=4 idleprocs=0 threads=5 spinningthreads=0 idlethreads=0 runqueue=0 gcwaiting=0 nmidlelocked=0 stopwait=0 sysmonwait=0
  P0: status=1 schedtick=2 syscalltick=0 m=3 runqsize=3 gfreecnt=0
  P1: status=1 schedtick=2 syscalltick=0 m=4 runqsize=1 gfreecnt=0
  P2: status=1 schedtick=2 syscalltick=0 m=0 runqsize=1 gfreecnt=0
  P3: status=1 schedtick=1 syscalltick=0 m=2 runqsize=1 gfreecnt=0
  M4: p=1 curg=18 mallocing=0 throwing=0 preemptoff= locks=0 dying=0 spinning=false blocked=false lockedg=-1
  M3: p=0 curg=22 mallocing=0 throwing=0 preemptoff= locks=0 dying=0 spinning=false blocked=false lockedg=-1
  M2: p=3 curg=24 mallocing=0 throwing=0 preemptoff= locks=0 dying=0 spinning=false blocked=false lockedg=-1
  M1: p=-1 curg=-1 mallocing=0 throwing=0 preemptoff= locks=1 dying=0 spinning=false blocked=false lockedg=-1
  M0: p=2 curg=26 mallocing=0 throwing=0 preemptoff= locks=0 dying=0 spinning=false blocked=false lockedg=-1
  G1: status=4(semacquire) m=-1 lockedm=-1
  G2: status=4(force gc (idle)) m=-1 lockedm=-1
  G3: status=4(GC sweep wait) m=-1 lockedm=-1
  G17: status=1() m=-1 lockedm=-1
  G18: status=2() m=4 lockedm=-1
  G19: status=1() m=-1 lockedm=-1
  G20: status=1() m=-1 lockedm=-1
  G21: status=1() m=-1 lockedm=-1
  G22: status=2() m=3 lockedm=-1
  G23: status=1() m=-1 lockedm=-1
  G24: status=2() m=2 lockedm=-1
  G25: status=1() m=-1 lockedm=-1
  G26: status=2() m=0 lockedm=-1
</code></pre>

<p>在这里我们抽取了 1000ms 时的调试信息来查看，信息量比较大，我们先从每一个字段开始了解。如下：</p>

<h4 id="g">G</h4>

<ul>
<li>status：G 的运行状态。</li>
<li>m：隶属哪一个 M。</li>
<li>lockedm：是否有锁定 M。</li>
</ul>

<p>在第一点中我们有提到 G 的运行状态，这对于分析内部流转非常的有用，共涉及如下 9 种状态：</p>

<table>
<thead>
<tr>
<th>状态</th>
<th>值</th>
<th>含义</th>
</tr>
</thead>

<tbody>
<tr>
<td>_Gidle</td>
<td>0</td>
<td>刚刚被分配，还没有进行初始化。</td>
</tr>

<tr>
<td>_Grunnable</td>
<td>1</td>
<td>已经在运行队列中，还没有执行用户代码。</td>
</tr>

<tr>
<td>_Grunning</td>
<td>2</td>
<td>不在运行队列里中，已经可以执行用户代码，此时已经分配了 M 和 P。</td>
</tr>

<tr>
<td>_Gsyscall</td>
<td>3</td>
<td>正在执行系统调用，此时分配了 M。</td>
</tr>

<tr>
<td>_Gwaiting</td>
<td>4</td>
<td>在运行时被阻止，没有执行用户代码，也不在运行队列中，此时它正在某处阻塞等待中。</td>
</tr>

<tr>
<td>_Gmoribund_unused</td>
<td>5</td>
<td>尚未使用，但是在 gdb 中进行了硬编码。</td>
</tr>

<tr>
<td>_Gdead</td>
<td>6</td>
<td>尚未使用，这个状态可能是刚退出或是刚被初始化，此时它并没有执行用户代码，有可能有也有可能没有分配堆栈。</td>
</tr>

<tr>
<td>_Genqueue_unused</td>
<td>7</td>
<td>尚未使用。</td>
</tr>

<tr>
<td>_Gcopystack</td>
<td>8</td>
<td>正在复制堆栈，并没有执行用户代码，也不在运行队列中。</td>
</tr>
</tbody>
</table>

<p>在理解了各类的状态的意思后，我们结合上述案例看看，如下：</p>

<pre><code>G1: status=4(semacquire) m=-1 lockedm=-1
G2: status=4(force gc (idle)) m=-1 lockedm=-1
G3: status=4(GC sweep wait) m=-1 lockedm=-1
G17: status=1() m=-1 lockedm=-1
G18: status=2() m=4 lockedm=-1
</code></pre>

<p>在这个片段中，G1 的运行状态为 <code>_Gwaiting</code>，并没有分配 M 和锁定。这时候你可能好奇在片段中括号里的是什么东西呢，其实是因为该 <code>status=4</code> 是表示 <code>Goroutine</code> 在<strong>运行时时被阻止</strong>，而阻止它的事件就是 <code>semacquire</code> 事件，是因为 <code>semacquire</code> 会检查信号量的情况，在合适的时机就调用 <code>goparkunlock</code> 函数，把当前 <code>Goroutine</code> 放进等待队列，并把它设为 <code>_Gwaiting</code> 状态。</p>

<p>那么在实际运行中还有什么原因会导致这种现象呢，我们一起看看，如下：</p>

<pre><code>	waitReasonZero                                    // &quot;&quot;
	waitReasonGCAssistMarking                         // &quot;GC assist marking&quot;
	waitReasonIOWait                                  // &quot;IO wait&quot;
	waitReasonChanReceiveNilChan                      // &quot;chan receive (nil chan)&quot;
	waitReasonChanSendNilChan                         // &quot;chan send (nil chan)&quot;
	waitReasonDumpingHeap                             // &quot;dumping heap&quot;
	waitReasonGarbageCollection                       // &quot;garbage collection&quot;
	waitReasonGarbageCollectionScan                   // &quot;garbage collection scan&quot;
	waitReasonPanicWait                               // &quot;panicwait&quot;
	waitReasonSelect                                  // &quot;select&quot;
	waitReasonSelectNoCases                           // &quot;select (no cases)&quot;
	waitReasonGCAssistWait                            // &quot;GC assist wait&quot;
	waitReasonGCSweepWait                             // &quot;GC sweep wait&quot;
	waitReasonChanReceive                             // &quot;chan receive&quot;
	waitReasonChanSend                                // &quot;chan send&quot;
	waitReasonFinalizerWait                           // &quot;finalizer wait&quot;
	waitReasonForceGGIdle                             // &quot;force gc (idle)&quot;
	waitReasonSemacquire                              // &quot;semacquire&quot;
	waitReasonSleep                                   // &quot;sleep&quot;
	waitReasonSyncCondWait                            // &quot;sync.Cond.Wait&quot;
	waitReasonTimerGoroutineIdle                      // &quot;timer goroutine (idle)&quot;
	waitReasonTraceReaderBlocked                      // &quot;trace reader (blocked)&quot;
	waitReasonWaitForGCCycle                          // &quot;wait for GC cycle&quot;
	waitReasonGCWorkerIdle                            // &quot;GC worker (idle)&quot;
</code></pre>

<p>我们通过以上 <code>waitReason</code> 可以了解到 <code>Goroutine</code> 会被暂停运行的原因要素，也就是会出现在括号中的事件。</p>

<h4 id="m">M</h4>

<ul>
<li>p：隶属哪一个 P。</li>
<li>curg：当前正在使用哪个 G。</li>
<li>runqsize：运行队列中的 G 数量。</li>
<li>gfreecnt：可用的G（状态为 Gdead）。</li>
<li>mallocing：是否正在分配内存。</li>
<li>throwing：是否抛出异常。</li>
<li>preemptoff：不等于空字符串的话，保持 curg 在这个 m 上运行。</li>
</ul>

<h4 id="p">P</h4>

<ul>
<li>status：P 的运行状态。</li>
<li>schedtick：P 的调度次数。</li>
<li>syscalltick：P 的系统调用次数。</li>
<li>m：隶属哪一个 M。</li>
<li>runqsize：运行队列中的 G 数量。</li>
<li>gfreecnt：可用的G（状态为 Gdead）。</li>
</ul>

<table>
<thead>
<tr>
<th>状态</th>
<th>值</th>
<th>含义</th>
</tr>
</thead>

<tbody>
<tr>
<td>_Pidle</td>
<td>0</td>
<td>刚刚被分配，还没有进行进行初始化。</td>
</tr>

<tr>
<td>_Prunning</td>
<td>1</td>
<td>当 M 与 P 绑定调用 acquirep 时，P 的状态会改变为 _Prunning。</td>
</tr>

<tr>
<td>_Psyscall</td>
<td>2</td>
<td>正在执行系统调用。</td>
</tr>

<tr>
<td>_Pgcstop</td>
<td>3</td>
<td>暂停运行，此时系统正在进行 GC，直至 GC 结束后才会转变到下一个状态阶段。</td>
</tr>

<tr>
<td>_Pdead</td>
<td>4</td>
<td>废弃，不再使用。</td>
</tr>
</tbody>
</table>

<h2 id="总结">总结</h2>

<p>通过本文我们学习到了调度的一些基础知识，再通过神奇的 GODEBUG 掌握了观察调度器的方式方法，你想想，是不是可以和我上一篇文章的 <code>go tool trace</code> 来结合使用呢，在实际的使用中，类似的办法有很多，组合巧用是重点。</p>

<h2 id="参考">参考</h2>

<ul>
<li><a href="https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs">Debugging performance issues in Go programs</a></li>
<li><a href="https://dave.cheney.net/tag/godebug">A whirlwind tour of Go’s runtime environment variables</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=Mzg3MTA0NDQ1OQ==&amp;mid=2247483907&amp;idx=2&amp;sn=c955372683bc0078e14227702ab0a35e&amp;chksm=ce85c607f9f24f116158043f63f7ca11dc88cd519393ba182261f0d7fc328c7b6a94fef4e416&amp;scene=38#wechat_redirect">Go调度器系列（2）宏观看调度器</a></li>
<li><a href="https://rakyll.org/scheduler/">Go&rsquo;s work-stealing scheduler</a></li>
<li><a href="https://www.ardanlabs.com/blog/2015/02/scheduler-tracing-in-go.html">Scheduler Tracing In Go</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/42057783">Head First of Golang Scheduler</a></li>
<li><a href="http://xargin.com/state-of-goroutine/">goroutine 的状态切换</a></li>
<li><a href="https://golang.org/pkg/runtime/#hdr-Environment_Variables">Environment_Variables</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>Go 大杀器之跟踪剖析 trace</title>
			<link>https://yulibaozi.com/posts/go/tools/2019-07-12-go-tool-trace/</link>
			<pubDate>Fri, 12 Jul 2019 12:00:00 +0000</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/tools/2019-07-12-go-tool-trace/</guid>
			<description>在 Go 中有许许多多的分析工具，在之前我有写过一篇 《Golang 大杀器之性能剖析 PProf》 来介绍 PProf，如果有小伙伴感兴趣可以去我博客看看。
但单单使用 PProf 有时候不一定足够完整，因为在真实的程序中还包含许多的隐藏动作，例如 Goroutine 在执行时会做哪些操作？执行/阻塞了多长时间？在什么时候阻止？在哪里被阻止的？谁又锁/解锁了它们？GC 是怎么影响到 Goroutine 的执行的？这些东西用 PProf 是很难分析出来的，但如果你又想知道上述的答案的话，你可以用本文的主角 go tool trace 来打开新世界的大门。目录如下：
初步了解 import ( &amp;#34;os&amp;#34; &amp;#34;runtime/trace&amp;#34; ) func main() { trace.Start(os.Stderr) defer trace.Stop() ch := make(chan string) go func() { ch &amp;lt;- &amp;#34;EDDYCJY&amp;#34; }() &amp;lt;-ch } 生成跟踪文件：
$ go run main.go 2&amp;gt; trace.out  启动可视化界面：
$ go tool trace trace.out 2019/06/22 16:14:52 Parsing trace... 2019/06/22 16:14:52 Splitting trace... 2019/06/22 16:14:52 Opening browser.</description>
			<content type="html"><![CDATA[

<p><img src="https://s2.ax1x.com/2020/02/15/1x1phF.png" alt="image" /></p>

<p>在 Go 中有许许多多的分析工具，在之前我有写过一篇 《Golang 大杀器之性能剖析 PProf》 来介绍 PProf，如果有小伙伴感兴趣可以去我博客看看。</p>

<p>但单单使用 PProf 有时候不一定足够完整，因为在真实的程序中还包含许多的隐藏动作，例如 Goroutine 在执行时会做哪些操作？执行/阻塞了多长时间？在什么时候阻止？在哪里被阻止的？谁又锁/解锁了它们？GC 是怎么影响到 Goroutine 的执行的？这些东西用 PProf 是很难分析出来的，但如果你又想知道上述的答案的话，你可以用本文的主角 <code>go tool trace</code> 来打开新世界的大门。目录如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1P1J.png" alt="image" /></p>

<h2 id="初步了解">初步了解</h2>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kn">import</span> <span class="p">(</span>
	<span class="s">&#34;os&#34;</span>
	<span class="s">&#34;runtime/trace&#34;</span>
<span class="p">)</span>

<span class="kd">func</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
	<span class="nx">trace</span><span class="p">.</span><span class="nf">Start</span><span class="p">(</span><span class="nx">os</span><span class="p">.</span><span class="nx">Stderr</span><span class="p">)</span>
	<span class="k">defer</span> <span class="nx">trace</span><span class="p">.</span><span class="nf">Stop</span><span class="p">()</span>

	<span class="nx">ch</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">(</span><span class="kd">chan</span> <span class="kt">string</span><span class="p">)</span>
	<span class="k">go</span> <span class="kd">func</span><span class="p">()</span> <span class="p">{</span>
		<span class="nx">ch</span> <span class="o">&lt;-</span> <span class="s">&#34;EDDYCJY&#34;</span>
	<span class="p">}()</span>

	<span class="o">&lt;-</span><span class="nx">ch</span>
<span class="p">}</span></code></pre></div>
<p>生成跟踪文件：</p>

<pre><code>$ go run main.go 2&gt; trace.out
</code></pre>

<p>启动可视化界面：</p>

<pre><code>$ go tool trace trace.out
2019/06/22 16:14:52 Parsing trace...
2019/06/22 16:14:52 Splitting trace...
2019/06/22 16:14:52 Opening browser. Trace viewer is listening on http://127.0.0.1:57321
</code></pre>

<p>查看可视化界面：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1FXR.png" alt="image" /></p>

<ul>
<li>View trace：查看跟踪</li>
<li>Goroutine analysis：Goroutine 分析</li>
<li>Network blocking profile：网络阻塞概况</li>
<li>Synchronization blocking profile：同步阻塞概况</li>
<li>Syscall blocking profile：系统调用阻塞概况</li>
<li>Scheduler latency profile：调度延迟概况</li>
<li>User defined tasks：用户自定义任务</li>
<li>User defined regions：用户自定义区域</li>
<li>Minimum mutator utilization：最低 Mutator 利用率</li>
</ul>

<h3 id="scheduler-latency-profile">Scheduler latency profile</h3>

<p>在刚开始查看问题时，除非是很明显的现象，否则不应该一开始就陷入细节，因此我们一般先查看 “Scheduler latency profile”，我们能通过 Graph 看到整体的调用开销情况，如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1K9e.png" alt="image" /></p>

<p>演示程序比较简单，因此这里就两块，一个是 <code>trace</code> 本身，另外一个是 <code>channel</code> 的收发。</p>

<h3 id="goroutine-analysis">Goroutine analysis</h3>

<p>第二步看 “Goroutine analysis”，我们能通过这个功能看到整个运行过程中，每个函数块有多少个有 Goroutine 在跑，并且观察每个的 Goroutine 的运行开销都花费在哪个阶段。如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1ljA.png" alt="image" /></p>

<p>通过上图我们可以看到共有 3 个 goroutine，分别是 <code>runtime.main</code>、<code>runtime/trace.Start.func1</code>、<code>main.main.func1</code>，那么它都做了些什么事呢，接下来我们可以通过点击具体细项去观察。如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x18Bt.jpg" alt="image" /></p>

<p>同时也可以看到当前 Goroutine 在整个调用耗时中的占比，以及 GC 清扫和 GC 暂停等待的一些开销。如果你觉得还不够，可以把图表下载下来分析，相当于把整个 Goroutine 运行时掰开来看了，这块能够很好的帮助我们<strong>对 Goroutine 运行阶段做一个的剖析，可以得知到底慢哪，然后再决定下一步的排查方向</strong>。如下：</p>

<table>
<thead>
<tr>
<th>名称</th>
<th>含义</th>
<th>耗时</th>
</tr>
</thead>

<tbody>
<tr>
<td>Execution Time</td>
<td>执行时间</td>
<td>3140ns</td>
</tr>

<tr>
<td>Network Wait Time</td>
<td>网络等待时间</td>
<td>0ns</td>
</tr>

<tr>
<td>Sync Block Time</td>
<td>同步阻塞时间</td>
<td>0ns</td>
</tr>

<tr>
<td>Blocking Syscall Time</td>
<td>调用阻塞时间</td>
<td>0ns</td>
</tr>

<tr>
<td>Scheduler Wait Time</td>
<td>调度等待时间</td>
<td>14ns</td>
</tr>

<tr>
<td>GC Sweeping</td>
<td>GC 清扫</td>
<td>0ns</td>
</tr>

<tr>
<td>GC Pause</td>
<td>GC 暂停</td>
<td>0ns</td>
</tr>
</tbody>
</table>

<h3 id="view-trace">View trace</h3>

<p>在对当前程序的 Goroutine 运行分布有了初步了解后，我们再通过 “查看跟踪” 看看之间的关联性，如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1GHP.png" alt="image" /></p>

<p>这个跟踪图粗略一看，相信有的小伙伴会比较懵逼，我们可以依据注解一块块查看，如下：</p>

<ol>
<li>时间线：显示执行的时间单元，根据时间维度的不同可以调整区间，具体可执行 <code>shift</code> + <code>?</code> 查看帮助手册。</li>
<li>堆：显示执行期间的内存分配和释放情况。</li>
<li>协程：显示在执行期间的每个 Goroutine 运行阶段有多少个协程在运行，其包含 GC 等待（GCWaiting）、可运行（Runnable）、运行中（Running）这三种状态。</li>
<li>OS 线程：显示在执行期间有多少个线程在运行，其包含正在调用 Syscall（InSyscall）、运行中（Running）这两种状态。</li>
<li>虚拟处理器：每个虚拟处理器显示一行，虚拟处理器的数量一般默认为系统内核数。</li>
<li>协程和事件：显示在每个虚拟处理器上有什么 Goroutine 正在运行，而连线行为代表事件关联。</li>
</ol>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1YAf.jpg" alt="image" /></p>

<p>点击具体的 Goroutine 行为后可以看到其相关联的详细信息，这块很简单，大家实际操作一下就懂了。文字解释如下：</p>

<ul>
<li>Start：开始时间</li>
<li>Wall Duration：持续时间</li>
<li>Self Time：执行时间</li>
<li>Start Stack Trace：开始时的堆栈信息</li>
<li>End Stack Trace：结束时的堆栈信息</li>
<li>Incoming flow：输入流</li>
<li>Outgoing flow：输出流</li>
<li>Preceding events：之前的事件</li>
<li>Following events：之后的事件</li>
<li>All connected：所有连接的事件</li>
</ul>

<h3 id="view-events">View Events</h3>

<p>我们可以通过点击 View Options-Flow events、Following events 等方式，查看我们应用运行中的事件流情况。如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1d3Q.png" alt="image" /></p>

<p>通过分析图上的事件流，我们可得知这程序从 <code>G1 runtime.main</code> 开始运行，在运行时创建了 2 个 Goroutine，先是创建 <code>G18 runtime/trace.Start.func1</code>，然后再是 <code>G19 main.main.func1</code> 。而同时我们可以通过其 Goroutine Name 去了解它的调用类型，如：<code>runtime/trace.Start.func1</code> 就是程序中在 <code>main.main</code> 调用了 <code>runtime/trace.Start</code> 方法，然后该方法又利用协程创建了一个闭包 <code>func1</code> 去进行调用。</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1Dun.png" alt="image" /></p>

<p>在这里我们结合开头的代码去看的话，很明显就是 <code>ch</code> 的输入输出的过程了。</p>

<h2 id="结合实战">结合实战</h2>

<p>今天生产环境突然出现了问题，机智的你早已埋好 <code>_ &quot;net/http/pprof&quot;</code> 这个神奇的工具，你麻利的执行了如下命令：</p>

<ul>
<li>curl <a href="http://127.0.0.1:6060/debug/pprof/trace?seconds=20">http://127.0.0.1:6060/debug/pprof/trace?seconds=20</a> &gt; trace.out</li>
<li>go tool trace trace.out</li>
</ul>

<h3 id="view-trace-1">View trace</h3>

<p>你很快的看到了熟悉的 List 界面，然后不信邪点开了 View trace 界面，如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1cNT.jpg" alt="image" /></p>

<p>完全看懵的你，稳住，对着合适的区域执行快捷键 <code>W</code> 不断地放大时间线，如下：</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1x1ID1.jpg" alt="image" /></p>

<p>经过初步排查，你发现上述绝大部分的 G 竟然都和 <code>google.golang.org/grpc.(*Server).Serve.func</code> 有关，关联的一大串也是 <code>Serve</code> 所触发的相关动作。</p>

<p><img src="https://s2.ax1x.com/2020/02/16/3pNw9I.jpg" alt="image" /></p>

<p>这时候有经验的你心里已经有了初步结论，你可以继续追踪 View trace 深入进去，不过我建议先鸟瞰全貌，因此我们再往下看 “Network blocking profile” 和 “Syscall blocking profile” 所提供的信息，如下：</p>

<h3 id="network-blocking-profile">Network blocking profile</h3>

<p><img src="https://s2.ax1x.com/2020/02/16/3pNfCn.jpg" alt="image" /></p>

<h3 id="syscall-blocking-profile">Syscall blocking profile</h3>

<p><img src="https://s2.ax1x.com/2020/02/16/3pN7bF.jpg" alt="image" /></p>

<p>通过对以上三项的跟踪分析，加上这个泄露，这个阻塞的耗时，这个涉及的内部方法名，很明显就是哪位又忘记关闭客户端连接了，赶紧改改改。</p>

<h2 id="总结">总结</h2>

<p>通过本文我们习得了 <code>go tool trace</code> 的武林秘籍，它能够跟踪捕获各种执行中的事件，例如 Goroutine 的创建/阻塞/解除阻塞，Syscall 的进入/退出/阻止，GC 事件，Heap 的大小改变，Processor 启动/停止等等。</p>

<p>希望你能够用好 Go 的两大杀器 pprof + trace 组合，此乃排查好搭档，谁用谁清楚，即使他并不万能。</p>

<h2 id="参考">参考</h2>

<ul>
<li><a href="https://about.sourcegraph.com/go/an-introduction-to-go-tool-trace-rhys-hiltner">https://about.sourcegraph.com/go/an-introduction-to-go-tool-trace-rhys-hiltner</a></li>
<li><a href="https://www.itcodemonkey.com/article/5419.html">https://www.itcodemonkey.com/article/5419.html</a></li>
<li><a href="https://making.pusher.com/go-tool-trace/">https://making.pusher.com/go-tool-trace/</a></li>
<li><a href="https://golang.org/cmd/trace/">https://golang.org/cmd/trace/</a></li>
<li><a href="https://docs.google.com/document/d/1FP5apqzBgr7ahCCgFO-yoVhk4YZrNIDNf9RybngBc14/pub">https://docs.google.com/document/d/1FP5apqzBgr7ahCCgFO-yoVhk4YZrNIDNf9RybngBc14/pub</a></li>
<li><a href="https://godoc.org/runtime/trace">https://godoc.org/runtime/trace</a></li>
</ul>
]]></content>
		</item>
		
		<item>
			<title>[译]使用 Kubebuilder 开发 Operator</title>
			<link>https://yulibaozi.com/posts/kubernetes/2019-06-11-use-kubebuilder-to-operator/</link>
			<pubDate>Tue, 11 Jun 2019 01:10:50 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/2019-06-11-use-kubebuilder-to-operator/</guid>
			<description>写给那些人 原文地址
写给那些Kubernetes用户 Kubernetes用户将通过学习API的设计和实现背后的基本概念，更深入的理解Kubernetes。本书将教会读者怎样开发自己的Kubernetes APIs 和学习Kubernetes API核心的设计原则。
包括:
 Kubernetes API和resource的结构设计 API 版本语义 自托管/自愈 垃圾回收和终结器 声明式和命令式APIs 基于级别和基于边缘的APIs 资源和子资源  Kubernetes API 扩展开发 API 扩展开发者将学习到实现Kubernetes API背后的原理，概念和实现规范，以及用户快速开发的简单工具和库。 这本书涵盖了开发人员遇到的陷阱和思维上的误区。
包括:
 如果将多个事件批处理为单次调用对比,(翻译疑问) 如何配置定期对比 即将有的 如何使用缓存和实时查找 垃圾收集和终结器 如何使用声明与Webhook验证 如何实现API版本控制  快速开始 安装 安装Kubebuilder:
os=$(go env GOOS) arch=$(go env GOARCH) # download kubebuilder and extract it to tmp curl -sL https://go.kubebuilder.io/dl/2.0.0-alpha.2/${os}/${arch} | tar -xz -C /tmp/ # move to a long-term location and put it on your path # (you&#39;ll need to set the KUBEBUILDER_ASSETS env var if you put it somewhere else) sudo mv /tmp/kubebuilder_2.</description>
			<content type="html"><![CDATA[

<h2 id="写给那些人">写给那些人</h2>

<p><a href="https://book.kubebuilder.io/introduction.html">原文地址</a></p>

<h4 id="写给那些kubernetes用户">写给那些Kubernetes用户</h4>

<p>Kubernetes用户将通过学习API的设计和实现背后的基本概念，更深入的理解Kubernetes。本书将教会读者怎样开发自己的Kubernetes APIs 和学习Kubernetes API核心的设计原则。</p>

<p>包括:</p>

<ul>
<li>Kubernetes API和resource的结构设计</li>
<li>API 版本语义</li>
<li>自托管/自愈</li>
<li>垃圾回收和终结器</li>
<li>声明式和命令式APIs</li>
<li>基于级别和基于边缘的APIs</li>
<li>资源和子资源</li>
</ul>

<h4 id="kubernetes-api-扩展开发">Kubernetes API 扩展开发</h4>

<p>API 扩展开发者将学习到实现Kubernetes API背后的原理，概念和实现规范，以及用户快速开发的简单工具和库。 这本书涵盖了开发人员遇到的陷阱和思维上的误区。</p>

<p>包括:</p>

<ul>
<li>如果将多个事件批处理为单次调用对比,(翻译疑问)</li>
<li>如何配置定期对比</li>
<li>即将有的</li>
<li>如何使用缓存和实时查找</li>
<li>垃圾收集和终结器</li>
<li>如何使用声明与Webhook验证</li>
<li>如何实现API版本控制</li>
</ul>

<h2 id="快速开始">快速开始</h2>

<h4 id="安装">安装</h4>

<p>安装Kubebuilder:</p>

<pre><code>os=$(go env GOOS)
arch=$(go env GOARCH)

# download kubebuilder and extract it to tmp
curl -sL https://go.kubebuilder.io/dl/2.0.0-alpha.2/${os}/${arch} | tar -xz -C /tmp/

# move to a long-term location and put it on your path
# (you'll need to set the KUBEBUILDER_ASSETS env var if you put it somewhere else)
sudo mv /tmp/kubebuilder_2.0.0-alpha.2_${os}_${arch} /usr/local/kubebuilder
export PATH=$PATH:/usr/local/kubebuilder/bin
</code></pre>

<h4 id="创建一个项目">创建一个项目</h4>

<p>初始化一个新项目</p>

<pre><code>kubebuilder init --domain my.domain
</code></pre>

<p>如果你没有在<code>GOPATH</code>下，你可以运行<code>go mod init &lt;modulename&gt;</code>来告诉kubebuilder和Go来基于你的module来导入包/路径。</p>

<h4 id="增加一个api">增加一个API</h4>

<p>创建一个名为<code>webapp/v1</code>的新API组版本,以及该版本有类型为<code>Guestbook</code>的Kind。</p>

<pre><code>kubebuilder create api --group webapp --version v1 --kind Guestbook
</code></pre>

<p>执行上面的命令将会创建文件<code>api/v1/guestbook_types.go</code> 和 <code>controller/guestbook_controller.go</code></p>

<p><strong><em>可选:</em></strong> 编辑API定义或业务逻辑,更多详情查看什么是控制器 什么是资源。</p>

<h4 id="本地测试">本地测试</h4>

<p>首先，你需要有个kuberentes 集群来运行, 您可以使用KIND获取本地群集进行测试，或者针对远程群集运行。</p>

<p>你的控制器会自动使用你的kubeconfig文件中的当前上下文(查看集群上下文:<code>kubectl cluster-info</code>显示)</p>

<p>安装CRDs 到集群:</p>

<pre><code>make install
</code></pre>

<p>运行你的控制器(前台运行,你可以切换到另外一个终端来操作)：</p>

<pre><code>make run
</code></pre>

<h4 id="安装示例">安装示例</h4>

<p>如果你修改过你的配置，你需要重新发布:</p>

<pre><code>kubectl apply -f config/samples/
</code></pre>

<h4 id="在集群上运行">在集群上运行</h4>

<p>打包和推送你的镜像到镜像库:</p>

<pre><code>make docker-build docker-push IMG=&lt;some-registry&gt;/controller
</code></pre>

<p>部署你的控制器到集群:</p>

<pre><code>make deploy
</code></pre>

<p>如遇到RBAC错误,你需要给控制器授权,最简单的方式就是使用管理员权限.</p>

<h2 id="教程-构建一个cronjob">教程: 构建一个CronJob</h2>

<p>太多的教程是从一些非常人为的设定开始的,或者写一些玩具级的应用程序，从而得到一些基础知识,而停止了更深入的探索,相反的是, 这个教程会带你通过kubebuilder完成所有(几乎)复杂的工作, 从简入繁构建一个功能非常全面的东西.</p>

<p>来，我们假设(小的场景), 我们终于受够了kubernetes中非kubebuilder实现的CronJob控制器的维护负担,你非常的想使用kubebuilder重写它(嗯~,这种假设有点意思)。</p>

<p>CronJob控制的job定期在kubernetes集群运行一次性任务,它需要通过Job控制器进行构建,任务是一次性任务，直到它们的任务完成。</p>

<p>我们不会试图重写job控制器,而是作为一个支点来了解如何与外部类型进行交互.</p>

<h4 id="构建我们的项目">构建我们的项目</h4>

<p>如快速入门那样,我们需要构建一个新项目。确保你已经安装了<code>Kubebuilder</code>,然后创建了一个新项目:</p>

<pre><code># we'll use a domain of tutorial.kubebuilder.io,
# so all API groups will be &lt;group&gt;.tutorial.kubebuilder.io.
kubebuilder init --domain tutorial.kubebuilder.io
</code></pre>

<p>到目前为止,我们已经有个一个项目，来看看kubebuilder搭建的架子&hellip;</p>

<h2 id="什么是基础项目">什么是基础项目?</h2>

<p>当我们构建了一个新项目,kubebuilder为我们提供了一些基本的样本文件。</p>

<h4 id="构建结构">构建结构</h4>

<p>首先, 构建项目的基础架构:</p>

<ul>
<li><p><code>go.mod</code>  : go 包依赖</p>

<pre><code>module tutorial.kubebuilder.io/project

go 1.12

require (
github.com/go-logr/logr v0.1.0
github.com/robfig/cron v1.1.0
k8s.io/api v0.0.0-20190222213804-5cb15d344471
k8s.io/apimachinery v0.0.0-20190221213512-86fb29eff628
k8s.io/client-go v0.0.0-20190228174230-b40b2a5939e4
sigs.k8s.io/controller-runtime v0.2.0-alpha.0.0.20190503051552-b666157c41da
sigs.k8s.io/controller-tools v0.2.0-alpha.1 // indirect
)

</code></pre></li>

<li><p><code>Makefile</code>: 构建和部署你的控制器</p>

<pre><code># Image URL to use all building/pushing image targets
IMG ?= controller:latest
# Produce CRDs that work back to Kubernetes 1.11 (no version conversion)
CRD_OPTIONS ?= &quot;crd:trivialVersions=true&quot;

all: manager

# Run tests
test: generate fmt vet manifests
go test ./api/... ./controllers/... -coverprofile cover.out

# Build manager binary
manager: generate fmt vet
go build -o bin/manager main.go

# Run against the configured Kubernetes cluster in ~/.kube/config
run: generate fmt vet
go run ./main.go

# Install CRDs into a cluster
install: manifests
kubectl apply -f config/crd/bases

# Deploy controller in the configured Kubernetes cluster in ~/.kube/config
deploy: manifests
kubectl apply -f config/crd/bases
kustomize build config/default | kubectl apply -f -

# Generate manifests e.g. CRD, RBAC etc.
manifests: controller-gen
$(CONTROLLER_GEN) $(CRD_OPTIONS) rbac:roleName=manager-role webhook paths=&quot;./api/...;./controllers/...&quot; output:crd:artifacts:config=config/crd/bases

# Run go fmt against code
fmt:
go fmt ./...

# Run go vet against code
vet:
go vet ./...

# Generate code
generate: controller-gen
$(CONTROLLER_GEN) object:headerFile=./hack/boilerplate.go.txt paths=./api/...

# Build the docker image
docker-build: test
docker build . -t ${IMG}
@echo &quot;updating kustomize image patch file for manager resource&quot;
sed -i'' -e 's@image: .*@image: '&quot;${IMG}&quot;'@' ./config/default/manager_image_patch.yaml

# Push the docker image
docker-push:
docker push ${IMG}

# find or download controller-gen
# download controller-gen if necessary
controller-gen:
ifeq (, $(shell which controller-gen))
go get sigs.k8s.io/controller-tools/cmd/controller-gen@v0.2.0-alpha.1
CONTROLLER_GEN=$(shell go env GOPATH)/bin/controller-gen
else
CONTROLLER_GEN=$(shell which controller-gen)
endif

</code></pre></li>

<li><p>PROJECT: 用于搭建新组件的Kubebuilder元数据</p>

<pre><code>version: &quot;2&quot;
domain: tutorial.kubebuilder.io
repo: tutorial.kubebuilder.io/project
</code></pre></li>
</ul>

<h4 id="启动配置">启动配置</h4>

<p>我们还在<code>config/</code>目录下获得启动配置. 现在,他只是包含在集群上启动控制器所需的kustomize YAML定义,但是,一旦我们开始编写控制器,它将保留我们的CustomResourceDefinitions，RBAC配置和WebhookConfigurations。</p>

<h4 id="入口点">入口点</h4>

<p>最后,同样重要的是,Kubebuilder 支持我们项目的基本如何点:main.go,接着向下看</p>

<h2 id="每个旅程都需要一个开始-每个项目都需要main">每个旅程都需要一个开始，每个项目都需要main</h2>

<ul>
<li>Apache License</li>
</ul>

<p>根据Apache许可证2.0版（“许可证”）获得许可;除非符合许可，否则您不得使用此文件。您可以在以下位置获取许可证副本</p>

<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>

<p>除非适用法律要求或书面同意，否则根据许可证分发的软件将按“原样”分发，不附带任何明示或暗示的担保或条件。有关管理许可下的权限和限制的特定语言，请参阅许可证。</p>

<p>我们的包从基本的进口开始。尤其：</p>

<ul>
<li>核心控制器 - 运行时库</li>

<li><p>默认的控制器 - 运行时日志记录，Zap（稍后会详细介绍）</p>

<pre><code>package main

import (
&quot;flag&quot;
&quot;os&quot;

&quot;k8s.io/apimachinery/pkg/runtime&quot;
_ &quot;k8s.io/client-go/plugin/pkg/client/auth/gcp&quot;
ctrl &quot;sigs.k8s.io/controller-runtime&quot;
&quot;sigs.k8s.io/controller-runtime/pkg/log/zap&quot;
// +kubebuilder:scaffold:imports
)
</code></pre></li>
</ul>

<p>每组控制器都需要一个Scheme，它提供了Kinds与其相应Go类型之间的映射。当我们编写API定义时，我们会更多地了解Kinds，所以请稍后记住这一点。</p>

<pre><code>var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName(&quot;setup&quot;)
)

func init() {

    // +kubebuilder:scaffold:scheme
}
</code></pre>

<p>在这一点上，我们的主要功能相当简单：
* 我们设置了一些基本标志(注释中的// +xxx)。
* 我们实例化一个管理器，它追踪我们所有控制器的运行，以及为API服务器设置共享缓存和客户端（注意:我们需要告诉管理员我们的Scheme）。
* 我们运行管理器,管理器又运行我们的控制器和webhooks，管理器会一直运行直到收到了终止信号, 这样,当我们在kubernetes上运行的时候,我们能够有优秀的表现,优雅的pod终止。</p>

<p>虽然我们还没有任何东西可以运行,但我们可以先记住<code>+kubebuilder:scaffold:builder</code>，很快就会发生有趣的事情了。</p>

<pre><code>func main() {
    var metricsAddr string
    flag.StringVar(&amp;metricsAddr, &quot;metrics-addr&quot;, &quot;:8080&quot;, &quot;The address the metric endpoint binds to.&quot;)
    flag.Parse()

    ctrl.SetLogger(zap.Logger(true))

    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
    if err != nil {
        setupLog.Error(err, &quot;unable to start manager&quot;)
        os.Exit(1)
    }

    // +kubebuilder:scaffold:builder

    setupLog.Info(&quot;starting manager&quot;)
    if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
        setupLog.Error(err, &quot;problem running manager&quot;)
        os.Exit(1)
    }
}
</code></pre>

<p>有了这个，我们可以继续使用我们的API脚手架！</p>

<h2 id="组-版本-种类和资源-groups-versions-和-kinds">组，版本，种类和资源 (Groups Versions 和 Kinds)</h2>

<p>实际上，在我们开始使用API之前，我们应该先谈谈术语。当我们在Kubernetes中讨论API时，我们经常使用4个术语：组，版本，种类和资源(<em>groups, versions, kinds, and resources</em>)。</p>

<h4 id="groups-和-versions">Groups 和 Versions</h4>

<p><em>Group</em> 只是相关功能的集合,每个group 都有一个或多个<em>versions</em>, 顾名思义,它允许我们随着时间的迁移,能够兼容性的变更API的工作方式。</p>

<h4 id="kinds-和-resources">Kinds 和 Resources</h4>

<p>每个 API group-version 包含了一个或多个API类型, 我们称之为<em>Kinds</em>. 虽然Kind可以在版本之间变更, 但是每个配置必须能够以某种方式存储其他配置的所有数据(我们可以将数据存储在字段中或者注释中)。这意味着使用较旧的API版本(version) 不会导致新的数据丢失或者损坏。
更多信息可以查看：KubernetesAPI指南。</p>

<p>你还会听到一些<em>resources</em>,这个resource 只是在API中使用的Kind. 通常,Kinds和resources之间是一对一的映射关系,比如,pods 资源对应<code>Pod</code> Kind。但是有时候,多个资源可能会返回相同的Kind. 例如,<code>Scale</code> Kind 由所有的scale子资源返回,比如:<code>deployments/scale</code> 或者 <code>replicasets/scale</code>, 这使得Kubernetes HorizontalPodAutoscaler可以与不同的资源进行交互,但是,对于CRD, 每个类型(Kind)对应于单个资源(resource).</p>

<p>那么，我们在Go中 上面的类型是如何对应的呢?</p>

<p>当我们引用特定 group-version的时候, 我们称之为 <em>GroupVersionKind</em>,或者简写为GVK. 和resource和GVR相同,稍后会看到 每个GVK对应于包中给定的 根(root) Go 类型</p>

<h4 id="额-scheme是个什么东东">额，scheme是个什么东东?</h4>

<p>我们之前看到的只是Go类型和GVK如何对应的方法。</p>

<p>例如:我们假设将<code>&quot;tutorial.kubebuilder.io/api/v1&quot;.CronJob{}</code> 类型标记为<code>batch.tutorial.kubebuilder.io/v1</code> API group(隐含地说,他是类型为CronJob的Kind).</p>

<p>然后, 我们稍后构建一个新的&amp;CronJob{},通过服务器提供的json</p>

<pre><code>{
    &quot;kind&quot;: &quot;CronJob&quot;,
    &quot;apiVersion&quot;: &quot;batch.tutorial.kubebuilder.io/v1&quot;,
    ...
}

</code></pre>

<p>当我们更新CronJob的时候，确认提交了正确的group-version</p>

<h2 id="添加一个新的api">添加一个新的API</h2>

<p>为了能够创建一个新的Kind和一个对应的控制器,我们需要使用到<code>kubebuilder create api</code>:</p>

<pre><code>kubebuilder create api --group batch --version v1 --kind CronJob
</code></pre>

<p>我们为group-version 调用这个命令,它将为group-version 创建一个目录。</p>

<p>在这种情况下,<code>api/v1/</code>目录被创建,与version:<code>batch.tutorial.kubebuilder.io/v1</code>相对应。
(在这里你需要基础你<a href="https://book.kubebuilder.io/cronjob-tutorial.html#scaffolding-out-our-project">一开始设置的域名设置</a>)</p>

<p>它还为我们的<code>CronJob</code>类型添加了一个文件:<code>api/v1/cronjob_types.go</code>,我们每次调用这个命令的时候,他会为不同的类型生成一个新的文件.</p>

<p>从一开始,我们导入了<code>meta/v1</code> API group，它通常不会自己单独存在,而是包含所有kubernetes Kinds的共有的元数据.</p>

<pre><code>package v1

import (
    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
)
</code></pre>

<p>接下来,我们需要为我们的Kind定义相关的规范(Spec)和相关的状态,Kubernetes 通过协调预期状态(Spec)与实际集群状态(其他对象状态),或者外部状态,然后记录他们观察到的状态(Status)来起作用。因此,每个功能对象都包含了两大部分规范(Sepc)和状态(Status)。</p>

<p>另外, 一些类型(如ConfigMap)不遵循这种模式,因为他们不需要所需的期望状态和实际状态的对比过程,但是大多数类型需要。</p>

<pre><code>// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file
}

// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file
}
</code></pre>

<p>再接下来,我们得到了与实际Kinds,CronJob和CronjobList相对应的类型,CronJob是我们的根类型(root type),描述Cronjob的类型和相关信息.与所有的Kubernetes对象一样,它包含了<code>TypeMeta</code>(描述API版本和Kind), 同时也包含了<code>ObjectMeata</code>,它包含了名称,命名空间,标签等内容。</p>

<p><code>CronJobList</code>只是多个CronJobs的容器而已,这是批量操作中使用的类型,像数据结构的List。</p>

<p>通常,我们从不会修改<code>TypeMeat</code>或者<code>ObjectMeta</code>等存放元数据的结构体,所有的修改都在Spec或者Status中.</p>

<p><code>+kubebuilder:object:root</code>这个注解被称为标记,接下来,我们会看下它,它充当了额外的元数据,告诉控制器工具(代码和Yaml生成器)额外的信息. 这个特殊的标记告诉对象生成器(object generator) 这个类型代表一个Kind,接下来,对象生成器为我们生成<code>runtime.Object</code>接口实现，这意味着Kinds必须实现所有类型的接口。</p>

<pre><code>// +kubebuilder:object:root=true

// CronJob is the Schema for the cronjobs API
type CronJob struct {
    metav1.TypeMeta   `json:&quot;,inline&quot;`
    metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

    Spec   CronJobSpec   `json:&quot;spec,omitempty&quot;`
    Status CronJobStatus `json:&quot;status,omitempty&quot;`
}

// +kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
    metav1.TypeMeta `json:&quot;,inline&quot;`
    metav1.ListMeta `json:&quot;metadata,omitempty&quot;`
    Items           []CronJob `json:&quot;items&quot;`
}
</code></pre>

<p>最后, 我们把Go 类型添加到API Group下, 我们可以把这个API group天骄到任何的<a href="https://book.kubebuilder.io/todo">scheme</a></p>

<pre><code>func init() {
    SchemeBuilder.Register(&amp;CronJob{}, &amp;CronJobList{})
}
</code></pre>

<p>现在,我们已经有了一些基本结构,接下来,我们把他填满。</p>

<h2 id="设计api">设计API</h2>

<p>在Kubernetes中, 我们对如何设计API需要明确一些规则,比如说:所有的需要序列化的字段必须是驼峰命名法,所以,我们可以JSON结构来标记;我们也可以使用<code>omitempty</code>来标记当字段数据为空时从序列化中省略这个字段。</p>

<p>字段可以是基本类型，但是数字是一个例外,出于对API兼容性的考虑,我们接受两种形式的数字, int32表示整数,resource.Quantity 表示小数。</p>

<ul>
<li>注意,什么是Quantity?</li>
</ul>

<p>我们有个特殊的使用类型:<code>metav1.Time</code>,除了具有固定的反/序列化方式以外,他的功能和<code>time.Time</code>相同。</p>

<p>有了这个类型,我们看看CronJob对象长什么样子。</p>

<ul>
<li><p>导入</p>

<pre><code>package v1

import (
batchv1beta1 &quot;k8s.io/api/batch/v1beta1&quot;
corev1 &quot;k8s.io/api/core/v1&quot;
metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
)

// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.
</code></pre></li>
</ul>

<p>首先，我们来看看我们的spec,正如我们讨论的那样,spec定义期望状态,因此我们控制器的数据来源都会出现在这里(input口):</p>

<p>从根本上来说,CronJob需要实现以下:</p>

<ul>
<li>运行job的时间表 (A schedule the cron in CronJob)</li>
<li>一个运行Jon的模板(这个Job来自于CronJob)</li>
</ul>

<p>我们需要一些额外的功能,这可以让我们的工作更轻松一些:</p>

<ul>
<li>启动job的最后期限(如果错过了截至日期,我们需要等到下一个预定时间)</li>
<li>如果需要一次运行多个job,我们该怎么办(等待?停止旧的?同时运行?)</li>
<li>暂停运行Cronjob的方法</li>
<li>限制旧的job历史(保留几个)</li>
</ul>

<p>记住,由于我们从来没有读取过Job的Status,所以,我们需要使用其他方式来跟踪job是否已经运行。我们可以使用旧的job来做这个事情。</p>

<p>我们将使用几个标记(// +注释)来指定其他的元数据,这些标记在使用控制工具(代码生成工具)生成CRD清单的时候用到,正如我们稍后看到的那样,代码生成工具也可以使用GoDoc来对字段进行含义描述。</p>

<pre><code>// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
    // The schedule in Cron format, see https://en.wikipedia.org/wiki/Cron.
    Schedule string `json:&quot;schedule&quot;`

    // Optional deadline in seconds for starting the job if it misses scheduled
    // time for any reason.  Missed jobs executions will be counted as failed ones.
    // +optional
    StartingDeadlineSeconds *int64 `json:&quot;startingDeadlineSeconds,omitempty&quot;`

    // Specifies how to treat concurrent executions of a Job.
    // Valid values are:
    // - &quot;Allow&quot; (default): allows CronJobs to run concurrently;
    // - &quot;Forbid&quot;: forbids concurrent runs, skipping next run if previous run hasn't finished yet;
    // - &quot;Replace&quot;: cancels currently running job and replaces it with a new one
    // +optional
    ConcurrencyPolicy ConcurrencyPolicy `json:&quot;concurrencyPolicy,omitempty&quot;`

    // This flag tells the controller to suspend subsequent executions, it does
    // not apply to already started executions.  Defaults to false.
    // +optional
    Suspend *bool `json:&quot;suspend,omitempty&quot;`

    // Specifies the job that will be created when executing a CronJob.
    JobTemplate batchv1beta1.JobTemplateSpec `json:&quot;jobTemplate&quot;`

    // The number of successful finished jobs to retain.
    // This is a pointer to distinguish between explicit zero and not specified.
    // +optional
    SuccessfulJobsHistoryLimit *int32 `json:&quot;successfulJobsHistoryLimit,omitempty&quot;`

    // The number of failed finished jobs to retain.
    // This is a pointer to distinguish between explicit zero and not specified.
    // +optional
    FailedJobsHistoryLimit *int32 `json:&quot;failedJobsHistoryLimit,omitempty&quot;`
}

</code></pre>

<p>我们定义了一个自定义类型来保存我们的并发策略,但是实际上,它只是一个字符串,但是,我们给它定义了额外的类型,并提供了额外的文档和几个常量,我们在类型上而不是在字段值上做附加判断,因为这样有利于验证重用。</p>

<pre><code>// ConcurrencyPolicy describes how the job will be handled.
// Only one of the following concurrent policies may be specified.
// If none of the following policies is specified, the default one
// is AllowConcurrent.
// +kubebuilder:validation:Enum=Allow;Forbid;Replace
type ConcurrencyPolicy string

const (
    // AllowConcurrent allows CronJobs to run concurrently.
    AllowConcurrent ConcurrencyPolicy = &quot;Allow&quot;

    // ForbidConcurrent forbids concurrent runs, skipping next run if previous
    // hasn't finished yet.
    ForbidConcurrent ConcurrencyPolicy = &quot;Forbid&quot;

    // ReplaceConcurrent cancels currently running job and replaces it with a new one.
    ReplaceConcurrent ConcurrencyPolicy = &quot;Replace&quot;
)
</code></pre>

<p>接下来,我们设计一下<code>Status</code>,它是记录观察到的实际状态,它包括了我们希望展示给用户或者其他控制器的任何信息。</p>

<p>我们将保留一份正在运行的工作清单,以及我们上次成功完成工作的时间。注意,我们使用<code>metav1.Time</code>而不是<code>time.Time</code>来确保序列化的稳定性,正如我们上面提到的那样。</p>

<pre><code>// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file

    // A list of pointers to currently running jobs.
    // +optional
    Active []corev1.ObjectReference `json:&quot;active,omitempty&quot;`

    // Information when was the last time the job was successfully scheduled.
    // +optional
    LastScheduleTime *metav1.Time `json:&quot;lastScheduleTime,omitempty&quot;`
}
</code></pre>

<p>最后，通常情况下,我们不需要更改它，但是如果我们需要标记我们子资源的状态，对这块的设计可以参照kubernetes内置类型,因为它们的设计类似于kubernetes内置类型。</p>

<pre><code>// +kubebuilder:object:root=true
// +kubebuilder:subresource:status

// CronJob is the Schema for the cronjobs API
type CronJob struct {
</code></pre>

<ul>
<li><p>根对象定义</p>

<pre><code>metav1.TypeMeta   `json:&quot;,inline&quot;`
metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

Spec   CronJobSpec   `json:&quot;spec,omitempty&quot;`
Status CronJobStatus `json:&quot;status,omitempty&quot;`
}

// +kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
metav1.TypeMeta `json:&quot;,inline&quot;`
metav1.ListMeta `json:&quot;metadata,omitempty&quot;`
Items           []CronJob `json:&quot;items&quot;`
}

func init() {
SchemeBuilder.Register(&amp;CronJob{}, &amp;CronJobList{})
}
</code></pre></li>
</ul>

<p>现在,我们有了API,接下来,我们需要编写一个控制器来实际实现这个功能。</p>

<h2 id="旁白">旁白:</h2>

<p>如果你浏览过<code>api/v1</code>目录下的其他文件,你可能会注意到除了<code>cronjob_types.go</code>外,还有两个另外的文件:<code>groupversion_info.go</code>和<code>zz_generated.deepcopy.go</code></p>

<p>这两个文件不需要任何的修改,前者保持不变,后者是自动生成的,但如果你了解其中的内容对你来说很有用。</p>

<h4 id="groupversion-info-go">groupversion_info.go</h4>

<p><code>groupversion_info.go</code> 包含了group-version的两项元数据。</p>

<p>首先,我们有一些包级别的标记,这些标记代表这个包里有Kubernetes对象,并且此表示group:<code>batch.tutorial.kubebuilder.io</code>.  对象生成器主要使用前面的标记来生成对象,而后者由CRD生成器用它创建CRD并生成正确的元数据。如下所示.</p>

<pre><code>// Package v1 contains API Schema definitions for the batch v1 API group
// +kubebuilder:object:generate=true
// +groupName=batch.tutorial.kubebuilder.io
package v1

import (
    &quot;k8s.io/apimachinery/pkg/runtime/schema&quot;
    &quot;sigs.k8s.io/controller-runtime/pkg/scheme&quot;
)
</code></pre>

<p>接着，我们需要常用的变量来帮助我们设置<code>Scheme</code>,由于我们需要在控制器中使用此包的所有类型,因此我们需要一个简便方法将所有类型天骄到其他的<code>Scheme</code>中。SchemeBuilder 会帮助我们完成这些事情。</p>

<pre><code>var (
    // GroupVersion is group version used to register these objects
    GroupVersion = schema.GroupVersion{Group: &quot;batch.tutorial.kubebuilder.io&quot;, Version: &quot;v1&quot;}

    // SchemeBuilder is used to add go types to the GroupVersionKind scheme
    SchemeBuilder = &amp;scheme.Builder{GroupVersion: GroupVersion}

    // AddToScheme adds the types in this group-version to the given scheme.
    AddToScheme = SchemeBuilder.AddToScheme
)
</code></pre>

<h4 id="zz-generated-deepcopy-go">zz_generated.deepcopy.go</h4>

<p><code>zz_generated.deepcopy.go</code> 包含了对上面提到的<code>runtime.Object</code>接口的自动生成实现,它更具root类型标记为Kinds。</p>

<p><code>runtime.Object</code>接口的核心是深入复制方法<code>DeepCopyObject</code>!</p>

<p>controller-tools中的对象生成器还为每个根类型及其所有子类型生成另外两个方便的方法：<code>DeepCopy</code>和<code>DeepCopyInto</code>。</p>

<h2 id="什么是控制器">什么是控制器?</h2>

<p>控制器是Kubernetes的核心。</p>

<p>controller的工作任务是:给定任何一对对象,保证真实世界的状态(包括集群状态,外部状态,如：kubelet运行的容器或者是云提供商提供的负载均衡器)和期望的状态相匹配。每一个控制器都聚焦于一个个root Kind,但是也可能需要和其他Kind交互。</p>

<p>我们可以称这个过程为:<code>reconciling</code>(持续的监听现实世界的状态是否和期望是否一致,不一致就修正)</p>

<p>在控制器运行时,实现某种特定的协调逻辑,我们可以称之为:<code>Reconciler</code>,Reconciler(协调器)获取对象的名称,并返回是否需要再次尝试协调(比如: 出现错误或周期性控制器时, 如HPA).</p>

<p>首先,我们从标准入口开始。 和以前一样,我们需要核心控制器运行时库<code>(core controller-runtime library)
</code>,客户端包<code>(client package)</code>以及API类型包<code>(the package for our API types)</code>.</p>

<pre><code>package controllers

import (
    &quot;context&quot;

    &quot;github.com/go-logr/logr&quot;
    ctrl &quot;sigs.k8s.io/controller-runtime&quot;
    &quot;sigs.k8s.io/controller-runtime/pkg/client&quot;

    batchv1 &quot;tutorial.kubebuilder.io/project/api/v1&quot;
)
</code></pre>

<p>接下来,kubebuilder为我们提供了基本的协调代码结构,几乎每个协调程序都需要日志,并且能够获取对象,所以,我们需要这些都是能够开箱即用的。</p>

<pre><code>// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Log logr.Logger
}
</code></pre>

<p>大多数控制器最终会在集群上运行,因此,他们需要RBAC权限,可能第一次你可以给他们赋最低权限,但是随着我们添加更加丰富的功能, 我们可能需要重新赋权。</p>

<pre><code>// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch
</code></pre>

<p><code>Reconcile</code> 事实上只是对单个对象做协调,通常,我们的请求里面只有一个对象名称,但是,我们可以通过客户端从缓存获取这个对象的其他数据。</p>

<p>如果我们返回一个空结果而没有任何的错误信息,这意味着我们已经成功地纠正了这个对象。并且如果后续我们没有修改过相关数据,我们不会再尝试去纠正/协调。</p>

<p>大多数控制器需要一个日志句柄和一个上下文(context),所以,我们需要设置这些。</p>

<p><code>context</code>可以用来取消请求,追踪操作,通常,它是所有方法的第一个参数,<code>Backgroup</code>这个上下文只是一个没有任何额外数据或时序限制的基本上下文,你可以使用其他方法来扩展他。</p>

<p>日志句柄可以让我记录日志,<code>controller-runtime</code>使用一个名叫<code>logr</code>的日志库来进行结构化日志输出,待会我们就能看到,通过键值对的方式填写静态消息来进行日志记录. 我们也可以在协调方法的顶部预先分配一些键值对,这样可能比较方便我们进行日志记录。</p>

<pre><code>func (r *CronJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    _ = context.Background()
    _ = r.Log.WithValues(&quot;cronjob&quot;, req.NamespacedName)

    // your logic here

    return ctrl.Result{}, nil
}
</code></pre>

<p>最后,我们需要把这个纠正器<code>reconciler</code>加入到管理器, 这样,在管理器启动的时候,就会启动它。</p>

<p>现在,我们只需要关注这个协调程序在CronJob上运行,后面,我们会用它来标记我们关心的对象。</p>

<pre><code>func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&amp;batchv1.CronJob{}).
        Complete(r)
}
</code></pre>

<p>现在,我们看到了协调器的基本结构, 接下来需要填写CronJob的逻辑。</p>

<h2 id="实现一个控制器">实现一个控制器</h2>

<p>实现CronJob的基本逻辑如下:
1. 加载指定的CronJob
2. 列出所有活跃的jobs,并且更新它们的状态
3. 根据历史限制设置清除老的jobs
4. 检查任务是否被暂停(如果暂停了,我们不需要做任何事情)
5. 获取下一次计划的运行时间等信息。
6. 如果job按照计划运行,查看我们设置的并发策略是否阻止。(如果没有超过截止时间运行,根据并发策略并不会阻止其运行）
7. 存在正在运行的job(自动完成)或者现在这个时间点是计划运行时间点需要重新排队。</p>

<p>我们从入口开始,你会看到相比于之前用脚手架搭建的基础结构,我们需要导入更多的包.</p>

<pre><code>package controllers

import (
    &quot;context&quot;
    &quot;fmt&quot;
    &quot;sort&quot;
    &quot;time&quot;

    &quot;github.com/go-logr/logr&quot;
    &quot;github.com/robfig/cron&quot;
    kbatch &quot;k8s.io/api/batch/v1&quot;
    corev1 &quot;k8s.io/api/core/v1&quot;
    apierrs &quot;k8s.io/apimachinery/pkg/api/errors&quot;
    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
    &quot;k8s.io/apimachinery/pkg/runtime&quot;
    ref &quot;k8s.io/client-go/tools/reference&quot;
    ctrl &quot;sigs.k8s.io/controller-runtime&quot;
    &quot;sigs.k8s.io/controller-runtime/pkg/client&quot;

    batch &quot;tutorial.kubebuilder.io/project/api/v1&quot;
)
</code></pre>

<p>接下来,我们需要在调节器<code>Reconciler</code>做更多的事情:</p>

<p>我们需要一个<code>Scheme</code>,以便调用一些helper中的包和引用,我们需要一个<code>Clock</code>,这可以帮助我们在测试中伪造时间。</p>

<pre><code>// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Log    logr.Logger
    Scheme *runtime.Scheme
    Clock
}
</code></pre>

<ul>
<li>Clock</li>
</ul>

<p>我们需要模拟一个时钟,这样可以方便我们在测试的时候更容易及时调整,而真正的时钟是调用<code>time.Now()</code>.</p>

<pre><code>type realClock struct{}

func (_ realClock) Now() time.Time { return time.Now() }

// clock knows how to get the current time.
// It can be used to fake out timing for testing.
type Clock interface {
    Now() time.Time
}
</code></pre>

<ul>
<li>ignoreNotFound</li>
</ul>

<p>我们通常希望忽略(而不是重新排队)NotFound错误,因为一旦对象存在,会触发协调请求,并且,及时我们触发了重排序也没有任何的作用。</p>

<pre><code>func ignoreNotFound(err error) error {
    if apierrs.IsNotFound(err) {
        return nil
    }
    return err
}
</code></pre>

<p>注意: 现在,我们需要更多的RBAC权限, 因为需要创建和管理job, 所以,我们需要更多的权限。</p>

<pre><code>// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch,resources=jobs/status,verbs=get
</code></pre>

<p>现在,我们进入控制器的核心&mdash; 协调逻辑<code>the reconciler logic</code>.</p>

<pre><code>var (
    scheduledTimeAnnotation = &quot;batch.tutorial.kubebuilder.io/scheduled-at&quot;
)

func (r *CronJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    ctx := context.Background()
    log := r.Log.WithValues(&quot;cronjob&quot;, req.NamespacedName)
</code></pre>

<h4 id="1-通过name加载cronjob">1. 通过name加载CronJob</h4>

<p>我们通过client获取CronJob。 所有的方法都将<code>context(可以取消)</code>作为第一个参数,并将相关的对象作为最后一个参数. <code>Get</code>这个方法有点特殊,他把<code>NamespacedName</code>作为中间参数。而大多数方法没有中间参数,接下来,我们会看到。</p>

<p>许多的客户端方法最后也采用了可变参数选项。</p>

<pre><code>
    var cronJob batch.CronJob
    if err := r.Get(ctx, req.NamespacedName, &amp;cronJob); err != nil {
        log.Error(err, &quot;unable to fetch CronJob&quot;)
        // we'll ignore not-found errors, since they can't be fixed by an immediate
        // requeue (we'll need to wait for a new notification), and we can get them
        // on deleted requests.
        return ctrl.Result{}, ignoreNotFound(err)
    }
</code></pre>

<h4 id="2-显示所有活跃的job-并且更新他们的状态">2. 显示所有活跃的job,并且更新他们的状态。</h4>

<p>如果想完全更新状态,我们需要列出此namespace下属于此CronJob的所有子Jobs, 与Get一样,我们可以使用List方法列出所有的子Jobs。注意,我们使用<code>variadic</code> 选项来设置namespace和字段匹配(事实上,我们在下面设置了索引查找)。</p>

<pre><code>
    var cronJob batch.CronJob
    if err := r.Get(ctx, req.NamespacedName, &amp;cronJob); err != nil {
        log.Error(err, &quot;unable to fetch CronJob&quot;)
        // we'll ignore not-found errors, since they can't be fixed by an immediate
        // requeue (we'll need to wait for a new notification), and we can get them
        // on deleted requests.
        return ctrl.Result{}, ignoreNotFound(err)
    }

</code></pre>

<p>一旦我们有了所有的jobs,我们就能够把这些jobs分成活跃的,成功完成的,失败的,根据最近的运行的job, 我们可以很方便的记录其中的状态。</p>

<pre><code>    // find the active list of jobs
    var activeJobs []*kbatch.Job
    var successfulJobs []*kbatch.Job
    var failedJobs []*kbatch.Job
    var mostRecentTime *time.Time // find the last run so we can update the status
</code></pre>

<ul>
<li>isJobFinished</li>
</ul>

<p>如果一个job已经&rdquo;完成&rdquo;了,他有个标记是成功的还是失败的. <code>Status</code>条件运行我们添加更多的扩展状态信息,其他人和控制器可以检查这些信息.比如检查是否完成和健康检查(非K8s的健康检查)之类的事情。</p>

<pre><code>    isJobFinished := func(job *kbatch.Job) (bool, kbatch.JobConditionType) {
        for _, c := range job.Status.Conditions {
            if (c.Type == kbatch.JobComplete || c.Type == kbatch.JobFailed) &amp;&amp; c.Status == corev1.ConditionTrue {
                return true, c.Type
            }
        }

        return false, &quot;&quot;
    }
</code></pre>

<ul>
<li>getScheduledTimeForJob</li>
</ul>

<p>我们可以使用helper 程序 从作业一开始创建的时候添加的注解(annotation)中提取计划时间。</p>

<pre><code>    getScheduledTimeForJob := func(job *kbatch.Job) (*time.Time, error) {
        timeRaw := job.Annotations[scheduledTimeAnnotation]
        if len(timeRaw) == 0 {
            return nil, nil
        }

        timeParsed, err := time.Parse(time.RFC3339, timeRaw)
        if err != nil {
            return nil, err
        }
        return &amp;timeParsed, nil
    }
</code></pre>

<pre><code>    for i, job := range childJobs.Items {
        _, finishedType := isJobFinished(&amp;job)
        switch finishedType {
        case &quot;&quot;: // ongoing
            activeJobs = append(activeJobs, &amp;childJobs.Items[i])
        case kbatch.JobFailed:
            failedJobs = append(failedJobs, &amp;childJobs.Items[i])
        case kbatch.JobComplete:
            successfulJobs = append(successfulJobs, &amp;childJobs.Items[i])
        }

        // We'll store the launch time in an annotation, so we'll reconsitute that from
        // the active jobs themselves.
        scheduledTimeForJob, err := getScheduledTimeForJob(&amp;job)
        if err != nil {
            log.Error(err, &quot;unable to parse schedule time for child job&quot;, &quot;job&quot;, &amp;job)
            continue
        }
        if scheduledTimeForJob != nil {
            if mostRecentTime == nil {
                mostRecentTime = scheduledTimeForJob
            } else if mostRecentTime.Before(*scheduledTimeForJob) {
                mostRecentTime = scheduledTimeForJob
            }
        }
    }

    if mostRecentTime != nil {
        cronJob.Status.LastScheduleTime = &amp;metav1.Time{Time: *mostRecentTime}
    } else {
        cronJob.Status.LastScheduleTime = nil
    }
    cronJob.Status.Active = nil
    for _, activeJob := range activeJobs {
        jobRef, err := ref.GetReference(r.Scheme, activeJob)
        if err != nil {
            log.Error(err, &quot;unable to make reference to active job&quot;, &quot;job&quot;, activeJob)
            continue
        }
        cronJob.Status.Active = append(cronJob.Status.Active, *jobRef)
    }
</code></pre>

<p>在这里,我们记录了较高的日志级别以便我们观察到作业数量,方便我们调试。值得注意的是,我们并没有使用格式化的字符串，而是使用了固定消息方式并使用键值对的方式来添加额外的附加信息。这样就可以更加轻松的过滤和查询日志行。</p>

<pre><code>    log.V(1).Info(&quot;job count&quot;, &quot;active jobs&quot;, len(activeJobs), &quot;successful jobs&quot;, len(successfulJobs), &quot;failed jobs&quot;, len(failedJobs))
</code></pre>

<p>使用我们上面收集到的时间日期,我们可以更新CRD的状态。和以前一样,我们使用我们的客户端。为了更新子资源的状态，我们使用client的<code>Update</code>方法来更新<code>Status</code>部分.</p>

<p>更新子资源状态可以忽略对规范的更改,因为,它不太可能和其他更新冲突,并且它也具有单独的权限。</p>

<pre><code>    if err := r.Status().Update(ctx, &amp;cronJob); err != nil {
        log.Error(err, &quot;unable to update CronJob status&quot;)
        return ctrl.Result{}, err
    }
</code></pre>

<p>一旦我们更新了状态,我们可以确保真实世界的状态符合我们期望的状态。</p>

<h4 id="3-根据历史限制清理老的job">3. 根据历史限制清理老的job</h4>

<p>我们尝试清理旧的job,这样我们就不会留下太多的历史.</p>

<pre><code>    // NB: deleting these is &quot;best effort&quot; -- if we fail on a particular one,
    // we won't requeue just to finish the deleting.
    if cronJob.Spec.FailedJobsHistoryLimit != nil {
        sort.Slice(failedJobs, func(i, j int) bool {
            if failedJobs[i].Status.StartTime == nil {
                return failedJobs[j].Status.StartTime != nil
            }
            return failedJobs[i].Status.StartTime.Before(failedJobs[j].Status.StartTime)
        })
        for i, job := range failedJobs {
            if err := r.Delete(ctx, job); err != nil {
                log.Error(err, &quot;unable to delete old failed job&quot;, &quot;job&quot;, job)
            }
            if int32(i) &gt;= *cronJob.Spec.FailedJobsHistoryLimit {
                break
            }
        }
    }

    if cronJob.Spec.SuccessfulJobsHistoryLimit != nil {
        sort.Slice(successfulJobs, func(i, j int) bool {
            if successfulJobs[i].Status.StartTime == nil {
                return successfulJobs[j].Status.StartTime != nil
            }
            return successfulJobs[i].Status.StartTime.Before(successfulJobs[j].Status.StartTime)
        })
        for i, job := range successfulJobs {
            if err := r.Delete(ctx, job); err != nil {
                log.Error(err, &quot;unable to delete old successful job&quot;, &quot;job&quot;, job)
            }
            if int32(i) &gt;= *cronJob.Spec.SuccessfulJobsHistoryLimit {
                break
            }
        }
    }
</code></pre>

<h4 id="4-检查job是否被暂停">4. 检查job是否被暂停</h4>

<p>如果对象已经被暂停,说明我们不想执行任何的job,所以,我们需要立即停止,我们期望中断正在运行的job(任务),而且只是希望暂时暂停运行而不是删除对象,所以,这个功能是非常有用的.</p>

<pre><code>    if cronJob.Spec.Suspend != nil &amp;&amp; *cronJob.Spec.Suspend {
        log.V(1).Info(&quot;cronjob suspended, skipping&quot;)
        return ctrl.Result{}, nil
    }
</code></pre>

<h4 id="5-获取下一次计划运行时间">5. 获取下一次计划运行时间</h4>

<p>如果我们没有暂停这个cronjob,我们需要计算按照计划下一次运行的时间,以及我们是否还存在尚未知执行的任务。</p>

<ul>
<li>getNextSchedule</li>
</ul>

<p>我们可以使用现有的cron库来计算下一个预定时间, 如果我们找不到最后一次运行时间,我们就从上一次运行开始计算合适的时间或者重新创建一个CronJob.</p>

<p>我们我们已经错过了太多的计划时间并且我们没有设置截止时间,我们不会尝试补偿运行,因为这样就不会再重启控制器或者锲子的时候造成问题。</p>

<p>否则,我们只返回错过的运行计划(使用最新的运行计划)和下次运行计划,这样可以方便我们知道何时再次镜像纠正。</p>

<pre><code>    getNextSchedule := func(cronJob *batch.CronJob, now time.Time) (lastMissed *time.Time, next time.Time, err error) {
        sched, err := cron.ParseStandard(cronJob.Spec.Schedule)
        if err != nil {
            return nil, time.Time{}, fmt.Errorf(&quot;Unparseable schedule %q: %v&quot;, cronJob.Spec.Schedule, err)
        }

        // for optimization purposes, cheat a bit and start from our last observed run time
        // we could reconstitute this here, but there's not much point, since we've
        // just updated it.
        var earliestTime time.Time
        if cronJob.Status.LastScheduleTime != nil {
            earliestTime = cronJob.Status.LastScheduleTime.Time
        } else {
            earliestTime = cronJob.ObjectMeta.CreationTimestamp.Time
        }
        if cronJob.Spec.StartingDeadlineSeconds != nil {
            // controller is not going to schedule anything below this point
            schedulingDeadline := now.Add(-time.Second * time.Duration(*cronJob.Spec.StartingDeadlineSeconds))

            if schedulingDeadline.After(earliestTime) {
                earliestTime = schedulingDeadline
            }
        }
        if earliestTime.After(now) {
            return nil, sched.Next(now), nil
        }

        starts := 0
        for t := sched.Next(earliestTime); !t.After(now); t = sched.Next(t) {
            lastMissed = &amp;t
            // An object might miss several starts. For example, if
            // controller gets wedged on Friday at 5:01pm when everyone has
            // gone home, and someone comes in on Tuesday AM and discovers
            // the problem and restarts the controller, then all the hourly
            // jobs, more than 80 of them for one hourly scheduledJob, should
            // all start running with no further intervention (if the scheduledJob
            // allows concurrency and late starts).
            //
            // However, if there is a bug somewhere, or incorrect clock
            // on controller's server or apiservers (for setting creationTimestamp)
            // then there could be so many missed start times (it could be off
            // by decades or more), that it would eat up all the CPU and memory
            // of this controller. In that case, we want to not try to list
            // all the missed start times.
            starts++
            if starts &gt; 100 {
                // We can't get the most recent times so just return an empty slice
                return nil, time.Time{}, fmt.Errorf(&quot;Too many missed start times (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.&quot;)
            }
        }
        return lastMissed, sched.Next(now), nil
    }
</code></pre>

<pre><code>    // figure out the next times that we need to create
    // jobs at (or anything we missed).
    missedRun, nextRun, err := getNextSchedule(&amp;cronJob, r.Now())
    if err != nil {
        log.Error(err, &quot;unable to figure out CronJob schedule&quot;)
        // we don't really care about requeuing until we get an update that
        // fixes the schedule, so don't return an error
        return ctrl.Result{}, nil
    }
</code></pre>

<p>准备将最后的请求重新排队直到下一个job,然后确定我们是否真的需要运行这个job。</p>

<pre><code>    scheduledResult := ctrl.Result{RequeueAfter: nextRun.Sub(r.Now())} // save this so we can re-use it elsewhere
    log = log.WithValues(&quot;now&quot;, r.Now(), &quot;next run&quot;, nextRun)
</code></pre>

<h4 id="6-按照计划运行新的job-且没有超过截止期限-也就不会被并发策略影响">6. 按照计划运行新的job,且没有超过截止期限,也就不会被并发策略影响</h4>

<p>如果我们错过了计划的运行时间,但仍然处于他启动的最后期限之内,我们仍然需要运行这个job.</p>

<pre><code>    if missedRun == nil {
        log.V(1).Info(&quot;no upcoming scheduled times, sleeping until next&quot;)
        return scheduledResult, nil
    }

    // make sure we're not too late to start the run
    log = log.WithValues(&quot;current run&quot;, missedRun)
    tooLate := false
    if cronJob.Spec.StartingDeadlineSeconds != nil {
        tooLate = missedRun.Add(time.Duration(*cronJob.Spec.StartingDeadlineSeconds) * time.Second).Before(r.Now())
    }
    if tooLate {
        log.V(1).Info(&quot;missed starting deadline for last run, sleeping till next&quot;)
        // TODO(directxman12): events
        return scheduledResult, nil
    }

</code></pre>

<p>如果我们按照执行计划必须得运行一个job,我们需要等待现有任务完成来替换已经存在的任务或者运行一个新的,如果因为缓存延迟导致了我们读取信息过期,我们需要得到最新信息后重新排队。</p>

<pre><code>    // figure out how to run this job -- concurrency policy might forbid us from running
    // multiple at the same time...
    if cronJob.Spec.ConcurrencyPolicy == batch.ForbidConcurrent &amp;&amp; len(activeJobs) &gt; 0 {
        log.V(1).Info(&quot;concurrency policy blocks concurrent runs, skipping&quot;, &quot;num active&quot;, len(activeJobs))
        return scheduledResult, nil
    }

    // ...or instruct us to replace existing ones...
    if cronJob.Spec.ConcurrencyPolicy == batch.ReplaceConcurrent {
        for _, activeJob := range activeJobs {
            // we don't care if the job was already deleted
            if err := r.Delete(ctx, activeJob); ignoreNotFound(err) != nil {
                log.Error(err, &quot;unable to delete active job&quot;, &quot;job&quot;, activeJob)
                return ctrl.Result{}, err
            }
        }
    }

</code></pre>

<pre><code>Once we've figured out what to do with existing jobs, we'll actually create our desired job
</code></pre>

<ul>
<li><p>constructJobForCronJob</p>

<pre><code>// actually make the job...
job, err := constructJobForCronJob(&amp;cronJob, *missedRun)
if err != nil {
    log.Error(err, &quot;unable to construct job from template&quot;)
    // don't bother requeuing until we get a change to the spec
    return scheduledResult, nil
}

// ...and create it on the cluster
if err := r.Create(ctx, job); err != nil {
    log.Error(err, &quot;unable to create Job for CronJob&quot;, &quot;job&quot;, job)
    return ctrl.Result{}, err
}

log.V(1).Info(&quot;created Job for CronJob run&quot;, &quot;job&quot;, job)
</code></pre></li>
</ul>

<h4 id="7-存在正在运行的job-自动完成-或者现在这个时间点是计划运行时间时需要重新排队">7. 存在正在运行的job(自动完成)或者现在这个时间点是计划运行时间时需要重新排队</h4>

<p>最后,我们会返回上面准备的结果,表示我们想要在下次运行时需要重新启动。这被称之为最大期限&mdash;如果其他事情发生了变化,比如我们的工作开始或者结束,我们会更新状态,这会触发再次纠正/协调。</p>

<pre><code>    // we'll requeue once we see the running job, and update our status
    return scheduledResult, nil
}

</code></pre>

<h4 id="设置">设置</h4>

<p>最后,我们需要更新我们的设置,这样是为了让协调器能够通过所有者查找到这些job, 我们需要一个索引,我们声明一个索引,我们可以把这个字段和客户端一起使用,用来描述如何从job中提取索引值。</p>

<p>索引器将自动为我们处理namespace,因此如果job具有CronJob所有者(owner)标识,我们只需要提取出所有者标识(owner)。</p>

<p>另外,我们会通知管理器这个控制器拥有一些Jobs,以便在Job发生变化,删除等时自动调用底层CronJob上的Reconcile。</p>

<pre><code>var (
    jobOwnerKey = &quot;.metadata.controller&quot;
    apiGVStr    = batch.GroupVersion.String()
)

func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
    // set up a real clock, since we're not in a test
    if r.Clock == nil {
        r.Clock = realClock{}
    }

    if err := mgr.GetFieldIndexer().IndexField(&amp;kbatch.Job{}, jobOwnerKey, func(rawObj runtime.Object) []string {
        // grab the job object, extract the owner...
        job := rawObj.(*kbatch.Job)
        owner := metav1.GetControllerOf(job)
        if owner == nil {
            return nil
        }
        // ...make sure it's a CronJob...
        if owner.APIVersion != apiGVStr || owner.Kind != &quot;CronJob&quot; {
            return nil
        }

        // ...and if so, return it
        return []string{owner.Name}
    }); err != nil {
        return err
    }

    return ctrl.NewControllerManagedBy(mgr).
        For(&amp;batch.CronJob{}).
        Owns(&amp;kbatch.Job{}).
        Complete(r)
}
</code></pre>

<p>现在我们有个工作的控制器,我们需要进行测试,如果没有问题,我们可以部署。</p>

<h2 id="关于main-我们需要说点什么">关于Main,我们需要说点什么?</h2>

<p>首先,记得我们之前说的再次回到<code>main.go</code>吗? 让我们看看发生了那些变化,需要添加些什么?</p>

<ul>
<li><p>Imports</p>

<pre><code>
var (
jobOwnerKey = &quot;.metadata.controller&quot;
apiGVStr    = batch.GroupVersion.String()
)

func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
// set up a real clock, since we're not in a test
if r.Clock == nil {
    r.Clock = realClock{}
}

if err := mgr.GetFieldIndexer().IndexField(&amp;kbatch.Job{}, jobOwnerKey, func(rawObj runtime.Object) []string {
    // grab the job object, extract the owner...
    job := rawObj.(*kbatch.Job)
    owner := metav1.GetControllerOf(job)
    if owner == nil {
        return nil
    }
    // ...make sure it's a CronJob...
    if owner.APIVersion != apiGVStr || owner.Kind != &quot;CronJob&quot; {
        return nil
    }

    // ...and if so, return it
    return []string{owner.Name}
}); err != nil {
    return err
}

return ctrl.NewControllerManagedBy(mgr).
    For(&amp;batch.CronJob{}).
    Owns(&amp;kbatch.Job{}).
    Complete(r)
}
</code></pre></li>
</ul>

<p>注意第一个区别是:kubebuilder已经将新的API group包(<code>kbatchvabeta1</code>)添加到我们的Scheme中,这意味着我们可以在控制器中使用这些对象。</p>

<p>我们还需要添加kubernetes batch v1 schme,因为我们需要我们创建和列表显示Job。</p>

<pre><code>var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName(&quot;setup&quot;)
)

func init() {

    kbatchv1beta1.AddToScheme(scheme) // we've added this ourselves
    batchv1.AddToScheme(scheme)
    // +kubebuilder:scaffold:scheme
}

</code></pre>

<p>另外一个不同的事情就是<code>kubebuilder</code>添加了一个block来调用我们的CronJob控制的<code>SetupWithManager</code>方法。这是因为我们自己也使用了一个<code>Scheme</code>,我们需要把我们自己把它注册给协调器。</p>

<pre><code>func main() {

</code></pre>

<ul>
<li><p>之前的代码:</p>

<pre><code>
var metricsAddr string
flag.StringVar(&amp;metricsAddr, &quot;metrics-addr&quot;, &quot;:8080&quot;, &quot;The address the metric endpoint binds to.&quot;)
flag.Parse()

ctrl.SetLogger(zap.Logger(true))

mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
if err != nil {
    setupLog.Error(err, &quot;unable to start manager&quot;)
    os.Exit(1)
}
</code></pre></li>
</ul>

<p>现在的代码:</p>

<pre><code>    var metricsAddr string
    flag.StringVar(&amp;metricsAddr, &quot;metrics-addr&quot;, &quot;:8080&quot;, &quot;The address the metric endpoint binds to.&quot;)
    flag.Parse()

    ctrl.SetLogger(zap.Logger(true))

    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
    if err != nil {
        setupLog.Error(err, &quot;unable to start manager&quot;)
        os.Exit(1)
    }
</code></pre>

<ul>
<li><p>之前的代码:</p>

<pre><code>setupLog.Info(&quot;starting manager&quot;)
if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
    setupLog.Error(err, &quot;problem running manager&quot;)
    os.Exit(1)
}
</code></pre></li>
</ul>

<p>现在的代码:</p>

<pre><code>}
</code></pre>

<p>现在,我们实现了控制器。</p>

<h2 id="1-8-运行和部署控制器">1.8 运行和部署控制器</h2>

<p>为测试控制器,我们可以再本地集群运行它,在做这件事情之前,我们需要安装我们的CRDs(依照快速开始),如果需要,你可以使用控制器工具自动更新yaml清单。</p>

<pre><code>make install
</code></pre>

<p>现在我们已经安装了CRD,我们可以运作控制器,由于RBAC权限之前已经设置过,我们就不用担心权限问题了.</p>

<p>换个终端运行:</p>

<pre><code>make run

</code></pre>

<p>你可以从控制器中看到启动日志,但是它现在还没有做任何事情.</p>

<p>现在, 我们需要一个CronJob来测试,所以,我们需要些一个yaml(<code>config/samples/batch_v1_cronjob.yaml</code>)来测试:</p>

<pre><code>apiVersion: batch.tutorial.kubebuilder.io/v1
kind: CronJob
metadata:
  name: cronjob-sample
spec:
  schedule: &quot;*/1 * * * *&quot;
  startingDeadlineSeconds: 60
  concurrencyPolicy: Allow # explicitly specify, but Allow is also default.
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
</code></pre>

<pre><code>kubectl create -f config/samples/batch_v1_cronjob.yaml
</code></pre>

<p>查看cronJob的状态:</p>

<pre><code>kubectl get cronjob.batch.tutorial.kubebuilder.io -o yaml
kubectl get job
</code></pre>

<p>现在,我们知道了它正在运行中,在集群中也能允许,停止<code>make run</code>调用,然后运行:</p>

<pre><code>make docker-build docker-push IMG=&lt;some-registry&gt;/controller
make deploy

</code></pre>

<p>我们可以再次列出cronjobs也能看到控制器再次运行了。</p>
]]></content>
		</item>
		
		<item>
			<title>在 Kubernetes 使用 ceph 快速部署mysql主从复制集群</title>
			<link>https://yulibaozi.com/posts/kubernetes/2018-09-06-k8s-ceph-mysql-master-slave/</link>
			<pubDate>Sun, 16 Sep 2018 01:03:46 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/2018-09-06-k8s-ceph-mysql-master-slave/</guid>
			<description>血与泪的感慨 我的天空星星都亮了，一扫之前的阴霾，喜上眉梢，多日的折磨成了幻影，程序员就是这样，痛并快乐着。接下来，开始我们伟大的分享之路，分享如何在kubernetes中搭建mysql主从集群,不踩坑的快速搭建master主从集群,让你们早点回家，多陪陪女友，愿码友们不终日与十姊妹为伍。
千里之行始于足下  集群版本信息
[root@kube03 ~]# kubectl version Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;9&amp;quot;, GitVersion:&amp;quot;v1.9.6&amp;quot;, BuildDate:&amp;quot;2018-03-21T15:21:50Z&amp;quot;, GoVersion:&amp;quot;go1.9.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;} Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;9&amp;quot;, GitVersion:&amp;quot;v1.9.6&amp;quot;, BuildDate:&amp;quot;2018-03-21T15:13:31Z&amp;quot;, GoVersion:&amp;quot;go1.9.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}  需要ceph的相关配置:key,rbdImage名字(需要找ceph管理员)
 需要基础镜像
镜像仓库: docker.io/yulibaozi master: yulibaozi/mysql-master:201807261706 sha256:65ec774cd3a5e71bae1864dffbb1b37b34a2832cce1b8eb6c87f5b1e24b54267 slave: yulibaozi/mysql-slave:201807261706 sha256:712359cbe130bf282876ea7df85fb7f67d8843ab64e54f96a73fee72cef67d87  需要定义k8s相关的资源(svc,sts,secret)
  相关资源的解释  为什么需要secret？
secret是来存储ceph的配置信息中的key的,一定要记得在存入secret之前进行base64编码。如果管理员提供给你了一个key，2个rbdImage，那么只需要一个secret，如果给了你两个key,那么需要两个secret
 为什么需要sts？
sts和deployment类似，只不过deployment用于无状态的应用而sts应用于有状态的应用，master和slave各一个sts
 为什么需要svc？
svc是为外部提供服务的，master一个svc，slave一个svc。
  千里之行 创建secret  假设管理员给你的ceph信息如下:
key = AQAqfzNaofxHLBAAS7qY64uE/ddqWLOMVDhkAA== rbdImagename = k8s_image01 user = admin（假设默认是admin,如果不是就问ceph管理员要）  执行命令把ceph的Key使用base64编码</description>
			<content type="html"><![CDATA[

<h3 id="血与泪的感慨">血与泪的感慨</h3>

<p>我的天空星星都亮了，一扫之前的阴霾，喜上眉梢，多日的折磨成了幻影，程序员就是这样，痛并快乐着。接下来，开始我们伟大的分享之路，分享如何在kubernetes中搭建mysql主从集群,不踩坑的快速搭建master主从集群,让你们早点回家，多陪陪女友，愿码友们不终日与十姊妹为伍。</p>

<h3 id="千里之行始于足下">千里之行始于足下</h3>

<ol>
<li><p>集群版本信息</p>

<pre><code>[root@kube03 ~]# kubectl version

Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, BuildDate:&quot;2018-03-21T15:21:50Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
</code></pre></li>

<li><p>需要ceph的相关配置:key,rbdImage名字(需要找ceph管理员)</p></li>

<li><p>需要基础镜像</p>

<pre><code>镜像仓库: docker.io/yulibaozi
master: yulibaozi/mysql-master:201807261706 sha256:65ec774cd3a5e71bae1864dffbb1b37b34a2832cce1b8eb6c87f5b1e24b54267
slave: yulibaozi/mysql-slave:201807261706   sha256:712359cbe130bf282876ea7df85fb7f67d8843ab64e54f96a73fee72cef67d87
</code></pre></li>

<li><p>需要定义k8s相关的资源(svc,sts,secret)</p></li>
</ol>

<h4 id="相关资源的解释">相关资源的解释</h4>

<ol>
<li><p>为什么需要secret？</p>

<p>secret是来存储ceph的配置信息中的key的,一定要记得在存入secret之前进行base64编码。如果管理员提供给你了一个key，2个rbdImage，那么只需要一个secret，如果给了你两个key,那么需要两个secret</p></li>

<li><p>为什么需要sts？</p>

<p>sts和deployment类似，只不过deployment用于无状态的应用而sts应用于有状态的应用，master和slave各一个sts</p></li>

<li><p>为什么需要svc？</p>

<p>svc是为外部提供服务的，master一个svc，slave一个svc。</p></li>
</ol>

<h3 id="千里之行">千里之行</h3>

<h4 id="创建secret">创建secret</h4>

<ol>
<li><p>假设管理员给你的ceph信息如下:</p>

<pre><code>key = AQAqfzNaofxHLBAAS7qY64uE/ddqWLOMVDhkAA==
rbdImagename = k8s_image01
user = admin（假设默认是admin,如果不是就问ceph管理员要）
</code></pre></li>

<li><p>执行命令把ceph的Key使用base64编码</p>

<pre><code>[root@kube03 yuli]# echo &quot;AQAqfzNaofxHLBAAS7qY64uE/ddqWLOMVDhkAA==&quot; | base64
QVFBcWZ6TmFvZnhITEJBQVM3cVk2NHVFL2RkcVdMT01WRGhrQUE9PQo=
</code></pre></li>

<li><p>创建secret文件</p>

<pre><code>[root@kube03 sts]# cat 1ceph-secret.json

{
&quot;apiVersion&quot;: &quot;v1&quot;,
&quot;data&quot;: {
    &quot;key&quot;: &quot;QVFBcWZ6TmFvZnhITEJBQVM3cVk2NHVFL2RkcVdMT01WRGhrQUE9PQo=&quot;
},
&quot;kind&quot;: &quot;Secret&quot;,
&quot;metadata&quot;: {
    &quot;name&quot;: &quot;ceph-secret&quot;,
    &quot;namespace&quot;: &quot;yulibaozi&quot;
},
&quot;type&quot;: &quot;Opaque&quot;
}
</code></pre></li>
</ol>

<p>（可选）如果没有namespace,需要创建,创建namespace的命令如下:</p>

<pre><code>kubectl create namespace yulibaozi
</code></pre>

<p>执行命令创建secret</p>

<pre><code>kubectl create -f 1ceph-secret.json
</code></pre>

<p>secret 创建完毕了</p>

<h4 id="创建master的sts">创建master的sts</h4>

<p>master的sts如下:</p>

<pre><code>[root@kube03 sts]# cat 2master-sts.json
{
    &quot;apiVersion&quot;: &quot;apps/v1&quot;,
    &quot;kind&quot;: &quot;StatefulSet&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;name&quot;: &quot;mysql-master&quot;
        },
        &quot;name&quot;: &quot;mysql-master&quot;,
        &quot;namespace&quot;: &quot;yulibaozi&quot;
    },
    &quot;spec&quot;: {
        &quot;podManagementPolicy&quot;: &quot;OrderedReady&quot;,
        &quot;replicas&quot;: 1,
        &quot;revisionHistoryLimit&quot;: 10,
        &quot;selector&quot;: {
            &quot;matchLabels&quot;: {
                &quot;app&quot;: &quot;mysql-master&quot;
            }
        },
        &quot;serviceName&quot;: &quot;mysql-master&quot;,
        &quot;template&quot;: {
            &quot;metadata&quot;: {
                &quot;labels&quot;: {
                    &quot;app&quot;: &quot;mysql-master&quot;
                }
            },
            &quot;spec&quot;: {
                &quot;containers&quot;: [
                    {
                        &quot;env&quot;: [
                            {
                                &quot;name&quot;: &quot;MYSQL_ROOT_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_USER&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_PASSWORD&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            }
                        ],
                        &quot;image&quot;: &quot;yulibaozi/mysql-master:201807261706&quot;,
                        &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;,
                        &quot;name&quot;: &quot;mysql-master&quot;,
                        &quot;ports&quot;: [
                            {
                                &quot;containerPort&quot;: 3306,
                                &quot;protocol&quot;: &quot;TCP&quot;
                            }
                        ],
                        &quot;resources&quot;: {},
                        &quot;terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
                        &quot;terminationMessagePolicy&quot;: &quot;File&quot;,
                        &quot;volumeMounts&quot;: [
                            {
            ③                    &quot;mountPath&quot;: &quot;/var/lib/mysql&quot;,
                                &quot;name&quot;: &quot;data&quot;
                            }
                        ]
                    }
                ],
                &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
                &quot;restartPolicy&quot;: &quot;Always&quot;,
                &quot;schedulerName&quot;: &quot;default-scheduler&quot;,
                &quot;securityContext&quot;: {},
                &quot;terminationGracePeriodSeconds&quot;: 10,
                &quot;volumes&quot;: [
                    {
                        &quot;name&quot;: &quot;data&quot;,
                        &quot;rbd&quot;: {
                            &quot;fsType&quot;: &quot;xfs&quot;,
            ②               &quot;image&quot;: &quot;k8s_image01&quot;,
                            &quot;keyring&quot;: &quot;/etc/ceph/keyring&quot;,
                            &quot;monitors&quot;: [
            ⑤                   &quot;192.168.0.199:6789&quot;
                            ],
                            &quot;pool&quot;: &quot;rbd&quot;,
                            &quot;secretRef&quot;: {
            ①                   &quot;name&quot;: &quot;ceph-secret&quot;
                            },
            ④               &quot;user&quot;: &quot;admin&quot;
                        }
                    }
                ]
            }
        },
        &quot;updateStrategy&quot;: {
            &quot;rollingUpdate&quot;: {
                &quot;partition&quot;: 0
            },
            &quot;type&quot;: &quot;RollingUpdate&quot;
        }
    },
    &quot;status&quot;: {
    }
}

相关解释：
①: secret的名字,就是上文中创建的secret名字
②：ceph管理员给你的块名字
③：容器中挂载的目录,这么目录就是mysql存数据的目录,如果不知道怎么做就不要改
④：ceph的用户名，问下ceph管理员是什么，如果不是admin就得改
⑤：monitors:ceph的monitor地址，找ceph管理员要，可以是一个,也可以是多个

</code></pre>

<p>确认了4点后向下看
创建master-sts:</p>

<pre><code>kubectl create -f 2master-sts.json
</code></pre>

<p>查看状态:</p>

<pre><code>[root@kube03 sts]# kubectl get pod -n yulibaozi

NAME             READY     STATUS    RESTARTS   AGE
mysql-master-0   1/1       Running   0          2h

如果状态一直是CreateContaining，等10分钟左右还不是Running说明有问题,愿上天眷顾你。如果的确有问题，那么查问题。
</code></pre>

<p>master的sts创建完毕。</p>

<h4 id="创建slave的sts">创建slave的sts</h4>

<p>slave的sts配置如下：</p>

<pre><code>[root@kube03 sts]# cat 3slave-sts.json

{
    &quot;apiVersion&quot;: &quot;apps/v1&quot;,
    &quot;kind&quot;: &quot;StatefulSet&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;name&quot;: &quot;mysql-slave&quot;
        },
        &quot;name&quot;: &quot;mysql-slave&quot;,
        &quot;namespace&quot;: &quot;yulibaozi&quot;
    },
    &quot;spec&quot;: {
        &quot;podManagementPolicy&quot;: &quot;OrderedReady&quot;,
        &quot;replicas&quot;: 1,
        &quot;revisionHistoryLimit&quot;: 10,
        &quot;selector&quot;: {
            &quot;matchLabels&quot;: {
                &quot;name&quot;: &quot;mysql-slave&quot;
            }
        },
        &quot;serviceName&quot;: &quot;mysql-slave&quot;,
        &quot;template&quot;: {
            &quot;metadata&quot;: {
                &quot;creationTimestamp&quot;: null,
                &quot;labels&quot;: {
                    &quot;name&quot;: &quot;mysql-slave&quot;
                }
            },
            &quot;spec&quot;: {
                &quot;containers&quot;: [
                    {
                        &quot;env&quot;: [
                            {
                                &quot;name&quot;: &quot;MYSQL_ROOT_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_USER&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_PASSWORD&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MASTER_HOST&quot;,
        ①                        &quot;value&quot;: &quot;mysql-slave.yulibaozi&quot;
                            }
                        ],
                        &quot;image&quot;: &quot;yulibaozi/mysql-slave:201807261706&quot;,
                        &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;,
                        &quot;name&quot;: &quot;mysql-slave&quot;,
                        &quot;ports&quot;: [
                            {
                                &quot;containerPort&quot;: 3306,
                                &quot;protocol&quot;: &quot;TCP&quot;
                            }
                        ],
                        &quot;resources&quot;: {},
                        &quot;terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
                        &quot;terminationMessagePolicy&quot;: &quot;File&quot;,
                        &quot;volumeMounts&quot;: [
                            {
        ②                       &quot;mountPath&quot;: &quot;/var/lib/mysql&quot;,
                                &quot;name&quot;: &quot;data&quot;
                            }
                        ]
                    }
                ],
                &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
                &quot;restartPolicy&quot;: &quot;Always&quot;,
                &quot;schedulerName&quot;: &quot;default-scheduler&quot;,
                &quot;securityContext&quot;: {},
                &quot;terminationGracePeriodSeconds&quot;: 10,
                &quot;volumes&quot;: [
                    {
                        &quot;name&quot;: &quot;data&quot;,
                        &quot;rbd&quot;: {
                            &quot;fsType&quot;: &quot;xfs&quot;,
        ③                   &quot;image&quot;: &quot;k8s_image02&quot;,
                            &quot;keyring&quot;: &quot;/etc/ceph/keyring&quot;,
        ⑥                   &quot;monitors&quot;: [
                                &quot;192.168.0.199:6789&quot;
                            ],
                            &quot;pool&quot;: &quot;rbd&quot;,
                            &quot;secretRef&quot;: {
        ④                       &quot;name&quot;: &quot;ceph-secret&quot;
                            },
                            &quot;user&quot;: &quot;admin&quot;
        ⑤               }
                    }
                ]
            }
        },
        &quot;updateStrategy&quot;: {
            &quot;rollingUpdate&quot;: {
                &quot;partition&quot;: 0
            },
            &quot;type&quot;: &quot;RollingUpdate&quot;
        }
    },
    &quot;status&quot;: {
    }
}

相关解释：
①：如果你的namespace不是yulubaozi,需要把yulibaozi替换成你的命名空间名字
②：需要配置mysql的数据目录，可以选择不修改，默认是它
③：ceph管理分配给你的rbdImage名字，注意:不得与master是同一个名字（同一个名字是指:同一个key，同一个rbdImage名字）
④：secret的名字，secret里面的Key和rbdImage相对应，我这里master和slave使用的相同的key，而rbdImage名字不相同，所以master可以和slave使用一样的secret；如果你的key和rbdImage是不同的两套，那么需要创建两个secret,master一个,slave一个,secret的创建方式在上面提到了。
⑥：monitors:ceph的monitor地址，找ceph管理员要，可以是一个,也可以是多个,可以和master一致也可和master不一致
</code></pre>

<p>上述确认完毕后
执行命令创建slave的sts:</p>

<pre><code>kubectl create -f 3slave-sts.json
</code></pre>

<p>查看sts的pod状态:</p>

<pre><code>[root@kube03 sts]# kubectl get pod -n yulibaozi

NAME             READY     STATUS    RESTARTS   AGE
mysql-master-0   1/1       Running   0          2h
mysql-slave-0    1/1       Running   0          1h

如果等待15分钟不是Running，可能有问题，你可以选择耐心等待下，你可以选择采用和master一样的解决办法。
</code></pre>

<h4 id="创建master的svc">创建master的svc：</h4>

<p>master的svc配置如下：</p>

<pre><code>[root@kube03 sts]# cat 4master-svc.json

{
    &quot;apiVersion&quot;: &quot;v1&quot;,
    &quot;kind&quot;: &quot;Service&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;app&quot;: &quot;mysql-master&quot;
        },
        &quot;name&quot;: &quot;mysql-master&quot;,
        &quot;namespace&quot;: &quot;yulibaozi&quot;
    },
    &quot;spec&quot;: {
        &quot;ports&quot;: [
            {
                &quot;nodePort&quot;: 33306,
                &quot;port&quot;: 3306,
                &quot;protocol&quot;: &quot;TCP&quot;,
                &quot;targetPort&quot;: 3306
            }
        ],
        &quot;selector&quot;: {
            &quot;app&quot;: &quot;mysql-master&quot;
        },
        &quot;sessionAffinity&quot;: &quot;None&quot;,
        &quot;type&quot;: &quot;NodePort&quot;
    },
    &quot;status&quot;: {
        &quot;loadBalancer&quot;: {}
    }
}

如上,我们暴露的NodePort是33306端口，假设我们的宿主机IP:192.168.0.198

</code></pre>

<p>执行命令创建master的svc：</p>

<pre><code>kubectl create -f 4master-svc.json
</code></pre>

<p>访问master数据库验证是否可以连接：</p>

<ol>
<li><p>telnet的方式查看是否能连接</p>

<pre><code>telnet 192.168.0.198 33306 
</code></pre></li>

<li><p>进入mysql</p>

<pre><code>mysql -h 192.168.0.198 -P 33306 -uroot -proot
</code></pre></li>
</ol>

<p>如果能够进入mysql的master节点，执行：</p>

<pre><code>mysql&gt; show master status;

+------------+----------+--------------+------------------+-------------------------------------------+
| File       | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set                         |
+------------+----------+--------------+------------------+-------------------------------------------+
| bin.000005 |      194 |              |                  | 1f14ce9c-b1a8-11e8-becd-02420aa83b04:1-12 |
+------------+----------+--------------+------------------+-------------------------------------------+
1 row in set (0.00 sec)
</code></pre>

<p>一定记住其中的</p>

<pre><code>File
Position
</code></pre>

<p>能走到此处，至少说明master可用。</p>

<h4 id="创建mysql的slave节点">创建mysql的slave节点</h4>

<p>mysql的slave节点配置如下:</p>

<pre><code>[root@kube03 sts]# cat 5slave-svc.json

{
    &quot;apiVersion&quot;: &quot;v1&quot;,
    &quot;kind&quot;: &quot;Service&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;name&quot;: &quot;mysql-slave&quot;
        },
        &quot;name&quot;: &quot;mysql-slave&quot;,
        &quot;namespace&quot;: &quot;yuli&quot;
    },
    &quot;spec&quot;: {
        &quot;ports&quot;: [
            {
                &quot;nodePort&quot;:33307,
                &quot;port&quot;: 3306,
                &quot;protocol&quot;: &quot;TCP&quot;,
                &quot;targetPort&quot;: 3306
            }
        ],
        &quot;selector&quot;: {
            &quot;name&quot;: &quot;mysql-slave&quot;
        },
        &quot;sessionAffinity&quot;: &quot;None&quot;,
        &quot;type&quot;: &quot;NodePort&quot;
    },
    &quot;status&quot;: {
        &quot;loadBalancer&quot;: {}
    }
}

如上,我们暴露的NodePort是33307端口，假设我们的宿主机IP:192.168.0.198
</code></pre>

<p>执行命令创建slave的svc:</p>

<pre><code>kubectl create -f 5slave-svc.json
</code></pre>

<p>验证是否可用的方式和master节点一样</p>

<p>slave配置步骤：
执行命令:</p>

<pre><code>mysql -h 192.168.0.198 -P 33307 -uroot -proot
</code></pre>

<p>在slave的数据库中配置master：</p>

<pre><code>mysql&gt; change master to master_host='192.168.0.198', master_user='root', master_password='root', master_port=33306, master_log_file='bin.000005', master_log_pos=194, master_connect_retry=30;

Query OK, 0 rows affected, 2 warnings (0.19 sec)

</code></pre>

<p>可能你需要修改的是：</p>

<pre><code> master_host  
 master_user
 master_password
 master_port 
 master_log_file 上面让你记住的File
 master_log_pos  上面让你记住的Position
</code></pre>

<p>查看slave的复制状态：</p>

<pre><code>mysql&gt; show slave status \G;

*************************** 1. row ***************************
               Slave_IO_State:
                  Master_Host: 10.151.30.141
                  Master_User: root
                  Master_Port: 33306
                Connect_Retry: 30
              Master_Log_File: bin.000005
          Read_Master_Log_Pos: 194
               Relay_Log_File: rrelay.000001
                Relay_Log_Pos: 4
        Relay_Master_Log_File: bin.000005
             Slave_IO_Running: No
            Slave_SQL_Running: No
                            .
                            .
                            .
1 row in set (0.00 sec)
</code></pre>

<p>以下两个字段一定要注意，关键操作:</p>

<pre><code>  Slave_IO_Running: No
  Slave_SQL_Running: No
  
  这两个结果一定得是YES,如果不是,先不管，向下执行
</code></pre>

<p>执行名字开始主从复制:</p>

<pre><code>mysql&gt; start slave;

ERROR 1872 (HY000): Slave failed to initialize relay log info structure from the repository
</code></pre>

<p>报错了，问题出在mysql的配置文件mysqld.cnf中</p>

<p>执行命令进入slave容器:</p>

<pre><code>kubectl exec -it mysql-slave-0 -n yulibaozi /bin/bash
</code></pre>

<p>进入以下路径修改配置文件：</p>

<pre><code>路径在: /etc/mysql/mysql.conf.d

修改配置:
relay_log=rrelay.log =&gt; relay_log=relay.log
</code></pre>

<p>修改完毕后,执行:</p>

<pre><code>mysql&gt; reset slave;

Query OK, 0 rows affected (0.23 sec)
</code></pre>

<p>再次查看主从状态:</p>

<pre><code>mysql&gt; show slave status \G;
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 10.151.30.141
                  Master_User: root
                  Master_Port: 33306
                Connect_Retry: 30
              Master_Log_File: bin.000005
          Read_Master_Log_Pos: 194
               Relay_Log_File: rrelay.000002
                Relay_Log_Pos: 314
        Relay_Master_Log_File: bin.000005
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
                            .
                            .
                            .
1 row in set (0.00 sec)

</code></pre>

<h4 id="黎明已到">黎明已到</h4>

<p>然后创建数据库，创建表，写入数据，验证主从是否可用，同时删除pod，在启动判断数据是否持久化。基于ceph的mysql主从搭建完毕。</p>
]]></content>
		</item>
		
		<item>
			<title>Go 大杀器之性能剖析 PProf</title>
			<link>https://yulibaozi.com/posts/go/tools/2018-09-15-go-tool-pprof/</link>
			<pubDate>Sat, 15 Sep 2018 12:00:00 +0000</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/tools/2018-09-15-go-tool-pprof/</guid>
			<description>前言 写了几吨代码，实现了几百个接口。功能测试也通过了，终于成功的部署上线了
结果，性能不佳，什么鬼？😭
想做性能分析 PProf 想要进行性能优化，首先瞩目在 Go 自身提供的工具链来作为分析依据，本文将带你学习、使用 Go 后花园，涉及如下：
 runtime/pprof：采集程序（非 Server）的运行数据进行分析 net/http/pprof：采集 HTTP Server 的运行时数据进行分析  是什么 pprof 是用于可视化和分析性能分析数据的工具
pprof 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）
profile.proto 是一个 Protocol Buffer v3 的描述文件，它描述了一组 callstack 和 symbolization 信息， 作用是表示统计分析的一组采样的调用栈，是很常见的 stacktrace 配置文件格式
支持什么使用模式  Report generation：报告生成 Interactive terminal use：交互式终端使用 Web interface：Web 界面  可以做什么  CPU Profiling：CPU 分析，按照一定的频率采集所监听的应用程序 CPU（含寄存器）的使用情况，可确定应用程序在主动消耗 CPU 周期时花费时间的位置 Memory Profiling：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏 Block Profiling：阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置 Mutex Profiling：互斥锁分析，报告互斥锁的竞争情况  一个简单的例子 我们将编写一个简单且有点问题的例子，用于基本的程序初步分析
编写 demo 文件 （1）demo.</description>
			<content type="html"><![CDATA[

<h2 id="前言">前言</h2>

<p>写了几吨代码，实现了几百个接口。功能测试也通过了，终于成功的部署上线了</p>

<p>结果，性能不佳，什么鬼？😭</p>

<h2 id="想做性能分析">想做性能分析</h2>

<h3 id="pprof">PProf</h3>

<p>想要进行性能优化，首先瞩目在 Go 自身提供的工具链来作为分析依据，本文将带你学习、使用 Go 后花园，涉及如下：</p>

<ul>
<li>runtime/pprof：采集程序（非 Server）的运行数据进行分析</li>
<li>net/http/pprof：采集 HTTP Server 的运行时数据进行分析</li>
</ul>

<h3 id="是什么">是什么</h3>

<p>pprof 是用于可视化和分析性能分析数据的工具</p>

<p>pprof 以 <a href="https://github.com/google/pprof/blob/master/proto/profile.proto">profile.proto</a> 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告）</p>

<p>profile.proto 是一个 Protocol Buffer v3 的描述文件，它描述了一组 callstack 和 symbolization 信息， 作用是表示统计分析的一组采样的调用栈，是很常见的 stacktrace 配置文件格式</p>

<h3 id="支持什么使用模式">支持什么使用模式</h3>

<ul>
<li>Report generation：报告生成</li>
<li>Interactive terminal use：交互式终端使用</li>
<li>Web interface：Web 界面</li>
</ul>

<h3 id="可以做什么">可以做什么</h3>

<ul>
<li>CPU Profiling：CPU 分析，按照一定的频率采集所监听的应用程序 CPU（含寄存器）的使用情况，可确定应用程序在主动消耗 CPU 周期时花费时间的位置</li>
<li>Memory Profiling：内存分析，在应用程序进行堆分配时记录堆栈跟踪，用于监视当前和历史内存使用情况，以及检查内存泄漏</li>
<li>Block Profiling：阻塞分析，记录 goroutine 阻塞等待同步（包括定时器通道）的位置</li>
<li>Mutex Profiling：互斥锁分析，报告互斥锁的竞争情况</li>
</ul>

<h2 id="一个简单的例子">一个简单的例子</h2>

<p>我们将编写一个简单且有点问题的例子，用于基本的程序初步分析</p>

<h3 id="编写-demo-文件">编写 demo 文件</h3>

<p>（1）demo.go，文件内容：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kn">package</span> <span class="nx">main</span>

<span class="kn">import</span> <span class="p">(</span>
	<span class="s">&#34;log&#34;</span>
	<span class="s">&#34;net/http&#34;</span>
	<span class="nx">_</span> <span class="s">&#34;net/http/pprof&#34;</span>
	<span class="s">&#34;github.com/EDDYCJY/go-pprof-example/data&#34;</span>
<span class="p">)</span>

<span class="kd">func</span> <span class="nf">main</span><span class="p">()</span> <span class="p">{</span>
	<span class="k">go</span> <span class="kd">func</span><span class="p">()</span> <span class="p">{</span>
		<span class="k">for</span> <span class="p">{</span>
			<span class="nx">log</span><span class="p">.</span><span class="nf">Println</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nf">Add</span><span class="p">(</span><span class="s">&#34;https://github.com/EDDYCJY&#34;</span><span class="p">))</span>
		<span class="p">}</span>
	<span class="p">}()</span>

	<span class="nx">http</span><span class="p">.</span><span class="nf">ListenAndServe</span><span class="p">(</span><span class="s">&#34;0.0.0.0:6060&#34;</span><span class="p">,</span> <span class="kc">nil</span><span class="p">)</span>
<span class="p">}</span></code></pre></div>
<p>（2）data/d.go，文件内容：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kn">package</span> <span class="nx">data</span>

<span class="kd">var</span> <span class="nx">datas</span> <span class="p">[]</span><span class="kt">string</span>

<span class="kd">func</span> <span class="nf">Add</span><span class="p">(</span><span class="nx">str</span> <span class="kt">string</span><span class="p">)</span> <span class="kt">string</span> <span class="p">{</span>
	<span class="nx">data</span> <span class="o">:=</span> <span class="p">[]</span><span class="nb">byte</span><span class="p">(</span><span class="nx">str</span><span class="p">)</span>
	<span class="nx">sData</span> <span class="o">:=</span> <span class="nb">string</span><span class="p">(</span><span class="nx">data</span><span class="p">)</span>
	<span class="nx">datas</span> <span class="p">=</span> <span class="nb">append</span><span class="p">(</span><span class="nx">datas</span><span class="p">,</span> <span class="nx">sData</span><span class="p">)</span>

	<span class="k">return</span> <span class="nx">sData</span>
<span class="p">}</span></code></pre></div>
<p>运行这个文件，你的 HTTP 服务会多出 /debug/pprof 的 endpoint 可用于观察应用程序的情况</p>

<h3 id="分析">分析</h3>

<h4 id="一-通过-web-界面">一、通过 Web 界面</h4>

<p>查看当前总览：访问 <code>http://127.0.0.1:6060/debug/pprof/</code></p>

<pre><code>/debug/pprof/

profiles:
0	block
5	goroutine
3	heap
0	mutex
9	threadcreate

full goroutine stack dump
</code></pre>

<p>这个页面中有许多子页面，咱们继续深究下去，看看可以得到什么？</p>

<ul>
<li>cpu（CPU Profiling）: <code>$HOST/debug/pprof/profile</code>，默认进行 30s 的 CPU Profiling，得到一个分析用的 profile 文件</li>
<li>block（Block Profiling）：<code>$HOST/debug/pprof/block</code>，查看导致阻塞同步的堆栈跟踪</li>
<li>goroutine：<code>$HOST/debug/pprof/goroutine</code>，查看当前所有运行的 goroutines 堆栈跟踪</li>
<li>heap（Memory Profiling）: <code>$HOST/debug/pprof/heap</code>，查看活动对象的内存分配情况</li>
<li>mutex（Mutex Profiling）：<code>$HOST/debug/pprof/mutex</code>，查看导致互斥锁的竞争持有者的堆栈跟踪</li>
<li>threadcreate：<code>$HOST/debug/pprof/threadcreate</code>，查看创建新 OS 线程的堆栈跟踪</li>
</ul>

<h4 id="二-通过交互式终端使用">二、通过交互式终端使用</h4>

<p>（1）go tool pprof <a href="http://localhost:6060/debug/pprof/profile?seconds=60">http://localhost:6060/debug/pprof/profile?seconds=60</a></p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ go tool pprof http://localhost:6060/debug/pprof/profile<span class="se">\?</span>seconds<span class="se">\=</span><span class="m">60</span>

Fetching profile over HTTP from http://localhost:6060/debug/pprof/profile?seconds<span class="o">=</span><span class="m">60</span>
Saved profile in /Users/eddycjy/pprof/pprof.samples.cpu.007.pb.gz
Type: cpu
Duration: 1mins, Total <span class="nv">samples</span> <span class="o">=</span> <span class="m">26</span>.55s <span class="o">(</span><span class="m">44</span>.15%<span class="o">)</span>
Entering interactive mode <span class="o">(</span><span class="nb">type</span> <span class="s2">&#34;help&#34;</span> <span class="k">for</span> commands, <span class="s2">&#34;o&#34;</span> <span class="k">for</span> options<span class="o">)</span>
<span class="o">(</span>pprof<span class="o">)</span></code></pre></div>
<p>执行该命令后，需等待 60 秒（可调整 seconds 的值），pprof 会进行 CPU Profiling。结束后将默认进入 pprof 的交互式命令模式，可以对分析的结果进行查看或导出。具体可执行 <code>pprof help</code> 查看命令说明</p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh"><span class="o">(</span>pprof<span class="o">)</span> top10
Showing nodes accounting <span class="k">for</span> <span class="m">25</span>.92s, <span class="m">97</span>.63% of <span class="m">26</span>.55s total
Dropped <span class="m">85</span> nodes <span class="o">(</span>cum &lt;<span class="o">=</span> <span class="m">0</span>.13s<span class="o">)</span>
Showing top <span class="m">10</span> nodes out of <span class="m">21</span>
      flat  flat%   sum%        cum   cum%
    <span class="m">23</span>.28s <span class="m">87</span>.68% <span class="m">87</span>.68%     <span class="m">23</span>.29s <span class="m">87</span>.72%  syscall.Syscall
     <span class="m">0</span>.77s  <span class="m">2</span>.90% <span class="m">90</span>.58%      <span class="m">0</span>.77s  <span class="m">2</span>.90%  runtime.memmove
     <span class="m">0</span>.58s  <span class="m">2</span>.18% <span class="m">92</span>.77%      <span class="m">0</span>.58s  <span class="m">2</span>.18%  runtime.freedefer
     <span class="m">0</span>.53s  <span class="m">2</span>.00% <span class="m">94</span>.76%      <span class="m">1</span>.42s  <span class="m">5</span>.35%  runtime.scanobject
     <span class="m">0</span>.36s  <span class="m">1</span>.36% <span class="m">96</span>.12%      <span class="m">0</span>.39s  <span class="m">1</span>.47%  runtime.heapBitsForObject
     <span class="m">0</span>.35s  <span class="m">1</span>.32% <span class="m">97</span>.44%      <span class="m">0</span>.45s  <span class="m">1</span>.69%  runtime.greyobject
     <span class="m">0</span>.02s <span class="m">0</span>.075% <span class="m">97</span>.51%     <span class="m">24</span>.96s <span class="m">94</span>.01%  main.main.func1
     <span class="m">0</span>.01s <span class="m">0</span>.038% <span class="m">97</span>.55%     <span class="m">23</span>.91s <span class="m">90</span>.06%  os.<span class="o">(</span>*File<span class="o">)</span>.Write
     <span class="m">0</span>.01s <span class="m">0</span>.038% <span class="m">97</span>.59%      <span class="m">0</span>.19s  <span class="m">0</span>.72%  runtime.mallocgc
     <span class="m">0</span>.01s <span class="m">0</span>.038% <span class="m">97</span>.63%     <span class="m">23</span>.30s <span class="m">87</span>.76%  syscall.Write</code></pre></div>
<ul>
<li>flat：给定函数上运行耗时</li>
<li>flat%：同上的 CPU 运行耗时总比例</li>
<li>sum%：给定函数累积使用 CPU 总比例</li>
<li>cum：当前函数加上它之上的调用运行总耗时</li>
<li>cum%：同上的 CPU 运行耗时总比例</li>
</ul>

<p>最后一列为函数名称，在大多数的情况下，我们可以通过这五列得出一个应用程序的运行情况，加以优化 🤔</p>

<p>（2）go tool pprof <a href="http://localhost:6060/debug/pprof/heap">http://localhost:6060/debug/pprof/heap</a></p>
<div class="highlight"><pre class="chroma"><code class="language-sh" data-lang="sh">$ go tool pprof http://localhost:6060/debug/pprof/heap
Fetching profile over HTTP from http://localhost:6060/debug/pprof/heap
Saved profile in /Users/eddycjy/pprof/pprof.alloc_objects.alloc_space.inuse_objects.inuse_space.008.pb.gz
Type: inuse_space
Entering interactive mode <span class="o">(</span><span class="nb">type</span> <span class="s2">&#34;help&#34;</span> <span class="k">for</span> commands, <span class="s2">&#34;o&#34;</span> <span class="k">for</span> options<span class="o">)</span>
<span class="o">(</span>pprof<span class="o">)</span> top
Showing nodes accounting <span class="k">for</span> <span class="m">837</span>.48MB, <span class="m">100</span>% of <span class="m">837</span>.48MB total
      flat  flat%   sum%        cum   cum%
  <span class="m">837</span>.48MB   <span class="m">100</span>%   <span class="m">100</span>%   <span class="m">837</span>.48MB   <span class="m">100</span>%  main.main.func1</code></pre></div>
<ul>
<li><p>-inuse_space：分析应用程序的常驻内存占用情况</p></li>

<li><p>-alloc_objects：分析应用程序的内存临时分配情况</p></li>
</ul>

<p>（3） go tool pprof <a href="http://localhost:6060/debug/pprof/block">http://localhost:6060/debug/pprof/block</a></p>

<p>（4） go tool pprof <a href="http://localhost:6060/debug/pprof/mutex">http://localhost:6060/debug/pprof/mutex</a></p>

<h4 id="三-pprof-可视化界面">三、PProf 可视化界面</h4>

<p>这是令人期待的一小节。在这之前，我们需要简单的编写好测试用例来跑一下</p>

<h5 id="编写测试用例">编写测试用例</h5>

<p>（1）新建 data/d_test.go，文件内容：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kn">package</span> <span class="nx">data</span>

<span class="kn">import</span> <span class="s">&#34;testing&#34;</span>

<span class="kd">const</span> <span class="nx">url</span> <span class="p">=</span> <span class="s">&#34;https://github.com/EDDYCJY&#34;</span>

<span class="kd">func</span> <span class="nf">TestAdd</span><span class="p">(</span><span class="nx">t</span> <span class="o">*</span><span class="nx">testing</span><span class="p">.</span><span class="nx">T</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">s</span> <span class="o">:=</span> <span class="nf">Add</span><span class="p">(</span><span class="nx">url</span><span class="p">)</span>
	<span class="k">if</span> <span class="nx">s</span> <span class="o">==</span> <span class="s">&#34;&#34;</span> <span class="p">{</span>
		<span class="nx">t</span><span class="p">.</span><span class="nf">Errorf</span><span class="p">(</span><span class="s">&#34;Test.Add error!&#34;</span><span class="p">)</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nf">BenchmarkAdd</span><span class="p">(</span><span class="nx">b</span> <span class="o">*</span><span class="nx">testing</span><span class="p">.</span><span class="nx">B</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="p">&lt;</span> <span class="nx">b</span><span class="p">.</span><span class="nx">N</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span> <span class="p">{</span>
		<span class="nf">Add</span><span class="p">(</span><span class="nx">url</span><span class="p">)</span>
	<span class="p">}</span>
<span class="p">}</span></code></pre></div>
<p>（2）执行测试用例</p>

<pre><code>$ go test -bench=. -cpuprofile=cpu.prof
pkg: github.com/EDDYCJY/go-pprof-example/data
BenchmarkAdd-4   	10000000	       187 ns/op
PASS
ok  	github.com/EDDYCJY/go-pprof-example/data	2.300s
</code></pre>

<p>-memprofile 也可以了解一下</p>

<h5 id="启动-pprof-可视化界面">启动 PProf 可视化界面</h5>

<h6 id="方法一">方法一：</h6>

<pre><code>$ go tool pprof -http=:8080 cpu.prof
</code></pre>

<h6 id="方法二">方法二：</h6>

<pre><code>$ go tool pprof cpu.prof
$ (pprof) web
</code></pre>

<p>如果出现 <code>Could not execute dot; may need to install graphviz.</code>，就是提示你要安装 <code>graphviz</code> 了 （请右拐谷歌）</p>

<h5 id="查看-pprof-可视化界面">查看 PProf 可视化界面</h5>

<p>（1）Top</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1xlsYD.jpg" alt="image" /></p>

<p>（2）Graph</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1xlgld.jpg" alt="image" /></p>

<p>框越大，线越粗代表它占用的时间越大哦</p>

<p>（3）Peek</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1xlROI.jpg" alt="image" /></p>

<p>（4）Source</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1xl4Tf.jpg" alt="image" /></p>

<p>通过 PProf 的可视化界面，我们能够更方便、更直观的看到 Go 应用程序的调用链、使用情况等，并且在 View 菜单栏中，还支持如上多种方式的切换</p>

<p>你想想，在烦恼不知道什么问题的时候，能用这些辅助工具来检测问题，是不是瞬间效率翻倍了呢 👌</p>

<h4 id="四-pprof-火焰图">四、PProf 火焰图</h4>

<p>另一种可视化数据的方法是火焰图，需手动安装原生 PProf 工具：</p>

<p>（1） 安装 PProf</p>

<pre><code>$ go get -u github.com/google/pprof
</code></pre>

<p>（2） 启动 PProf 可视化界面:</p>

<pre><code>$ pprof -http=:8080 cpu.prof
</code></pre>

<p>（3） 查看 PProf 可视化界面</p>

<p>打开 PProf 的可视化界面时，你会明显发现比官方工具链的 PProf 精致一些，并且多了 Flame Graph（火焰图）</p>

<p>它就是本次的目标之一，它的最大优点是动态的。调用顺序由上到下（A -&gt; B -&gt; C -&gt; D），每一块代表一个函数，越大代表占用 CPU 的时间更长。同时它也支持点击块深入进行分析！</p>

<p><img src="https://s2.ax1x.com/2020/02/15/1xlj00.jpg" alt="image" /></p>

<h2 id="总结">总结</h2>

<p>在本章节，粗略地介绍了 Go 的性能利器 PProf。在特定的场景中，PProf 给定位、剖析问题带了极大的帮助</p>

<p>希望本文对你有所帮助，另外建议能够自己实际操作一遍，最好是可以深入琢磨一下，内含大量的用法、知识点 🤓</p>

<h2 id="思考题">思考题</h2>

<p>你很优秀的看到了最后，那么有两道简单的思考题，希望拓展你的思路</p>

<p>（1）flat 一定大于 cum 吗，为什么？什么场景下 cum 会比 flat 大？</p>

<p>（2）本章节的 demo 代码，有什么性能问题？怎么解决它？</p>
]]></content>
		</item>
		
	</channel>
</rss>

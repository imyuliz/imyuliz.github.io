<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Posts on yulibaozi</title>
		<link>https://yulibaozi.com/posts/</link>
		<description>Recent content in Posts on yulibaozi</description>
		<generator>Hugo -- gohugo.io</generator>
		<language>zh-hans</language>
		<copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
		<lastBuildDate>Sat, 25 Apr 2020 20:04:44 +0800</lastBuildDate>
		<atom:link href="https://yulibaozi.com/posts/index.xml" rel="self" type="application/rss+xml" />
		
		<item>
			<title>.Net 配置文件在Kubernetes中无法自动加载</title>
			<link>https://yulibaozi.com/posts/kubernetes/knowledge/2020-04-25-dotnet-configuration-auto-reload/</link>
			<pubDate>Sat, 25 Apr 2020 20:04:44 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/knowledge/2020-04-25-dotnet-configuration-auto-reload/</guid>
			<description>问题描述 在使用 kubernetes 部署应用时, 我使用 kubernetes 的 configmap 来管理配置文件: appsettings.json , 修改configmap 的配置文件后, 我来到了容器里, 通过 cat /app/config/appsetting.json 命令查看容器是否已经加载了最新的配置文件, 很幸运的是, 通过命令行查看容器配置发现已经处于最新状态(修改configmap后10-15s 生效), 我尝试请求应用的API, 发现API 在执行过程中使用的配置是老旧的内容, 而不是最新的内容。在本地执行应用时并未出现配置无法热更新的问题。
# 相关版本 kubernetes 版本: 1.14.2 .Net core: 3.1  问题猜想 通过命令行排查发现最新的 configmap 配置内容已经在容器的指定目录上更新到最新，但是应用仍然使用老旧的配置内容, 这意味着问题发生在: configmap-&amp;gt;容器-&amp;gt;应用, 容器和应用之间, 容器指定目录下的配置更新并没有触发 .Net 热加载机制, 那究竟是为什么没有触发配置热加载,需要深挖根本原因, 直觉猜想是: 查看 .Net Core 标准库的配置热加载的实现检查触发条件, 很有可能是触发的条件不满足导致应用配置无法重新加载。
问题排查 猜想方向是热更新的触发条件不满足, 我们熟知使用 configmap 挂载文件是使用symlink来挂载, 而非常用的物理文件系统, 在修改完 configmap , 容器重新加载配置后,这一过程并不会改变文件的修改时间等信息(从容器的角度看)。对此，我们做了一个实验,通过对比configmap修改前和修改后来观察配置( appsettings.json )在容器的属性变化(注: 均在容器加载最新配置后对比), 使用 stat 命令来佐证了这个细节点。
 BeforeAfter     root@app-785bc59df6-gdmnf:/app/Config# stat appsettings.</description>
			<content type="html"><![CDATA[

<h2 id="问题描述">问题描述</h2>

<p>在使用 kubernetes 部署应用时, 我使用 <code>kubernetes</code> 的 <code>configmap</code> 来管理配置文件: <code>appsettings.json</code>
,  修改configmap 的配置文件后, 我来到了容器里, 通过 <code>cat /app/config/appsetting.json</code> 命令查看容器是否已经加载了最新的配置文件, 很幸运的是, 通过命令行查看容器配置发现已经处于最新状态(修改configmap后10-15s 生效),  我尝试请求应用的API, 发现API 在执行过程中使用的配置是老旧的内容, 而不是最新的内容。在本地执行应用时并未出现配置无法热更新的问题。</p>

<pre><code># 相关版本
kubernetes 版本: 1.14.2
.Net core: 3.1
</code></pre>

<h2 id="问题猜想">问题猜想</h2>

<p>通过命令行排查发现最新的 <code>configmap</code> 配置内容已经在容器的指定目录上更新到最新，但是应用仍然使用老旧的配置内容, 这意味着问题发生在: configmap-&gt;<strong>容器-&gt;应用</strong>, 容器和应用之间, 容器指定目录下的配置更新并没有触发 <code>.Net</code> 热加载机制,  那究竟是为什么没有触发配置热加载,需要深挖根本原因, 直觉猜想是: 查看 <code>.Net Core</code> 标准库的配置热加载的实现检查触发条件, 很有可能是触发的条件不满足导致应用配置无法重新加载。</p>

<h2 id="问题排查">问题排查</h2>

<p>猜想方向是热更新的触发条件不满足, 我们熟知使用 <code>configmap</code> 挂载文件是使用<a href="https://en.wikipedia.org/wiki/Symbolic_link">symlink</a>来挂载, 而非常用的物理文件系统, 在修改完 <code>configmap</code> , 容器重新加载配置后,这一过程并不会改变文件的修改时间等信息(从容器的角度看)。对此，我们做了一个实验,通过对比configmap修改前和修改后来观察配置( <code>appsettings.json</code> )在容器的属性变化(注: 均在容器加载最新配置后对比), 使用 <code>stat</code>  命令来佐证了这个细节点。</p>

<table>
<thead><tr><th>Before</th><th>After</th></tr></thead>
<tbody>
<tr><td>
<pre>
<code>
root@app-785bc59df6-gdmnf:/app/Config# stat appsettings.json
File: Config/appsettings.json -> ..data/appsettings.json
 Size: 35           Blocks: 0          IO Block: 4096   symbolic link
Device: ca01h/51713d    Inode: 27263079    Links: 1
Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2020-04-25 08:21:18.490453316 +0000
Modify: 2020-04-25 08:21:18.490453316 +0000
Change: 2020-04-25 08:21:18.490453316 +0000
Birth: -
</code>
</pre>

</td><td>

<pre>
<code>
root@app-785bc59df6-gdmnf:/app/Config# stat appsettings.json
 File: appsettings.json -> ..data/appsettings.json
 Size: 35           Blocks: 0          IO Block: 4096   symbolic link
Device: ca01h/51713d    Inode: 27263079    Links: 1
Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2020-04-25 08:21:18.490453316 +0000
Modify: 2020-04-25 08:21:18.490453316 +0000
Change: 2020-04-25 08:21:18.490453316 +0000
Birth: -
</pre>
</code>
</td></tr>
</tbody></table>

<p>通过标准库源码发现, <code>.Net core</code> 配置热更新机制似乎是基于文件的最后修改日期来触发的, 根据上面的前后对比显而易见,  <code>configmap</code> 的修改并没有让容器里的指定的文件的最后修改日期改变，也就未触发 <code>.Net</code> 应用配置的热加载。</p>

<h2 id="解决办法">解决办法</h2>

<p>既然猜想基本得到证实, 由于不太熟悉这门语言, 我们尝试在网络上寻找解决办法，很幸运的是我们找到了找到了相关的内容, <a href="https://github.com/fbeltrao">fbeltrao</a> 开源了一个第三方库(<a href="https://github.com/fbeltrao/ConfigMapFileProvider">ConfigMapFileProvider</a>) 来专门解决这个问题，<strong>通过监听文件内容hash值的变化实现配置热加载</strong>。
于是, 我们在修改了项目的代码:</p>

<table>
<thead><tr><th>Before</th><th>After</th></tr></thead>
<tbody>
<tr><td>

<pre>
<code>
// 配置被放在了/app/Config/ 目录下
var configPath = Path.Combine(env.ContentRootPath, "Config");
config.AddJsonFile(Path.Combine(configPath, "appsettings.json"), 
                                optional: false, 
                                reloadOnChange: true);
</pre>
</code>
</td><td>

<pre>
<code>
// 配置被放在了/app/Config/ 目录下
config.AddJsonFile(ConfigMapFileProvider.FromRelativePath("Config"),
                        "appsettings.json",
                        optional: false,
                        reloadOnChange: true);
</pre>
</code>

</td></tr>
</tbody></table>

<p>修改完项目的代码后, 重新构建镜像, 更新部署在 <code>kubernetes</code> 上的应用, 然后再次测试, 到此为止, 会出现两种状态:</p>

<ol>
<li>一种是你热加载配置完全可用, 非常值得祝贺, 你已经成功修复了这个bug;</li>
<li>一种是你的热加载配置功能还存在 bug, 比如: 上一次请求, 配置仍然使用的老旧配置内容, 下一次请求却使用了最新的配置内容,这个时候, 我们需要继续向下排查: <code>.NET Core</code> 引入了<code>Options</code>模式，使用类来表示相关的设置组,用强类型的类来表达配置项(白话大概表述为: 代码里面有个对象对应配置里的某个字段, 配置里对应的字段更改会触发代码里对象的属性变化), 示例如下:</li>
</ol>

<table>
<thead><tr><th>配置示例</th><th>代码示例</th></tr></thead>
<tbody>
<tr><td>

<pre>
<code>
cat appsettings.json
 "JwtIssuerOptions": {
    "Issuer": "test",
    "Audience": "test",
    "SecretKey": "test"
    ...
  }
</pre>
</code>
</td><td>

<pre>
<code>
services.Configure<JwtIssuerOptions>(Configuration.GetSection("JwtIssuerOptions"));
</pre>
</code>

</td></tr>
</tbody></table>

<p>而 Options 模式分为三种:</p>

<ol>
<li><code>IOptions</code>: Singleton(单例)，值一旦生成, 除非通过代码的方式更改，否则它的值不会更新</li>
<li><code>IOptionsMonitor</code>: Singleton(单例), 通过IOptionsChangeTokenSource&lt;&gt; 能够和配置文件一起更新，也能通过代码的方式更改值</li>
<li><code>IOptionsSnapshot</code>: Scoped，配置文件更新的下一次访问，它的值会更新，但是它不能跨范围通过代码的方式更改值，只能在当前范围（请求）内有效。</li>
</ol>

<p>在知道这三种模式的意义后，我们已经完全找到了问题的根因, 把 <code>Options</code> 模式设置为:<code>IOptionsMonitor</code>就能解决完全解决配置热加载的问题。</p>

<h2 id="相关链接">相关链接</h2>

<ol>
<li><p><a href="https://github.com/fbeltrao/ConfigMapFileProvider">配置监听ConfigMapFileProvider</a></p></li>

<li><p><a href="https://github.com/dotnet/extensions/issues/1175">相似的Issue: 1175</a></p></li>

<li><p><a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration/options?view=aspnetcore-3.1">官方Options 描述</a></p></li>

<li><p><a href="https://www.cnblogs.com/wenhx/p/ioptions-ioptionsmonitor-and-ioptionssnapshot.html">IOptions、IOptionsMonitor以及IOptionsSnapshot 测试</a></p></li>
</ol>
]]></content>
		</item>
		
		<item>
			<title>Kubernetes高级调度- Taint和Toleration、Node Affinity分析</title>
			<link>https://yulibaozi.com/posts/kubernetes/knowledge/2019-06-24-advanced-scheduling/</link>
			<pubDate>Tue, 21 Apr 2020 20:47:34 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/knowledge/2019-06-24-advanced-scheduling/</guid>
			<description>Kubernetes高级调度- Taint和Toleration、Node Affinity分析 节点即Node
一、Taint和Toleration 污点的理论支撑 1.污点设置有哪些影响效果 使用效果(Effect):
 PreferNoSchedule:调度器尽量避免把Pod调度到具有该污点效果的节点上,如果不能避免(如其他节点资源不足),Pod也能调度到这个污点节点上。 NoSchedule:不容忍该污点效果的Pod永不会被调度到该节点上，通过kubelet管理的Pod(static Pod)不受限制;之前没有设置污点的Pod如果已运行在此节点(有污点的节点)上，可以继续运行。 NoExecute: 调度器不会把Pod调度到具有该污点效果的节点上，同时会将节点上已存在的Pod驱逐出去。  污点设置的第一前提是: 节点上的污点key和Pod上的污点容忍key相匹配。
2. 设置污点的效果实测 当Pod未设置污点容忍而节点设置了污点时  当节点的污点影响效果被设置为:PreferNoSchedule时,已存在于此节点上的Pod不会被驱逐；新增但未设置污点容忍的Pod仍然可以被调度到此节点上。 当节点的污点影响效果被设置为:NoSchedule时,已存在于此节点上的Pod不会被驱逐;同时,新增的Pod不会被调度此节点上. 当节点的污点影响效果被设置为:NoExecute时,已存在于此节点上的Pod会发生驱逐(驱逐时间由tolerationSeconds字段确定,小于等于0会立即驱逐);新增的Pod不会调度到此节点上。  以上所说的Pod未设置任何的污点容忍时
当Node设置了污点且Pod设置了对应的污点容忍时    Pod容忍效果 Node污点效果 是否会调度到此Node上 原因     PreferNoSchedule PreferNoSchedule √ Node的污点效果和Pod的容忍效果相匹配   PreferNoSchedule NoSchedule x Node的污点效果高于Pod的容忍效果   PreferNoSchedule NoExecute x Node的污点效果高于Pod的容忍效果   NoSchedule PreferNoSchedule √ Node的污点效果低于Pod的容忍效果   NoSchedule NoSchedule √ Node的污点效果和Pod的容忍效果相匹配   NoSchedule NoExecute x Node的污点效果高于Pod设置的容忍效果   NoExecute PreferNoSchedule √ Node的污点效果低于Pod的容忍效果   NoExecute NoSchedule x Node的污点效果和Pod的容忍程度互斥   NoExecute NoExecute x Pod在Node上不断重建和杀掉。    污点容忍设置, Exists和Equal这两个操作符之间的区别是什么?</description>
			<content type="html"><![CDATA[

<h2 id="kubernetes高级调度-taint和toleration-node-affinity分析">Kubernetes高级调度- Taint和Toleration、Node Affinity分析</h2>

<p>节点即Node</p>

<h2 id="一-taint和toleration">一、Taint和Toleration</h2>

<h3 id="污点的理论支撑">污点的理论支撑</h3>

<h4 id="1-污点设置有哪些影响效果">1.污点设置有哪些影响效果</h4>

<p>使用效果(Effect):</p>

<ol>
<li>PreferNoSchedule:调度器尽量避免把Pod调度到具有该污点效果的节点上,如果不能避免(如其他节点资源不足),Pod也能调度到这个污点节点上。</li>
<li>NoSchedule:不容忍该污点效果的Pod永不会被调度到该节点上，通过kubelet管理的Pod(static Pod)不受限制;之前没有设置污点的Pod如果已运行在此节点(有污点的节点)上，可以继续运行。</li>
<li>NoExecute: 调度器不会把Pod调度到具有该污点效果的节点上，同时会将节点上已存在的Pod驱逐出去。</li>
</ol>

<p>污点设置的第一前提是: 节点上的污点key和Pod上的污点容忍key相匹配。</p>

<h3 id="2-设置污点的效果实测">2. 设置污点的效果实测</h3>

<h4 id="当pod未设置污点容忍而节点设置了污点时">当Pod未设置污点容忍而节点设置了污点时</h4>

<ol>
<li>当节点的污点影响效果被设置为:PreferNoSchedule时,已存在于此节点上的Pod不会被驱逐；新增但未设置污点容忍的Pod仍然可以被调度到此节点上。</li>
<li>当节点的污点影响效果被设置为:NoSchedule时,已存在于此节点上的Pod不会被驱逐;同时,新增的Pod不会被调度此节点上.</li>
<li>当节点的污点影响效果被设置为:NoExecute时,已存在于此节点上的Pod会发生驱逐(驱逐时间由tolerationSeconds字段确定,小于等于0会立即驱逐);新增的Pod不会调度到此节点上。</li>
</ol>

<p><strong>以上所说的Pod未设置任何的污点容忍时</strong></p>

<h4 id="当node设置了污点且pod设置了对应的污点容忍时">当Node设置了污点且Pod设置了对应的污点容忍时</h4>

<table>
<thead>
<tr>
<th>Pod容忍效果</th>
<th>Node污点效果</th>
<th>是否会调度到此Node上</th>
<th>原因</th>
</tr>
</thead>

<tbody>
<tr>
<td>PreferNoSchedule</td>
<td>PreferNoSchedule</td>
<td>√</td>
<td>Node的污点效果和Pod的容忍效果相匹配</td>
</tr>

<tr>
<td>PreferNoSchedule</td>
<td>NoSchedule</td>
<td>x</td>
<td>Node的污点效果高于Pod的容忍效果</td>
</tr>

<tr>
<td>PreferNoSchedule</td>
<td>NoExecute</td>
<td>x</td>
<td>Node的污点效果高于Pod的容忍效果</td>
</tr>

<tr>
<td>NoSchedule</td>
<td>PreferNoSchedule</td>
<td>√</td>
<td>Node的污点效果低于Pod的容忍效果</td>
</tr>

<tr>
<td>NoSchedule</td>
<td>NoSchedule</td>
<td>√</td>
<td>Node的污点效果和Pod的容忍效果相匹配</td>
</tr>

<tr>
<td>NoSchedule</td>
<td>NoExecute</td>
<td>x</td>
<td>Node的污点效果高于Pod设置的容忍效果</td>
</tr>

<tr>
<td>NoExecute</td>
<td>PreferNoSchedule</td>
<td>√</td>
<td>Node的污点效果低于Pod的容忍效果</td>
</tr>

<tr>
<td>NoExecute</td>
<td>NoSchedule</td>
<td>x</td>
<td>Node的污点效果和Pod的容忍程度互斥</td>
</tr>

<tr>
<td>NoExecute</td>
<td>NoExecute</td>
<td>x</td>
<td>Pod在Node上不断重建和杀掉。</td>
</tr>
</tbody>
</table>

<h4 id="污点容忍设置-exists和equal这两个操作符之间的区别是什么">污点容忍设置, Exists和Equal这两个操作符之间的区别是什么?</h4>

<p>在配置上:</p>

<p>Exists必须把值设置为空字符串，而只关心key与节点的污点key是否匹配。</p>

<p>Equal需要同时设置key和value来匹配污点节点的Key和value。</p>

<p>两者之间的理解加深:
1. 若一个节点存在多个污点, Pod使用Exists只容忍了其中一个污点, 仍然不能调度此节点, 原因在于Pod不容忍此节点的其他污点。
2. 若一个节点存在多个污点, Pod使用Equal只容忍了其中一个污点, 仍然不能调度此节点, 原因在于Pod还是不容忍此节点的其他污点。
3. 若想要一个Pod能够调度到含有多个污点的节点上, 那么此Pod需要容忍此节点的所有污点。</p>

<h3 id="3-污点容忍里的一些小窍门">3.污点容忍里的一些小窍门:</h3>

<ol>
<li><p>在污点容忍设置时,若key,value是空字符且操作符是Exists 那么能Pod容忍节点的所有污点。(注意:仍然遵从于容忍效果的等级设置)。</p>

<pre><code>例如:一个Pod在设置污点容忍时，key,value为空且操作符是Exists,容忍效果为:NoExecute,那么该Pod不会调度到污点效果为:NoSchedule的节点上。
</code></pre></li>

<li><p>在设置污点容忍时, 若Pod的容忍效果(effect)被设置为空字符,那么Pod能匹配所有的容忍效果。</p></li>

<li><p>在设置污点容忍时, 若key,value是空字符、操作符是Exists且容忍效果(effect)也为空时，则等于没有设置。</p></li>

<li><p>默认情况下，操作符是Equal。</p></li>

<li><p>如果节点的影响效果是NoExecute,且不想Pod被<strong>立即</strong>驱逐,那么可以设置TolerationSeconds(延迟驱逐时间),若值是0或者负数会立即驱逐,若值大于0,则在此时间后开始驱逐。</p></li>

<li><p>从测试结果来看，只要节点设置了污点且效果是:NoExecute,不管Pod是否容忍了该污点都不能在对应节点上正常运行(一直处于删除，重建的过程),原因是能被调度到节点上是调度器选择的结果，Pod被杀掉是本地kubelet的结果，这是两个组件分管不同工作产生的效果,下面这种配置除外。</p>

<pre><code>  tolerations:
  - operator: Exists

#此Pod的污点配置能够容忍所有的污点，所有的影响效果，所有能调度到所有的节点上(包括影响效果被设置为:NoExecute的Node).
</code></pre></li>
</ol>

<h3 id="4-认知误区">4. 认知误区</h3>

<ol>
<li>当一个节点设置了污点，那么只要设置Pod对此污点容忍就能调度上去且能正常运行。</li>
</ol>

<p>错误。当节点的一个污点的影响效果被设置为:NoExecute,此时Pod对此污点的容忍效果也是NoExecute时, Pod能调度上去，但是也会被Terminating，不断的处于Terminating,ContainerCreating的过程。</p>

<p>对Node设置污点:</p>

<pre><code>kubectl taint nodes 1xx status=unavailable:NoExecute
</code></pre>

<p>Pod设置的污点容忍:</p>

<pre><code>      tolerations:
      - effect: NoExecute
        key: status
        operator: Equal
        tolerationSeconds: 0
        value: unavailable
</code></pre>

<p>效果:</p>

<pre><code>Events:
  Type     Reason            Age               From                    Message
  ----     ------            ----              ----                    -------
  Warning  FailedScheduling  1m (x25 over 1m)  default-scheduler       0/18 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 17 Insufficient memory.
</code></pre>

<ol>
<li>当一个节点设置了多个污点，只要使用Exists操作符匹配到其中一个污点，此Pod就能调度到对应的节点上。</li>
</ol>

<p>错误。原因在于Pod只能匹配到其中一个污点,但是还是不能匹配到其他污点。所以，不能调度上去。</p>

<ol>
<li>在设置Pod容忍时，只要匹配到key和value就行了,不用关心容忍效果的设置。</li>
</ol>

<p>错误。容忍效果的设置和key/value的设置一样重要，甚至更加重要。如果容忍效果不匹配。也会导致Pod调度不能到对应节点上。</p>

<ol>
<li>如果Pod没有设置任何的污点容忍,Pod就不能调度到有污点的节点上。</li>
</ol>

<p>错误。如果节点的污染效果是: PreferNoSchedule, 没有设置任何污点容忍的Pod也能调度到此节点上。原因在于:PreferNoSchedule的意思是优先不调度,但是当没有节点可用时,Pod仍然能调度到此节点。</p>

<h2 id="二-node-affinity">二、Node Affinity</h2>

<p>Node Affinity可以让指定应用调度到指定的节点,这有利于应用的稳定性,减少重要业务和不重要业务之间相互抢占资源的可能,同时也可以降低不重要业务对重要业务的影响,另一方面,也能够进行多租户之间的隔离。根据租户需求为租户提供特定的运行环境。</p>

<h3 id="nodeaffinity配置要点">NodeAffinity配置要点:</h3>

<p>NodeAffinity配置分类两大部分:</p>

<ol>
<li>requiredDuringSchedulingIgnoredDuringExecution (强亲和性)</li>
<li>preferredDuringSchedulingIgnoredDuringExecution (首选亲和性)</li>
</ol>

<p>但是,在真实的配置环节时,又会犯迷糊:
  1. 强亲和性到底多强算强?
  2. 首选亲和性中的首选体现在那些方面?
  3. 强亲和性配置时,有两种配置方式,两种的区别是什么?
  4. 首选亲和性中的权重值到底是什么规则? 值越大权重值越高么？还是值越小越高(1最大)?
  5. 首选亲和性配置中, 如果Pod能匹配A节点的多个Label,也能匹配B节点的一个Label(A的Label权重之和等于B单个Label的权重),这时Pod会优先调度到A节点么?
  6. 缩容时,是先从低权重的节点上开始杀么？
这些问题, 我们都不能全靠注释和理解去揣测答案,必须经过实测得到真实答案,否则一旦上了生产再想修改就需要付出更大的成本。
7. 如果Pod是以强亲和性的方式绑定在节点上且Pod已经正常运行在此节点上,此时删除节点的标签是否会导致Pod重启发生漂移。</p>

<h4 id="1-强亲和性-requiredduringschedulingignoredduringexecution">1、强亲和性:requiredDuringSchedulingIgnoredDuringExecution</h4>

<blockquote>
<p>例子Node Labels设定:</p>
</blockquote>

<pre><code>level: important(重要)，general(一般),unimportant(不重要)
</code></pre>

<table>
<thead>
<tr>
<th>Label</th>
<th>Node</th>
<th>中文解释</th>
</tr>
</thead>

<tbody>
<tr>
<td>level=important</td>
<td>10.x.x.78, 10.1.x.x.79, 10.x.x.80</td>
<td>重要的</td>
</tr>

<tr>
<td>level=general</td>
<td>10.x.x.82, 10.x.x.82, 10.x.x.84</td>
<td>一般的</td>
</tr>

<tr>
<td>level=unimportant</td>
<td>10.x.x.86, 10.x.x.87, 10.x.x.88</td>
<td>不重要的</td>
</tr>

<tr>
<td>app=1</td>
<td>10.x.x.80</td>
<td></td>
</tr>

<tr>
<td>master=1</td>
<td>10.x.x.85</td>
<td></td>
</tr>
</tbody>
</table>

<h5 id="pod与运算的配置">Pod与运算的配置:</h5>

<p>注意: 强亲和性的配置分为: 与运算、或运算两部分</p>

<pre><code>          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: level
                operator: In
                values:
                - important
              - key: app
                operator: In
                values:
                - 1
</code></pre>

<p>在与运算的配置中,我们发现,在同一个matchExpressions中既需要匹配level=important的标签也需要匹配app=1的标签。也就是说:Pod只会选择同时匹配这两个Label的节点.</p>

<p>根据上面Pod的Node亲和性设置,两个Label求一个交集,只有同时满足两个Label的节点才会纳入这个Pod的调度池,显而易见,只有10.x.x.80这个节点。所以,此Pod只能调度到这个节点,如果这个节点资源不足,那么此Pod调度失败。</p>

<h5 id="pod或运算配置">Pod或运算配置:</h5>

<pre><code>          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: level
                operator: In
                values:
                - important
            - matchExpressions:
              - key: level
                operator: In
                values:
                - unimportant
</code></pre>

<p>在或运算的配置中,我们发现有一个matchExpressions数组,里面的Label列表求并集。也就是说:Pod可以选择只要匹配其中一个Label的节点就行,不用全匹配。</p>

<p>举个例子:</p>

<p>节点的Label设置沿用上一个例子的。 节点的标签只要能满足Pod的其中一个标签, 节点就能纳入这个Pod的调度池,显而易见,这个Pod可选的节点有:10.x.x.78, 10.x.x.79,10.x.x.80, 10.x.x.86, 10.x.x.87, 10.x.x.88。</p>

<h4 id="首选亲和性-preferredduringschedulingignoredduringexecution">首选亲和性: preferredDuringSchedulingIgnoredDuringExecution</h4>

<p>它的使用风格应该是:如果Pod能调度到指定Label的节点最好,如果不能,也不强求,Pod选择其他的节点也行,即使这个节点根本没有Label或者节点的Label和我完全不匹配。</p>

<p>Pod首选亲和性设置:</p>

<pre><code>          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: level
                operator: In
                values:
                - important
            weight: 5
          - preference:
              matchExpressions:
              - key: app
                operator: In
                values:
                - &quot;1&quot;
            weight: 4
          - preference:
              matchExpressions:
              - key: master
                operator: In
                values:
                - &quot;1&quot;
            weight: 10
</code></pre>

<p>示例:
Node的Label设置沿用上一个例子的,
根据上面的配置,我们会看到:</p>

<table>
<thead>
<tr>
<th>Label</th>
<th>Pod设置的标签权重</th>
<th>Label对应的节点</th>
<th>首选</th>
</tr>
</thead>

<tbody>
<tr>
<td>master=1</td>
<td>10</td>
<td>10.x.x.85</td>
<td>√</td>
</tr>

<tr>
<td>app=1</td>
<td>4</td>
<td>10.x.x.80</td>
<td></td>
</tr>

<tr>
<td>level=important</td>
<td>5</td>
<td>10.x.x.78,10.x.x.79,10.x.x.80</td>
<td></td>
</tr>
</tbody>
</table>

<p>如表所示, 最终Pod优先调度到10.x.x.85, 原因在于app=1的权重是4, level=important的权重是5, 所以:节点 10.x.x.80的权重是:9,但是仍然小于节点:10.x.x.85的权重。</p>

<h4 id="问题总结">问题总结:</h4>

<ol>
<li>其实强亲和性和首选亲和性区别体现在:Pod对节点的选择上。就强亲和性而言,如果节点不能匹配Pod对Label的要求, Pod永远不会调度到这类节点上,即使是Pod调度失败(没错,就是头铁),就首选亲和性来说,能调度到最优节点是一件非常值得开心的事情,如果不能调度到最优节点可以退而求其次,总有适合自己的。 (回答问题1)</li>
<li>首选亲和性体现在PodLabel的权重值上,而与节点Label的匹配个数无关。(回答问题2)</li>
<li>在首选亲和性配置中会多一个权重值的字段(weight),这个值越大,权重越大,Pod调度到对应此Label的节点的概率越高。(回答问题4)</li>
<li>一个节点有多个Label且节点能满足Pod所要求的所有Label,如果多个Label的权重值相加仍然小于某单个Label的节点,那么Pod首选是权重值高的节点;如果Pod能匹配到A 节点的所有Label,同时也能匹配到B 节点某一个Label.但是,A节点 Label的权重之和刚好等于B 节点的单个Label的权重，这时,Pod优先选择的A还是B这是随机的(只针对亲和性来说是随机的,实际情况还要考虑其他情况)。而不会管Label的匹配个数。(回答问题5)</li>
<li>创建或扩容Pod时,优先选择Label匹配权重值大的节点,若此节点的其他条件不满足(比如内存不足),选择次权重的,最后选择Label不匹配或者根本没有Label的节点。</li>
<li>(回答问题6)缩容时,随机选择Pod杀掉,而不是我们预想的从低权重的节点开始杀,这点值得注意。</li>
<li>(回答问题7)答案是不会,正在运行的Pod不会被调度到新的节点去, 当Pod因为某种原因重启(指Pod名字改变,触发重调度,名字不改变,意味着不触发调度器调度,只是原地重启)后,会自动调度到符合亲和性选择的节点上。</li>
</ol>

<h2 id="三-污点和node-affinity的使用总结">三、污点和Node Affinity的使用总结</h2>

<ol>
<li>就污点而言,它的使用通常是负向的, 也就说, 污点常用在某Node不让大多数Pod调度只让少部分Pod调度时,又或者节点根本不参加工作负载时。比如:我们常见的master节点上不调度负载pod,保证master组件的稳定性；节点有特殊资源，大部分应用不需要而少部分应用需要,如GPU。</li>
<li>就Node Affinity来说,他的使用可以正向的,也就是说,我们想让某个应用的Pod部署在指定的一堆节点上。当然,也可以是负向的,比如说我们常说的Node 反亲和性,只需要把操作符设置为NotIn就能达成预期目标。</li>
<li>就污点而言，如果节点设置的污点效果是NoSchedule或者NoExecute,意味着没有设置污点容忍的Pod绝不可能调度到这些节点上。</li>
<li>就Node Affinity而言,如果节点设置了Label,但是Pod没有任何的Node Affinity设置,那么Pod是可以调度到这些节点上的。
<br /></li>
</ol>
]]></content>
		</item>
		
		<item>
			<title>[译]使用 Kubebuilder 开发 Operator</title>
			<link>https://yulibaozi.com/posts/kubernetes/extend/2020-04-22-use-kubebuilder-to-operator/</link>
			<pubDate>Tue, 21 Apr 2020 01:10:50 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/extend/2020-04-22-use-kubebuilder-to-operator/</guid>
			<description>写给那些人 原文地址
写给那些Kubernetes用户 Kubernetes用户将通过学习API的设计和实现背后的基本概念，更深入的理解Kubernetes。本书将教会读者怎样开发自己的Kubernetes APIs 和学习Kubernetes API核心的设计原则。
包括:
 Kubernetes API和resource的结构设计 API 版本语义 自托管/自愈 垃圾回收和终结器 声明式和命令式APIs 基于级别和基于边缘的APIs 资源和子资源  Kubernetes API 扩展开发 API 扩展开发者将学习到实现Kubernetes API背后的原理，概念和实现规范，以及用户快速开发的简单工具和库。 这本书涵盖了开发人员遇到的陷阱和思维上的误区。
包括:
 如果将多个事件批处理为单次调用对比,(翻译疑问) 如何配置定期对比 即将有的 如何使用缓存和实时查找 垃圾收集和终结器 如何使用声明与Webhook验证 如何实现API版本控制  快速开始 安装 安装Kubebuilder:
os=$(go env GOOS) arch=$(go env GOARCH) # download kubebuilder and extract it to tmp curl -sL https://go.kubebuilder.io/dl/2.0.0-alpha.2/${os}/${arch} | tar -xz -C /tmp/ # move to a long-term location and put it on your path # (you&#39;ll need to set the KUBEBUILDER_ASSETS env var if you put it somewhere else) sudo mv /tmp/kubebuilder_2.</description>
			<content type="html"><![CDATA[

<h2 id="写给那些人">写给那些人</h2>

<p><a href="https://book.kubebuilder.io/introduction.html">原文地址</a></p>

<h4 id="写给那些kubernetes用户">写给那些Kubernetes用户</h4>

<p>Kubernetes用户将通过学习API的设计和实现背后的基本概念，更深入的理解Kubernetes。本书将教会读者怎样开发自己的Kubernetes APIs 和学习Kubernetes API核心的设计原则。</p>

<p>包括:</p>

<ul>
<li>Kubernetes API和resource的结构设计</li>
<li>API 版本语义</li>
<li>自托管/自愈</li>
<li>垃圾回收和终结器</li>
<li>声明式和命令式APIs</li>
<li>基于级别和基于边缘的APIs</li>
<li>资源和子资源</li>
</ul>

<h4 id="kubernetes-api-扩展开发">Kubernetes API 扩展开发</h4>

<p>API 扩展开发者将学习到实现Kubernetes API背后的原理，概念和实现规范，以及用户快速开发的简单工具和库。 这本书涵盖了开发人员遇到的陷阱和思维上的误区。</p>

<p>包括:</p>

<ul>
<li>如果将多个事件批处理为单次调用对比,(翻译疑问)</li>
<li>如何配置定期对比</li>
<li>即将有的</li>
<li>如何使用缓存和实时查找</li>
<li>垃圾收集和终结器</li>
<li>如何使用声明与Webhook验证</li>
<li>如何实现API版本控制</li>
</ul>

<h2 id="快速开始">快速开始</h2>

<h4 id="安装">安装</h4>

<p>安装Kubebuilder:</p>

<pre><code>os=$(go env GOOS)
arch=$(go env GOARCH)

# download kubebuilder and extract it to tmp
curl -sL https://go.kubebuilder.io/dl/2.0.0-alpha.2/${os}/${arch} | tar -xz -C /tmp/

# move to a long-term location and put it on your path
# (you'll need to set the KUBEBUILDER_ASSETS env var if you put it somewhere else)
sudo mv /tmp/kubebuilder_2.0.0-alpha.2_${os}_${arch} /usr/local/kubebuilder
export PATH=$PATH:/usr/local/kubebuilder/bin
</code></pre>

<h4 id="创建一个项目">创建一个项目</h4>

<p>初始化一个新项目</p>

<pre><code>kubebuilder init --domain my.domain
</code></pre>

<p>如果你没有在<code>GOPATH</code>下，你可以运行<code>go mod init &lt;modulename&gt;</code>来告诉kubebuilder和Go来基于你的module来导入包/路径。</p>

<h4 id="增加一个api-the-kubebuilder-book">增加一个API    The Kubebuilder Book</h4>

<p><a href="https://book.kubebuilder.io/introduction.html">原文地址</a></p>

<p>创建一个名为<code>webapp/v1</code>的新API组版本,以及该版本有类型为<code>Guestbook</code>的Kind。</p>

<pre><code>kubebuilder create api --group webapp --version v1 --kind Guestbook
</code></pre>

<p>执行上面的命令将会创建文件<code>api/v1/guestbook_types.go</code> 和 <code>controller/guestbook_controller.go</code></p>

<p><strong><em>可选:</em></strong> 编辑API定义或业务逻辑,更多详情查看什么是控制器 什么是资源。</p>

<h4 id="本地测试">本地测试</h4>

<p>首先，你需要有个kuberentes 集群来运行, 您可以使用KIND获取本地群集进行测试，或者针对远程群集运行。</p>

<p>你的控制器会自动使用你的kubeconfig文件中的当前上下文(查看集群上下文:<code>kubectl cluster-info</code>显示)</p>

<p>安装CRDs 到集群:</p>

<pre><code>make install
</code></pre>

<p>运行你的控制器(前台运行,你可以切换到另外一个终端来操作)：</p>

<pre><code>make run
</code></pre>

<h4 id="安装示例">安装示例</h4>

<p>如果你修改过你的配置，你需要重新发布:</p>

<pre><code>kubectl apply -f config/samples/
</code></pre>

<h4 id="在集群上运行">在集群上运行</h4>

<p>打包和推送你的镜像到镜像库:</p>

<pre><code>make docker-build docker-push IMG=&lt;some-registry&gt;/controller
</code></pre>

<p>部署你的控制器到集群:</p>

<pre><code>make deploy
</code></pre>

<p>如遇到RBAC错误,你需要给控制器授权,最简单的方式就是使用管理员权限.</p>

<h2 id="教程-构建一个cronjob">教程: 构建一个CronJob</h2>

<p>太多的教程是从一些非常人为的设定开始的,或者写一些玩具级的应用程序，从而得到一些基础知识,而停止了更深入的探索,相反的是, 这个教程会带你通过kubebuilder完成所有(几乎)复杂的工作, 从简入繁构建一个功能非常全面的东西.</p>

<p>来，我们假设(小的场景), 我们终于受够了kubernetes中非kubebuilder实现的CronJob控制器的维护负担,你非常的想使用kubebuilder重写它(嗯~,这种假设有点意思)。</p>

<p>CronJob控制的job定期在kubernetes集群运行一次性任务,它需要通过Job控制器进行构建,任务是一次性任务，直到它们的任务完成。</p>

<p>我们不会试图重写job控制器,而是作为一个支点来了解如何与外部类型进行交互.</p>

<h4 id="构建我们的项目">构建我们的项目</h4>

<p>如快速入门那样,我们需要构建一个新项目。确保你已经安装了<code>Kubebuilder</code>,然后创建了一个新项目:</p>

<pre><code># we'll use a domain of tutorial.kubebuilder.io,
# so all API groups will be &lt;group&gt;.tutorial.kubebuilder.io.
kubebuilder init --domain tutorial.kubebuilder.io
</code></pre>

<p>到目前为止,我们已经有个一个项目，来看看kubebuilder搭建的架子&hellip;</p>

<h2 id="什么是基础项目">什么是基础项目?</h2>

<p>当我们构建了一个新项目,kubebuilder为我们提供了一些基本的样本文件。</p>

<h4 id="构建结构">构建结构</h4>

<p>首先, 构建项目的基础架构:</p>

<ul>
<li><p><code>go.mod</code>  : go 包依赖</p>

<pre><code>module tutorial.kubebuilder.io/project

go 1.12

require (
github.com/go-logr/logr v0.1.0
github.com/robfig/cron v1.1.0
k8s.io/api v0.0.0-20190222213804-5cb15d344471
k8s.io/apimachinery v0.0.0-20190221213512-86fb29eff628
k8s.io/client-go v0.0.0-20190228174230-b40b2a5939e4
sigs.k8s.io/controller-runtime v0.2.0-alpha.0.0.20190503051552-b666157c41da
sigs.k8s.io/controller-tools v0.2.0-alpha.1 // indirect
)

</code></pre></li>

<li><p><code>Makefile</code>: 构建和部署你的控制器</p>

<pre><code># Image URL to use all building/pushing image targets
IMG ?= controller:latest
# Produce CRDs that work back to Kubernetes 1.11 (no version conversion)
CRD_OPTIONS ?= &quot;crd:trivialVersions=true&quot;

all: manager

# Run tests
test: generate fmt vet manifests
go test ./api/... ./controllers/... -coverprofile cover.out

# Build manager binary
manager: generate fmt vet
go build -o bin/manager main.go

# Run against the configured Kubernetes cluster in ~/.kube/config
run: generate fmt vet
go run ./main.go

# Install CRDs into a cluster
install: manifests
kubectl apply -f config/crd/bases

# Deploy controller in the configured Kubernetes cluster in ~/.kube/config
deploy: manifests
kubectl apply -f config/crd/bases
kustomize build config/default | kubectl apply -f -

# Generate manifests e.g. CRD, RBAC etc.
manifests: controller-gen
$(CONTROLLER_GEN) $(CRD_OPTIONS) rbac:roleName=manager-role webhook paths=&quot;./api/...;./controllers/...&quot; output:crd:artifacts:config=config/crd/bases

# Run go fmt against code
fmt:
go fmt ./...

# Run go vet against code
vet:
go vet ./...

# Generate code
generate: controller-gen
$(CONTROLLER_GEN) object:headerFile=./hack/boilerplate.go.txt paths=./api/...

# Build the docker image
docker-build: test
docker build . -t ${IMG}
@echo &quot;updating kustomize image patch file for manager resource&quot;
sed -i'' -e 's@image: .*@image: '&quot;${IMG}&quot;'@' ./config/default/manager_image_patch.yaml

# Push the docker image
docker-push:
docker push ${IMG}

# find or download controller-gen
# download controller-gen if necessary
controller-gen:
ifeq (, $(shell which controller-gen))
go get sigs.k8s.io/controller-tools/cmd/controller-gen@v0.2.0-alpha.1
CONTROLLER_GEN=$(shell go env GOPATH)/bin/controller-gen
else
CONTROLLER_GEN=$(shell which controller-gen)
endif

</code></pre></li>

<li><p>PROJECT: 用于搭建新组件的Kubebuilder元数据</p>

<pre><code>version: &quot;2&quot;
domain: tutorial.kubebuilder.io
repo: tutorial.kubebuilder.io/project
</code></pre></li>
</ul>

<h4 id="启动配置">启动配置</h4>

<p>我们还在<code>config/</code>目录下获得启动配置. 现在,他只是包含在集群上启动控制器所需的kustomize YAML定义,但是,一旦我们开始编写控制器,它将保留我们的CustomResourceDefinitions，RBAC配置和WebhookConfigurations。</p>

<h4 id="入口点">入口点</h4>

<p>最后,同样重要的是,Kubebuilder 支持我们项目的基本如何点:main.go,接着向下看</p>

<h2 id="每个旅程都需要一个开始-每个项目都需要main">每个旅程都需要一个开始，每个项目都需要main</h2>

<ul>
<li>Apache License</li>
</ul>

<p>根据Apache许可证2.0版（“许可证”）获得许可;除非符合许可，否则您不得使用此文件。您可以在以下位置获取许可证副本</p>

<pre><code>http://www.apache.org/licenses/LICENSE-2.0
</code></pre>

<p>除非适用法律要求或书面同意，否则根据许可证分发的软件将按“原样”分发，不附带任何明示或暗示的担保或条件。有关管理许可下的权限和限制的特定语言，请参阅许可证。</p>

<p>我们的包从基本的进口开始。尤其：</p>

<ul>
<li>核心控制器 - 运行时库</li>

<li><p>默认的控制器 - 运行时日志记录，Zap（稍后会详细介绍）</p>

<pre><code>package main

import (
&quot;flag&quot;
&quot;os&quot;

&quot;k8s.io/apimachinery/pkg/runtime&quot;
_ &quot;k8s.io/client-go/plugin/pkg/client/auth/gcp&quot;
ctrl &quot;sigs.k8s.io/controller-runtime&quot;
&quot;sigs.k8s.io/controller-runtime/pkg/log/zap&quot;
// +kubebuilder:scaffold:imports
)
</code></pre></li>
</ul>

<p>每组控制器都需要一个Scheme，它提供了Kinds与其相应Go类型之间的映射。当我们编写API定义时，我们会更多地了解Kinds，所以请稍后记住这一点。</p>

<pre><code>var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName(&quot;setup&quot;)
)

func init() {

    // +kubebuilder:scaffold:scheme
}
</code></pre>

<p>在这一点上，我们的主要功能相当简单：
* 我们设置了一些基本标志(注释中的// +xxx)。
* 我们实例化一个管理器，它追踪我们所有控制器的运行，以及为API服务器设置共享缓存和客户端（注意:我们需要告诉管理员我们的Scheme）。
* 我们运行管理器,管理器又运行我们的控制器和webhooks，管理器会一直运行直到收到了终止信号, 这样,当我们在kubernetes上运行的时候,我们能够有优秀的表现,优雅的pod终止。</p>

<p>虽然我们还没有任何东西可以运行,但我们可以先记住<code>+kubebuilder:scaffold:builder</code>，很快就会发生有趣的事情了。</p>

<pre><code>func main() {
    var metricsAddr string
    flag.StringVar(&amp;metricsAddr, &quot;metrics-addr&quot;, &quot;:8080&quot;, &quot;The address the metric endpoint binds to.&quot;)
    flag.Parse()

    ctrl.SetLogger(zap.Logger(true))

    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
    if err != nil {
        setupLog.Error(err, &quot;unable to start manager&quot;)
        os.Exit(1)
    }

    // +kubebuilder:scaffold:builder

    setupLog.Info(&quot;starting manager&quot;)
    if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
        setupLog.Error(err, &quot;problem running manager&quot;)
        os.Exit(1)
    }
}
</code></pre>

<p>有了这个，我们可以继续使用我们的API脚手架！</p>

<h2 id="组-版本-种类和资源-groups-versions-和-kinds">组，版本，种类和资源 (Groups Versions 和 Kinds)</h2>

<p>实际上，在我们开始使用API之前，我们应该先谈谈术语。当我们在Kubernetes中讨论API时，我们经常使用4个术语：组，版本，种类和资源(<em>groups, versions, kinds, and resources</em>)。</p>

<h4 id="groups-和-versions">Groups 和 Versions</h4>

<p><em>Group</em> 只是相关功能的集合,每个group 都有一个或多个<em>versions</em>, 顾名思义,它允许我们随着时间的迁移,能够兼容性的变更API的工作方式。</p>

<h4 id="kinds-和-resources">Kinds 和 Resources</h4>

<p>每个 API group-version 包含了一个或多个API类型, 我们称之为<em>Kinds</em>. 虽然Kind可以在版本之间变更, 但是每个配置必须能够以某种方式存储其他配置的所有数据(我们可以将数据存储在字段中或者注释中)。这意味着使用较旧的API版本(version) 不会导致新的数据丢失或者损坏。
更多信息可以查看：KubernetesAPI指南。</p>

<p>你还会听到一些<em>resources</em>,这个resource 只是在API中使用的Kind. 通常,Kinds和resources之间是一对一的映射关系,比如,pods 资源对应<code>Pod</code> Kind。但是有时候,多个资源可能会返回相同的Kind. 例如,<code>Scale</code> Kind 由所有的scale子资源返回,比如:<code>deployments/scale</code> 或者 <code>replicasets/scale</code>, 这使得Kubernetes HorizontalPodAutoscaler可以与不同的资源进行交互,但是,对于CRD, 每个类型(Kind)对应于单个资源(resource).</p>

<p>那么，我们在Go中 上面的类型是如何对应的呢?</p>

<p>当我们引用特定 group-version的时候, 我们称之为 <em>GroupVersionKind</em>,或者简写为GVK. 和resource和GVR相同,稍后会看到 每个GVK对应于包中给定的 根(root) Go 类型</p>

<h4 id="额-scheme是个什么东东">额，scheme是个什么东东?</h4>

<p>我们之前看到的只是Go类型和GVK如何对应的方法。</p>

<p>例如:我们假设将<code>&quot;tutorial.kubebuilder.io/api/v1&quot;.CronJob{}</code> 类型标记为<code>batch.tutorial.kubebuilder.io/v1</code> API group(隐含地说,他是类型为CronJob的Kind).</p>

<p>然后, 我们稍后构建一个新的&amp;CronJob{},通过服务器提供的json</p>

<pre><code>{
    &quot;kind&quot;: &quot;CronJob&quot;,
    &quot;apiVersion&quot;: &quot;batch.tutorial.kubebuilder.io/v1&quot;,
    ...
}

</code></pre>

<p>当我们更新CronJob的时候，确认提交了正确的group-version</p>

<h2 id="添加一个新的api">添加一个新的API</h2>

<p>为了能够创建一个新的Kind和一个对应的控制器,我们需要使用到<code>kubebuilder create api</code>:</p>

<pre><code>kubebuilder create api --group batch --version v1 --kind CronJob
</code></pre>

<p>我们为group-version 调用这个命令,它将为group-version 创建一个目录。</p>

<p>在这种情况下,<code>api/v1/</code>目录被创建,与version:<code>batch.tutorial.kubebuilder.io/v1</code>相对应。
(在这里你需要基础你<a href="https://book.kubebuilder.io/cronjob-tutorial.html#scaffolding-out-our-project">一开始设置的域名设置</a>)</p>

<p>它还为我们的<code>CronJob</code>类型添加了一个文件:<code>api/v1/cronjob_types.go</code>,我们每次调用这个命令的时候,他会为不同的类型生成一个新的文件.</p>

<p>从一开始,我们导入了<code>meta/v1</code> API group，它通常不会自己单独存在,而是包含所有kubernetes Kinds的共有的元数据.</p>

<pre><code>package v1

import (
    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
)
</code></pre>

<p>接下来,我们需要为我们的Kind定义相关的规范(Spec)和相关的状态,Kubernetes 通过协调预期状态(Spec)与实际集群状态(其他对象状态),或者外部状态,然后记录他们观察到的状态(Status)来起作用。因此,每个功能对象都包含了两大部分规范(Sepc)和状态(Status)。</p>

<p>另外, 一些类型(如ConfigMap)不遵循这种模式,因为他们不需要所需的期望状态和实际状态的对比过程,但是大多数类型需要。</p>

<pre><code>// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
    // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file
}

// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file
}
</code></pre>

<p>再接下来,我们得到了与实际Kinds,CronJob和CronjobList相对应的类型,CronJob是我们的根类型(root type),描述Cronjob的类型和相关信息.与所有的Kubernetes对象一样,它包含了<code>TypeMeta</code>(描述API版本和Kind), 同时也包含了<code>ObjectMeata</code>,它包含了名称,命名空间,标签等内容。</p>

<p><code>CronJobList</code>只是多个CronJobs的容器而已,这是批量操作中使用的类型,像数据结构的List。</p>

<p>通常,我们从不会修改<code>TypeMeat</code>或者<code>ObjectMeta</code>等存放元数据的结构体,所有的修改都在Spec或者Status中.</p>

<p><code>+kubebuilder:object:root</code>这个注解被称为标记,接下来,我们会看下它,它充当了额外的元数据,告诉控制器工具(代码和Yaml生成器)额外的信息. 这个特殊的标记告诉对象生成器(object generator) 这个类型代表一个Kind,接下来,对象生成器为我们生成<code>runtime.Object</code>接口实现，这意味着Kinds必须实现所有类型的接口。</p>

<pre><code>// +kubebuilder:object:root=true

// CronJob is the Schema for the cronjobs API
type CronJob struct {
    metav1.TypeMeta   `json:&quot;,inline&quot;`
    metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

    Spec   CronJobSpec   `json:&quot;spec,omitempty&quot;`
    Status CronJobStatus `json:&quot;status,omitempty&quot;`
}

// +kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
    metav1.TypeMeta `json:&quot;,inline&quot;`
    metav1.ListMeta `json:&quot;metadata,omitempty&quot;`
    Items           []CronJob `json:&quot;items&quot;`
}
</code></pre>

<p>最后, 我们把Go 类型添加到API Group下, 我们可以把这个API group天骄到任何的<a href="https://book.kubebuilder.io/todo">scheme</a></p>

<pre><code>func init() {
    SchemeBuilder.Register(&amp;CronJob{}, &amp;CronJobList{})
}
</code></pre>

<p>现在,我们已经有了一些基本结构,接下来,我们把他填满。</p>

<h2 id="设计api">设计API</h2>

<p>在Kubernetes中, 我们对如何设计API需要明确一些规则,比如说:所有的需要序列化的字段必须是驼峰命名法,所以,我们可以JSON结构来标记;我们也可以使用<code>omitempty</code>来标记当字段数据为空时从序列化中省略这个字段。</p>

<p>字段可以是基本类型，但是数字是一个例外,出于对API兼容性的考虑,我们接受两种形式的数字, int32表示整数,resource.Quantity 表示小数。</p>

<ul>
<li>注意,什么是Quantity?</li>
</ul>

<p>我们有个特殊的使用类型:<code>metav1.Time</code>,除了具有固定的反/序列化方式以外,他的功能和<code>time.Time</code>相同。</p>

<p>有了这个类型,我们看看CronJob对象长什么样子。</p>

<ul>
<li><p>导入</p>

<pre><code>package v1

import (
batchv1beta1 &quot;k8s.io/api/batch/v1beta1&quot;
corev1 &quot;k8s.io/api/core/v1&quot;
metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
)

// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.
</code></pre></li>
</ul>

<p>首先，我们来看看我们的spec,正如我们讨论的那样,spec定义期望状态,因此我们控制器的数据来源都会出现在这里(input口):</p>

<p>从根本上来说,CronJob需要实现以下:</p>

<ul>
<li>运行job的时间表 (A schedule the cron in CronJob)</li>
<li>一个运行Jon的模板(这个Job来自于CronJob)</li>
</ul>

<p>我们需要一些额外的功能,这可以让我们的工作更轻松一些:</p>

<ul>
<li>启动job的最后期限(如果错过了截至日期,我们需要等到下一个预定时间)</li>
<li>如果需要一次运行多个job,我们该怎么办(等待?停止旧的?同时运行?)</li>
<li>暂停运行Cronjob的方法</li>
<li>限制旧的job历史(保留几个)</li>
</ul>

<p>记住,由于我们从来没有读取过Job的Status,所以,我们需要使用其他方式来跟踪job是否已经运行。我们可以使用旧的job来做这个事情。</p>

<p>我们将使用几个标记(// +注释)来指定其他的元数据,这些标记在使用控制工具(代码生成工具)生成CRD清单的时候用到,正如我们稍后看到的那样,代码生成工具也可以使用GoDoc来对字段进行含义描述。</p>

<pre><code>// CronJobSpec defines the desired state of CronJob
type CronJobSpec struct {
    // The schedule in Cron format, see https://en.wikipedia.org/wiki/Cron.
    Schedule string `json:&quot;schedule&quot;`

    // Optional deadline in seconds for starting the job if it misses scheduled
    // time for any reason.  Missed jobs executions will be counted as failed ones.
    // +optional
    StartingDeadlineSeconds *int64 `json:&quot;startingDeadlineSeconds,omitempty&quot;`

    // Specifies how to treat concurrent executions of a Job.
    // Valid values are:
    // - &quot;Allow&quot; (default): allows CronJobs to run concurrently;
    // - &quot;Forbid&quot;: forbids concurrent runs, skipping next run if previous run hasn't finished yet;
    // - &quot;Replace&quot;: cancels currently running job and replaces it with a new one
    // +optional
    ConcurrencyPolicy ConcurrencyPolicy `json:&quot;concurrencyPolicy,omitempty&quot;`

    // This flag tells the controller to suspend subsequent executions, it does
    // not apply to already started executions.  Defaults to false.
    // +optional
    Suspend *bool `json:&quot;suspend,omitempty&quot;`

    // Specifies the job that will be created when executing a CronJob.
    JobTemplate batchv1beta1.JobTemplateSpec `json:&quot;jobTemplate&quot;`

    // The number of successful finished jobs to retain.
    // This is a pointer to distinguish between explicit zero and not specified.
    // +optional
    SuccessfulJobsHistoryLimit *int32 `json:&quot;successfulJobsHistoryLimit,omitempty&quot;`

    // The number of failed finished jobs to retain.
    // This is a pointer to distinguish between explicit zero and not specified.
    // +optional
    FailedJobsHistoryLimit *int32 `json:&quot;failedJobsHistoryLimit,omitempty&quot;`
}

</code></pre>

<p>我们定义了一个自定义类型来保存我们的并发策略,但是实际上,它只是一个字符串,但是,我们给它定义了额外的类型,并提供了额外的文档和几个常量,我们在类型上而不是在字段值上做附加判断,因为这样有利于验证重用。</p>

<pre><code>// ConcurrencyPolicy describes how the job will be handled.
// Only one of the following concurrent policies may be specified.
// If none of the following policies is specified, the default one
// is AllowConcurrent.
// +kubebuilder:validation:Enum=Allow;Forbid;Replace
type ConcurrencyPolicy string

const (
    // AllowConcurrent allows CronJobs to run concurrently.
    AllowConcurrent ConcurrencyPolicy = &quot;Allow&quot;

    // ForbidConcurrent forbids concurrent runs, skipping next run if previous
    // hasn't finished yet.
    ForbidConcurrent ConcurrencyPolicy = &quot;Forbid&quot;

    // ReplaceConcurrent cancels currently running job and replaces it with a new one.
    ReplaceConcurrent ConcurrencyPolicy = &quot;Replace&quot;
)
</code></pre>

<p>接下来,我们设计一下<code>Status</code>,它是记录观察到的实际状态,它包括了我们希望展示给用户或者其他控制器的任何信息。</p>

<p>我们将保留一份正在运行的工作清单,以及我们上次成功完成工作的时间。注意,我们使用<code>metav1.Time</code>而不是<code>time.Time</code>来确保序列化的稳定性,正如我们上面提到的那样。</p>

<pre><code>// CronJobStatus defines the observed state of CronJob
type CronJobStatus struct {
    // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
    // Important: Run &quot;make&quot; to regenerate code after modifying this file

    // A list of pointers to currently running jobs.
    // +optional
    Active []corev1.ObjectReference `json:&quot;active,omitempty&quot;`

    // Information when was the last time the job was successfully scheduled.
    // +optional
    LastScheduleTime *metav1.Time `json:&quot;lastScheduleTime,omitempty&quot;`
}
</code></pre>

<p>最后，通常情况下,我们不需要更改它，但是如果我们需要标记我们子资源的状态，对这块的设计可以参照kubernetes内置类型,因为它们的设计类似于kubernetes内置类型。</p>

<pre><code>// +kubebuilder:object:root=true
// +kubebuilder:subresource:status

// CronJob is the Schema for the cronjobs API
type CronJob struct {
</code></pre>

<ul>
<li><p>根对象定义</p>

<pre><code>metav1.TypeMeta   `json:&quot;,inline&quot;`
metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`

Spec   CronJobSpec   `json:&quot;spec,omitempty&quot;`
Status CronJobStatus `json:&quot;status,omitempty&quot;`
}

// +kubebuilder:object:root=true

// CronJobList contains a list of CronJob
type CronJobList struct {
metav1.TypeMeta `json:&quot;,inline&quot;`
metav1.ListMeta `json:&quot;metadata,omitempty&quot;`
Items           []CronJob `json:&quot;items&quot;`
}

func init() {
SchemeBuilder.Register(&amp;CronJob{}, &amp;CronJobList{})
}
</code></pre></li>
</ul>

<p>现在,我们有了API,接下来,我们需要编写一个控制器来实际实现这个功能。</p>

<h2 id="旁白">旁白:</h2>

<p>如果你浏览过<code>api/v1</code>目录下的其他文件,你可能会注意到除了<code>cronjob_types.go</code>外,还有两个另外的文件:<code>groupversion_info.go</code>和<code>zz_generated.deepcopy.go</code></p>

<p>这两个文件不需要任何的修改,前者保持不变,后者是自动生成的,但如果你了解其中的内容对你来说很有用。</p>

<h4 id="groupversion-info-go">groupversion_info.go</h4>

<p><code>groupversion_info.go</code> 包含了group-version的两项元数据。</p>

<p>首先,我们有一些包级别的标记,这些标记代表这个包里有Kubernetes对象,并且此表示group:<code>batch.tutorial.kubebuilder.io</code>.  对象生成器主要使用前面的标记来生成对象,而后者由CRD生成器用它创建CRD并生成正确的元数据。如下所示.</p>

<pre><code>// Package v1 contains API Schema definitions for the batch v1 API group
// +kubebuilder:object:generate=true
// +groupName=batch.tutorial.kubebuilder.io
package v1

import (
    &quot;k8s.io/apimachinery/pkg/runtime/schema&quot;
    &quot;sigs.k8s.io/controller-runtime/pkg/scheme&quot;
)
</code></pre>

<p>接着，我们需要常用的变量来帮助我们设置<code>Scheme</code>,由于我们需要在控制器中使用此包的所有类型,因此我们需要一个简便方法将所有类型天骄到其他的<code>Scheme</code>中。SchemeBuilder 会帮助我们完成这些事情。</p>

<pre><code>var (
    // GroupVersion is group version used to register these objects
    GroupVersion = schema.GroupVersion{Group: &quot;batch.tutorial.kubebuilder.io&quot;, Version: &quot;v1&quot;}

    // SchemeBuilder is used to add go types to the GroupVersionKind scheme
    SchemeBuilder = &amp;scheme.Builder{GroupVersion: GroupVersion}

    // AddToScheme adds the types in this group-version to the given scheme.
    AddToScheme = SchemeBuilder.AddToScheme
)
</code></pre>

<h4 id="zz-generated-deepcopy-go">zz_generated.deepcopy.go</h4>

<p><code>zz_generated.deepcopy.go</code> 包含了对上面提到的<code>runtime.Object</code>接口的自动生成实现,它更具root类型标记为Kinds。</p>

<p><code>runtime.Object</code>接口的核心是深入复制方法<code>DeepCopyObject</code>!</p>

<p>controller-tools中的对象生成器还为每个根类型及其所有子类型生成另外两个方便的方法：<code>DeepCopy</code>和<code>DeepCopyInto</code>。</p>

<h2 id="什么是控制器">什么是控制器?</h2>

<p>控制器是Kubernetes的核心。</p>

<p>controller的工作任务是:给定任何一对对象,保证真实世界的状态(包括集群状态,外部状态,如：kubelet运行的容器或者是云提供商提供的负载均衡器)和期望的状态相匹配。每一个控制器都聚焦于一个个root Kind,但是也可能需要和其他Kind交互。</p>

<p>我们可以称这个过程为:<code>reconciling</code>(持续的监听现实世界的状态是否和期望是否一致,不一致就修正)</p>

<p>在控制器运行时,实现某种特定的协调逻辑,我们可以称之为:<code>Reconciler</code>,Reconciler(协调器)获取对象的名称,并返回是否需要再次尝试协调(比如: 出现错误或周期性控制器时, 如HPA).</p>

<p>首先,我们从标准入口开始。 和以前一样,我们需要核心控制器运行时库<code>(core controller-runtime library)
</code>,客户端包<code>(client package)</code>以及API类型包<code>(the package for our API types)</code>.</p>

<pre><code>package controllers

import (
    &quot;context&quot;

    &quot;github.com/go-logr/logr&quot;
    ctrl &quot;sigs.k8s.io/controller-runtime&quot;
    &quot;sigs.k8s.io/controller-runtime/pkg/client&quot;

    batchv1 &quot;tutorial.kubebuilder.io/project/api/v1&quot;
)
</code></pre>

<p>接下来,kubebuilder为我们提供了基本的协调代码结构,几乎每个协调程序都需要日志,并且能够获取对象,所以,我们需要这些都是能够开箱即用的。</p>

<pre><code>// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Log logr.Logger
}
</code></pre>

<p>大多数控制器最终会在集群上运行,因此,他们需要RBAC权限,可能第一次你可以给他们赋最低权限,但是随着我们添加更加丰富的功能, 我们可能需要重新赋权。</p>

<pre><code>// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch
</code></pre>

<p><code>Reconcile</code> 事实上只是对单个对象做协调,通常,我们的请求里面只有一个对象名称,但是,我们可以通过客户端从缓存获取这个对象的其他数据。</p>

<p>如果我们返回一个空结果而没有任何的错误信息,这意味着我们已经成功地纠正了这个对象。并且如果后续我们没有修改过相关数据,我们不会再尝试去纠正/协调。</p>

<p>大多数控制器需要一个日志句柄和一个上下文(context),所以,我们需要设置这些。</p>

<p><code>context</code>可以用来取消请求,追踪操作,通常,它是所有方法的第一个参数,<code>Backgroup</code>这个上下文只是一个没有任何额外数据或时序限制的基本上下文,你可以使用其他方法来扩展他。</p>

<p>日志句柄可以让我记录日志,<code>controller-runtime</code>使用一个名叫<code>logr</code>的日志库来进行结构化日志输出,待会我们就能看到,通过键值对的方式填写静态消息来进行日志记录. 我们也可以在协调方法的顶部预先分配一些键值对,这样可能比较方便我们进行日志记录。</p>

<pre><code>func (r *CronJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    _ = context.Background()
    _ = r.Log.WithValues(&quot;cronjob&quot;, req.NamespacedName)

    // your logic here

    return ctrl.Result{}, nil
}
</code></pre>

<p>最后,我们需要把这个纠正器<code>reconciler</code>加入到管理器, 这样,在管理器启动的时候,就会启动它。</p>

<p>现在,我们只需要关注这个协调程序在CronJob上运行,后面,我们会用它来标记我们关心的对象。</p>

<pre><code>func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&amp;batchv1.CronJob{}).
        Complete(r)
}
</code></pre>

<p>现在,我们看到了协调器的基本结构, 接下来需要填写CronJob的逻辑。</p>

<h2 id="实现一个控制器">实现一个控制器</h2>

<p>实现CronJob的基本逻辑如下:
1. 加载指定的CronJob
2. 列出所有活跃的jobs,并且更新它们的状态
3. 根据历史限制设置清除老的jobs
4. 检查任务是否被暂停(如果暂停了,我们不需要做任何事情)
5. 获取下一次计划的运行时间等信息。
6. 如果job按照计划运行,查看我们设置的并发策略是否阻止。(如果没有超过截止时间运行,根据并发策略并不会阻止其运行）
7. 存在正在运行的job(自动完成)或者现在这个时间点是计划运行时间点需要重新排队。</p>

<p>我们从入口开始,你会看到相比于之前用脚手架搭建的基础结构,我们需要导入更多的包.</p>

<pre><code>package controllers

import (
    &quot;context&quot;
    &quot;fmt&quot;
    &quot;sort&quot;
    &quot;time&quot;

    &quot;github.com/go-logr/logr&quot;
    &quot;github.com/robfig/cron&quot;
    kbatch &quot;k8s.io/api/batch/v1&quot;
    corev1 &quot;k8s.io/api/core/v1&quot;
    apierrs &quot;k8s.io/apimachinery/pkg/api/errors&quot;
    metav1 &quot;k8s.io/apimachinery/pkg/apis/meta/v1&quot;
    &quot;k8s.io/apimachinery/pkg/runtime&quot;
    ref &quot;k8s.io/client-go/tools/reference&quot;
    ctrl &quot;sigs.k8s.io/controller-runtime&quot;
    &quot;sigs.k8s.io/controller-runtime/pkg/client&quot;

    batch &quot;tutorial.kubebuilder.io/project/api/v1&quot;
)
</code></pre>

<p>接下来,我们需要在调节器<code>Reconciler</code>做更多的事情:</p>

<p>我们需要一个<code>Scheme</code>,以便调用一些helper中的包和引用,我们需要一个<code>Clock</code>,这可以帮助我们在测试中伪造时间。</p>

<pre><code>// CronJobReconciler reconciles a CronJob object
type CronJobReconciler struct {
    client.Client
    Log    logr.Logger
    Scheme *runtime.Scheme
    Clock
}
</code></pre>

<ul>
<li>Clock</li>
</ul>

<p>我们需要模拟一个时钟,这样可以方便我们在测试的时候更容易及时调整,而真正的时钟是调用<code>time.Now()</code>.</p>

<pre><code>type realClock struct{}

func (_ realClock) Now() time.Time { return time.Now() }

// clock knows how to get the current time.
// It can be used to fake out timing for testing.
type Clock interface {
    Now() time.Time
}
</code></pre>

<ul>
<li>ignoreNotFound</li>
</ul>

<p>我们通常希望忽略(而不是重新排队)NotFound错误,因为一旦对象存在,会触发协调请求,并且,及时我们触发了重排序也没有任何的作用。</p>

<pre><code>func ignoreNotFound(err error) error {
    if apierrs.IsNotFound(err) {
        return nil
    }
    return err
}
</code></pre>

<p>注意: 现在,我们需要更多的RBAC权限, 因为需要创建和管理job, 所以,我们需要更多的权限。</p>

<pre><code>// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch.tutorial.kubebuilder.io,resources=cronjobs/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=batch,resources=jobs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=batch,resources=jobs/status,verbs=get
</code></pre>

<p>现在,我们进入控制器的核心&mdash; 协调逻辑<code>the reconciler logic</code>.</p>

<pre><code>var (
    scheduledTimeAnnotation = &quot;batch.tutorial.kubebuilder.io/scheduled-at&quot;
)

func (r *CronJobReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
    ctx := context.Background()
    log := r.Log.WithValues(&quot;cronjob&quot;, req.NamespacedName)
</code></pre>

<h4 id="1-通过name加载cronjob">1. 通过name加载CronJob</h4>

<p>我们通过client获取CronJob。 所有的方法都将<code>context(可以取消)</code>作为第一个参数,并将相关的对象作为最后一个参数. <code>Get</code>这个方法有点特殊,他把<code>NamespacedName</code>作为中间参数。而大多数方法没有中间参数,接下来,我们会看到。</p>

<p>许多的客户端方法最后也采用了可变参数选项。</p>

<pre><code>
    var cronJob batch.CronJob
    if err := r.Get(ctx, req.NamespacedName, &amp;cronJob); err != nil {
        log.Error(err, &quot;unable to fetch CronJob&quot;)
        // we'll ignore not-found errors, since they can't be fixed by an immediate
        // requeue (we'll need to wait for a new notification), and we can get them
        // on deleted requests.
        return ctrl.Result{}, ignoreNotFound(err)
    }
</code></pre>

<h4 id="2-显示所有活跃的job-并且更新他们的状态">2. 显示所有活跃的job,并且更新他们的状态。</h4>

<p>如果想完全更新状态,我们需要列出此namespace下属于此CronJob的所有子Jobs, 与Get一样,我们可以使用List方法列出所有的子Jobs。注意,我们使用<code>variadic</code> 选项来设置namespace和字段匹配(事实上,我们在下面设置了索引查找)。</p>

<pre><code>
    var cronJob batch.CronJob
    if err := r.Get(ctx, req.NamespacedName, &amp;cronJob); err != nil {
        log.Error(err, &quot;unable to fetch CronJob&quot;)
        // we'll ignore not-found errors, since they can't be fixed by an immediate
        // requeue (we'll need to wait for a new notification), and we can get them
        // on deleted requests.
        return ctrl.Result{}, ignoreNotFound(err)
    }

</code></pre>

<p>一旦我们有了所有的jobs,我们就能够把这些jobs分成活跃的,成功完成的,失败的,根据最近的运行的job, 我们可以很方便的记录其中的状态。</p>

<pre><code>    // find the active list of jobs
    var activeJobs []*kbatch.Job
    var successfulJobs []*kbatch.Job
    var failedJobs []*kbatch.Job
    var mostRecentTime *time.Time // find the last run so we can update the status
</code></pre>

<ul>
<li>isJobFinished</li>
</ul>

<p>如果一个job已经&rdquo;完成&rdquo;了,他有个标记是成功的还是失败的. <code>Status</code>条件运行我们添加更多的扩展状态信息,其他人和控制器可以检查这些信息.比如检查是否完成和健康检查(非K8s的健康检查)之类的事情。</p>

<pre><code>    isJobFinished := func(job *kbatch.Job) (bool, kbatch.JobConditionType) {
        for _, c := range job.Status.Conditions {
            if (c.Type == kbatch.JobComplete || c.Type == kbatch.JobFailed) &amp;&amp; c.Status == corev1.ConditionTrue {
                return true, c.Type
            }
        }

        return false, &quot;&quot;
    }
</code></pre>

<ul>
<li>getScheduledTimeForJob</li>
</ul>

<p>我们可以使用helper 程序 从作业一开始创建的时候添加的注解(annotation)中提取计划时间。</p>

<pre><code>    getScheduledTimeForJob := func(job *kbatch.Job) (*time.Time, error) {
        timeRaw := job.Annotations[scheduledTimeAnnotation]
        if len(timeRaw) == 0 {
            return nil, nil
        }

        timeParsed, err := time.Parse(time.RFC3339, timeRaw)
        if err != nil {
            return nil, err
        }
        return &amp;timeParsed, nil
    }
</code></pre>

<pre><code>    for i, job := range childJobs.Items {
        _, finishedType := isJobFinished(&amp;job)
        switch finishedType {
        case &quot;&quot;: // ongoing
            activeJobs = append(activeJobs, &amp;childJobs.Items[i])
        case kbatch.JobFailed:
            failedJobs = append(failedJobs, &amp;childJobs.Items[i])
        case kbatch.JobComplete:
            successfulJobs = append(successfulJobs, &amp;childJobs.Items[i])
        }

        // We'll store the launch time in an annotation, so we'll reconsitute that from
        // the active jobs themselves.
        scheduledTimeForJob, err := getScheduledTimeForJob(&amp;job)
        if err != nil {
            log.Error(err, &quot;unable to parse schedule time for child job&quot;, &quot;job&quot;, &amp;job)
            continue
        }
        if scheduledTimeForJob != nil {
            if mostRecentTime == nil {
                mostRecentTime = scheduledTimeForJob
            } else if mostRecentTime.Before(*scheduledTimeForJob) {
                mostRecentTime = scheduledTimeForJob
            }
        }
    }

    if mostRecentTime != nil {
        cronJob.Status.LastScheduleTime = &amp;metav1.Time{Time: *mostRecentTime}
    } else {
        cronJob.Status.LastScheduleTime = nil
    }
    cronJob.Status.Active = nil
    for _, activeJob := range activeJobs {
        jobRef, err := ref.GetReference(r.Scheme, activeJob)
        if err != nil {
            log.Error(err, &quot;unable to make reference to active job&quot;, &quot;job&quot;, activeJob)
            continue
        }
        cronJob.Status.Active = append(cronJob.Status.Active, *jobRef)
    }
</code></pre>

<p>在这里,我们记录了较高的日志级别以便我们观察到作业数量,方便我们调试。值得注意的是,我们并没有使用格式化的字符串，而是使用了固定消息方式并使用键值对的方式来添加额外的附加信息。这样就可以更加轻松的过滤和查询日志行。</p>

<pre><code>    log.V(1).Info(&quot;job count&quot;, &quot;active jobs&quot;, len(activeJobs), &quot;successful jobs&quot;, len(successfulJobs), &quot;failed jobs&quot;, len(failedJobs))
</code></pre>

<p>使用我们上面收集到的时间日期,我们可以更新CRD的状态。和以前一样,我们使用我们的客户端。为了更新子资源的状态，我们使用client的<code>Update</code>方法来更新<code>Status</code>部分.</p>

<p>更新子资源状态可以忽略对规范的更改,因为,它不太可能和其他更新冲突,并且它也具有单独的权限。</p>

<pre><code>    if err := r.Status().Update(ctx, &amp;cronJob); err != nil {
        log.Error(err, &quot;unable to update CronJob status&quot;)
        return ctrl.Result{}, err
    }
</code></pre>

<p>一旦我们更新了状态,我们可以确保真实世界的状态符合我们期望的状态。</p>

<h4 id="3-根据历史限制清理老的job">3. 根据历史限制清理老的job</h4>

<p>我们尝试清理旧的job,这样我们就不会留下太多的历史.</p>

<pre><code>    // NB: deleting these is &quot;best effort&quot; -- if we fail on a particular one,
    // we won't requeue just to finish the deleting.
    if cronJob.Spec.FailedJobsHistoryLimit != nil {
        sort.Slice(failedJobs, func(i, j int) bool {
            if failedJobs[i].Status.StartTime == nil {
                return failedJobs[j].Status.StartTime != nil
            }
            return failedJobs[i].Status.StartTime.Before(failedJobs[j].Status.StartTime)
        })
        for i, job := range failedJobs {
            if err := r.Delete(ctx, job); err != nil {
                log.Error(err, &quot;unable to delete old failed job&quot;, &quot;job&quot;, job)
            }
            if int32(i) &gt;= *cronJob.Spec.FailedJobsHistoryLimit {
                break
            }
        }
    }

    if cronJob.Spec.SuccessfulJobsHistoryLimit != nil {
        sort.Slice(successfulJobs, func(i, j int) bool {
            if successfulJobs[i].Status.StartTime == nil {
                return successfulJobs[j].Status.StartTime != nil
            }
            return successfulJobs[i].Status.StartTime.Before(successfulJobs[j].Status.StartTime)
        })
        for i, job := range successfulJobs {
            if err := r.Delete(ctx, job); err != nil {
                log.Error(err, &quot;unable to delete old successful job&quot;, &quot;job&quot;, job)
            }
            if int32(i) &gt;= *cronJob.Spec.SuccessfulJobsHistoryLimit {
                break
            }
        }
    }
</code></pre>

<h4 id="4-检查job是否被暂停">4. 检查job是否被暂停</h4>

<p>如果对象已经被暂停,说明我们不想执行任何的job,所以,我们需要立即停止,我们期望中断正在运行的job(任务),而且只是希望暂时暂停运行而不是删除对象,所以,这个功能是非常有用的.</p>

<pre><code>    if cronJob.Spec.Suspend != nil &amp;&amp; *cronJob.Spec.Suspend {
        log.V(1).Info(&quot;cronjob suspended, skipping&quot;)
        return ctrl.Result{}, nil
    }
</code></pre>

<h4 id="5-获取下一次计划运行时间">5. 获取下一次计划运行时间</h4>

<p>如果我们没有暂停这个cronjob,我们需要计算按照计划下一次运行的时间,以及我们是否还存在尚未知执行的任务。</p>

<ul>
<li>getNextSchedule</li>
</ul>

<p>我们可以使用现有的cron库来计算下一个预定时间, 如果我们找不到最后一次运行时间,我们就从上一次运行开始计算合适的时间或者重新创建一个CronJob.</p>

<p>我们我们已经错过了太多的计划时间并且我们没有设置截止时间,我们不会尝试补偿运行,因为这样就不会再重启控制器或者锲子的时候造成问题。</p>

<p>否则,我们只返回错过的运行计划(使用最新的运行计划)和下次运行计划,这样可以方便我们知道何时再次镜像纠正。</p>

<pre><code>    getNextSchedule := func(cronJob *batch.CronJob, now time.Time) (lastMissed *time.Time, next time.Time, err error) {
        sched, err := cron.ParseStandard(cronJob.Spec.Schedule)
        if err != nil {
            return nil, time.Time{}, fmt.Errorf(&quot;Unparseable schedule %q: %v&quot;, cronJob.Spec.Schedule, err)
        }

        // for optimization purposes, cheat a bit and start from our last observed run time
        // we could reconstitute this here, but there's not much point, since we've
        // just updated it.
        var earliestTime time.Time
        if cronJob.Status.LastScheduleTime != nil {
            earliestTime = cronJob.Status.LastScheduleTime.Time
        } else {
            earliestTime = cronJob.ObjectMeta.CreationTimestamp.Time
        }
        if cronJob.Spec.StartingDeadlineSeconds != nil {
            // controller is not going to schedule anything below this point
            schedulingDeadline := now.Add(-time.Second * time.Duration(*cronJob.Spec.StartingDeadlineSeconds))

            if schedulingDeadline.After(earliestTime) {
                earliestTime = schedulingDeadline
            }
        }
        if earliestTime.After(now) {
            return nil, sched.Next(now), nil
        }

        starts := 0
        for t := sched.Next(earliestTime); !t.After(now); t = sched.Next(t) {
            lastMissed = &amp;t
            // An object might miss several starts. For example, if
            // controller gets wedged on Friday at 5:01pm when everyone has
            // gone home, and someone comes in on Tuesday AM and discovers
            // the problem and restarts the controller, then all the hourly
            // jobs, more than 80 of them for one hourly scheduledJob, should
            // all start running with no further intervention (if the scheduledJob
            // allows concurrency and late starts).
            //
            // However, if there is a bug somewhere, or incorrect clock
            // on controller's server or apiservers (for setting creationTimestamp)
            // then there could be so many missed start times (it could be off
            // by decades or more), that it would eat up all the CPU and memory
            // of this controller. In that case, we want to not try to list
            // all the missed start times.
            starts++
            if starts &gt; 100 {
                // We can't get the most recent times so just return an empty slice
                return nil, time.Time{}, fmt.Errorf(&quot;Too many missed start times (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.&quot;)
            }
        }
        return lastMissed, sched.Next(now), nil
    }
</code></pre>

<pre><code>    // figure out the next times that we need to create
    // jobs at (or anything we missed).
    missedRun, nextRun, err := getNextSchedule(&amp;cronJob, r.Now())
    if err != nil {
        log.Error(err, &quot;unable to figure out CronJob schedule&quot;)
        // we don't really care about requeuing until we get an update that
        // fixes the schedule, so don't return an error
        return ctrl.Result{}, nil
    }
</code></pre>

<p>准备将最后的请求重新排队直到下一个job,然后确定我们是否真的需要运行这个job。</p>

<pre><code>    scheduledResult := ctrl.Result{RequeueAfter: nextRun.Sub(r.Now())} // save this so we can re-use it elsewhere
    log = log.WithValues(&quot;now&quot;, r.Now(), &quot;next run&quot;, nextRun)
</code></pre>

<h4 id="6-按照计划运行新的job-且没有超过截止期限-也就不会被并发策略影响">6. 按照计划运行新的job,且没有超过截止期限,也就不会被并发策略影响</h4>

<p>如果我们错过了计划的运行时间,但仍然处于他启动的最后期限之内,我们仍然需要运行这个job.</p>

<pre><code>    if missedRun == nil {
        log.V(1).Info(&quot;no upcoming scheduled times, sleeping until next&quot;)
        return scheduledResult, nil
    }

    // make sure we're not too late to start the run
    log = log.WithValues(&quot;current run&quot;, missedRun)
    tooLate := false
    if cronJob.Spec.StartingDeadlineSeconds != nil {
        tooLate = missedRun.Add(time.Duration(*cronJob.Spec.StartingDeadlineSeconds) * time.Second).Before(r.Now())
    }
    if tooLate {
        log.V(1).Info(&quot;missed starting deadline for last run, sleeping till next&quot;)
        // TODO(directxman12): events
        return scheduledResult, nil
    }

</code></pre>

<p>如果我们按照执行计划必须得运行一个job,我们需要等待现有任务完成来替换已经存在的任务或者运行一个新的,如果因为缓存延迟导致了我们读取信息过期,我们需要得到最新信息后重新排队。</p>

<pre><code>    // figure out how to run this job -- concurrency policy might forbid us from running
    // multiple at the same time...
    if cronJob.Spec.ConcurrencyPolicy == batch.ForbidConcurrent &amp;&amp; len(activeJobs) &gt; 0 {
        log.V(1).Info(&quot;concurrency policy blocks concurrent runs, skipping&quot;, &quot;num active&quot;, len(activeJobs))
        return scheduledResult, nil
    }

    // ...or instruct us to replace existing ones...
    if cronJob.Spec.ConcurrencyPolicy == batch.ReplaceConcurrent {
        for _, activeJob := range activeJobs {
            // we don't care if the job was already deleted
            if err := r.Delete(ctx, activeJob); ignoreNotFound(err) != nil {
                log.Error(err, &quot;unable to delete active job&quot;, &quot;job&quot;, activeJob)
                return ctrl.Result{}, err
            }
        }
    }

</code></pre>

<pre><code>Once we've figured out what to do with existing jobs, we'll actually create our desired job
</code></pre>

<ul>
<li><p>constructJobForCronJob</p>

<pre><code>// actually make the job...
job, err := constructJobForCronJob(&amp;cronJob, *missedRun)
if err != nil {
    log.Error(err, &quot;unable to construct job from template&quot;)
    // don't bother requeuing until we get a change to the spec
    return scheduledResult, nil
}

// ...and create it on the cluster
if err := r.Create(ctx, job); err != nil {
    log.Error(err, &quot;unable to create Job for CronJob&quot;, &quot;job&quot;, job)
    return ctrl.Result{}, err
}

log.V(1).Info(&quot;created Job for CronJob run&quot;, &quot;job&quot;, job)
</code></pre></li>
</ul>

<h4 id="7-存在正在运行的job-自动完成-或者现在这个时间点是计划运行时间时需要重新排队">7. 存在正在运行的job(自动完成)或者现在这个时间点是计划运行时间时需要重新排队</h4>

<p>最后,我们会返回上面准备的结果,表示我们想要在下次运行时需要重新启动。这被称之为最大期限&mdash;如果其他事情发生了变化,比如我们的工作开始或者结束,我们会更新状态,这会触发再次纠正/协调。</p>

<pre><code>    // we'll requeue once we see the running job, and update our status
    return scheduledResult, nil
}

</code></pre>

<h4 id="设置">设置</h4>

<p>最后,我们需要更新我们的设置,这样是为了让协调器能够通过所有者查找到这些job, 我们需要一个索引,我们声明一个索引,我们可以把这个字段和客户端一起使用,用来描述如何从job中提取索引值。</p>

<p>索引器将自动为我们处理namespace,因此如果job具有CronJob所有者(owner)标识,我们只需要提取出所有者标识(owner)。</p>

<p>另外,我们会通知管理器这个控制器拥有一些Jobs,以便在Job发生变化,删除等时自动调用底层CronJob上的Reconcile。</p>

<pre><code>var (
    jobOwnerKey = &quot;.metadata.controller&quot;
    apiGVStr    = batch.GroupVersion.String()
)

func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
    // set up a real clock, since we're not in a test
    if r.Clock == nil {
        r.Clock = realClock{}
    }

    if err := mgr.GetFieldIndexer().IndexField(&amp;kbatch.Job{}, jobOwnerKey, func(rawObj runtime.Object) []string {
        // grab the job object, extract the owner...
        job := rawObj.(*kbatch.Job)
        owner := metav1.GetControllerOf(job)
        if owner == nil {
            return nil
        }
        // ...make sure it's a CronJob...
        if owner.APIVersion != apiGVStr || owner.Kind != &quot;CronJob&quot; {
            return nil
        }

        // ...and if so, return it
        return []string{owner.Name}
    }); err != nil {
        return err
    }

    return ctrl.NewControllerManagedBy(mgr).
        For(&amp;batch.CronJob{}).
        Owns(&amp;kbatch.Job{}).
        Complete(r)
}
</code></pre>

<p>现在我们有个工作的控制器,我们需要进行测试,如果没有问题,我们可以部署。</p>

<h2 id="关于main-我们需要说点什么">关于Main,我们需要说点什么?</h2>

<p>首先,记得我们之前说的再次回到<code>main.go</code>吗? 让我们看看发生了那些变化,需要添加些什么?</p>

<ul>
<li><p>Imports</p>

<pre><code>
var (
jobOwnerKey = &quot;.metadata.controller&quot;
apiGVStr    = batch.GroupVersion.String()
)

func (r *CronJobReconciler) SetupWithManager(mgr ctrl.Manager) error {
// set up a real clock, since we're not in a test
if r.Clock == nil {
    r.Clock = realClock{}
}

if err := mgr.GetFieldIndexer().IndexField(&amp;kbatch.Job{}, jobOwnerKey, func(rawObj runtime.Object) []string {
    // grab the job object, extract the owner...
    job := rawObj.(*kbatch.Job)
    owner := metav1.GetControllerOf(job)
    if owner == nil {
        return nil
    }
    // ...make sure it's a CronJob...
    if owner.APIVersion != apiGVStr || owner.Kind != &quot;CronJob&quot; {
        return nil
    }

    // ...and if so, return it
    return []string{owner.Name}
}); err != nil {
    return err
}

return ctrl.NewControllerManagedBy(mgr).
    For(&amp;batch.CronJob{}).
    Owns(&amp;kbatch.Job{}).
    Complete(r)
}
</code></pre></li>
</ul>

<p>注意第一个区别是:kubebuilder已经将新的API group包(<code>kbatchvabeta1</code>)添加到我们的Scheme中,这意味着我们可以在控制器中使用这些对象。</p>

<p>我们还需要添加kubernetes batch v1 schme,因为我们需要我们创建和列表显示Job。</p>

<pre><code>var (
    scheme   = runtime.NewScheme()
    setupLog = ctrl.Log.WithName(&quot;setup&quot;)
)

func init() {

    kbatchv1beta1.AddToScheme(scheme) // we've added this ourselves
    batchv1.AddToScheme(scheme)
    // +kubebuilder:scaffold:scheme
}

</code></pre>

<p>另外一个不同的事情就是<code>kubebuilder</code>添加了一个block来调用我们的CronJob控制的<code>SetupWithManager</code>方法。这是因为我们自己也使用了一个<code>Scheme</code>,我们需要把我们自己把它注册给协调器。</p>

<pre><code>func main() {

</code></pre>

<ul>
<li><p>之前的代码:</p>

<pre><code>
var metricsAddr string
flag.StringVar(&amp;metricsAddr, &quot;metrics-addr&quot;, &quot;:8080&quot;, &quot;The address the metric endpoint binds to.&quot;)
flag.Parse()

ctrl.SetLogger(zap.Logger(true))

mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
if err != nil {
    setupLog.Error(err, &quot;unable to start manager&quot;)
    os.Exit(1)
}
</code></pre></li>
</ul>

<p>现在的代码:</p>

<pre><code>    var metricsAddr string
    flag.StringVar(&amp;metricsAddr, &quot;metrics-addr&quot;, &quot;:8080&quot;, &quot;The address the metric endpoint binds to.&quot;)
    flag.Parse()

    ctrl.SetLogger(zap.Logger(true))

    mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{Scheme: scheme, MetricsBindAddress: metricsAddr})
    if err != nil {
        setupLog.Error(err, &quot;unable to start manager&quot;)
        os.Exit(1)
    }
</code></pre>

<ul>
<li><p>之前的代码:</p>

<pre><code>setupLog.Info(&quot;starting manager&quot;)
if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
    setupLog.Error(err, &quot;problem running manager&quot;)
    os.Exit(1)
}
</code></pre></li>
</ul>

<p>现在的代码:</p>

<pre><code>}
</code></pre>

<p>现在,我们实现了控制器。</p>

<h2 id="1-8-运行和部署控制器">1.8 运行和部署控制器</h2>

<p>为测试控制器,我们可以再本地集群运行它,在做这件事情之前,我们需要安装我们的CRDs(依照快速开始),如果需要,你可以使用控制器工具自动更新yaml清单。</p>

<pre><code>make install
</code></pre>

<p>现在我们已经安装了CRD,我们可以运作控制器,由于RBAC权限之前已经设置过,我们就不用担心权限问题了.</p>

<p>换个终端运行:</p>

<pre><code>make run

</code></pre>

<p>你可以从控制器中看到启动日志,但是它现在还没有做任何事情.</p>

<p>现在, 我们需要一个CronJob来测试,所以,我们需要些一个yaml(<code>config/samples/batch_v1_cronjob.yaml</code>)来测试:</p>

<pre><code>apiVersion: batch.tutorial.kubebuilder.io/v1
kind: CronJob
metadata:
  name: cronjob-sample
spec:
  schedule: &quot;*/1 * * * *&quot;
  startingDeadlineSeconds: 60
  concurrencyPolicy: Allow # explicitly specify, but Allow is also default.
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
</code></pre>

<pre><code>kubectl create -f config/samples/batch_v1_cronjob.yaml
</code></pre>

<p>查看cronJob的状态:</p>

<pre><code>kubectl get cronjob.batch.tutorial.kubebuilder.io -o yaml
kubectl get job
</code></pre>

<p>现在,我们知道了它正在运行中,在集群中也能允许,停止<code>make run</code>调用,然后运行:</p>

<pre><code>make docker-build docker-push IMG=&lt;some-registry&gt;/controller
make deploy

</code></pre>

<p>我们可以再次列出cronjobs也能看到控制器再次运行了。</p>
]]></content>
		</item>
		
		<item>
			<title>[Beku] 如何使用三行代码发布你的应用到Kubernetes</title>
			<link>https://yulibaozi.com/posts/kubernetes/extend/2018-12-03-beku/</link>
			<pubDate>Sat, 21 Mar 2020 20:15:22 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/extend/2018-12-03-beku/</guid>
			<description>beku的概要 众所周知，Kubernetes主要分为两大部分:部署时和运行时,Beku的定位在部署时,无需任何额外的心智负担,快速编写Kubernetes资源对象然后发布到到Kubernetes集群，这对那些开发PaaS平台的公司来说显得尤其重要,尤其是在前后端人员接口对接时和资安全保障。beku就是这样, 一个Golang人性化Kubernetes资源对象创建库,极简，无额外心智负担。
beku的初衷  部署应用时：
 beku期望同僚们在发布应用的时候,
 不用担心某个字段应该放在yaml或者json的那个层级(比如:imagePullSecrets这个字段我应该填在Pod里面还是Containers里,还是Deployment里,可能你一会记得,过一段时间你又不记得了); 也不用顾虑这个字段名是否填写正确,字段所对应的值是否符合Kubernetes要求（比如:imagepullsecrets应该是imagePullSecrets还是imagePullSecret呢; 更不用忧虑重复填写某字段不一致导致发布失败(比如:我们在使用Deployment的时候，通常会填写Pod的Labels,然后填写Deployment的Selectors,如果你需要Service时，还需要填写Service的Selectors,这可能会因为某个很小的错误填写导致发布不符合预期,引发这样的错误其实是很没必要的)等一些这些没必要担心的问题。   Paas平台研发时:
 beku期望同僚们在开发PaaS平台前后端对接时，后端开发者不用把Kubernetes资源对象的json和交给前端填写，由于在于Kubernetes资源对象的层级比较偏复杂，这需要时间成本沟通；在沟通层级的时候，还需要沟通相关的字段，这会分散前端开发者的精力和增加开发耗时；另一方面可能也会带来安全上面的问题，比如:某些存在不怀好意的人会尝试填写其他字段来试图攻击你的应用。如果使用beku，你可以很好的解决上面的问题，在解决和前端沟通成本和减少开发的时间成本时，安全也得到了保障，后端也增加了掌控能力,同时也达到了快速开发的目地,岂不美哉, 我们仍然举个示例来说明,假设我们在开发Deployment和Service的联合发布,前端可能只需要传入:
 { &amp;quot;name&amp;quot;:&amp;quot;httpd&amp;quot;, &amp;quot;namespace&amp;quot;:&amp;quot;yulibaozi&amp;quot;, &amp;quot;serviceType&amp;quot;:&amp;quot;NodePort&amp;quot;, &amp;quot;port&amp;quot;:&amp;quot;8080&amp;quot;, &amp;quot;replics&amp;quot;:2 }  前端只需要传入这些必要的字段,然后调用beku就可以发布Deployment和Service两个Kubernetes资源对象到Kubernetes了,是不是很惊叹太过简单了，你可能还会忧虑高阶需求能否得到满足,beku当然能满足,比如环境变量，挂载ceph和NFS等，不过这些都是选填,你可以很轻松的找到对他们的支持。
beku的特性  自动发布资源对象到Kubernetes 灵活的自定义开发 极简的JSON和YAML输入/输出 自动判断资源对象的必要字段 人性化的资源对象关联发布 严谨的QOS等级设置 准确的字段自动填充 写意的链式调用  使用示例 接下来,我们以一个示例来说明如何使用beku? 这次示例的的目标是Deployment和Service的关联发布?
使用beku创建deployment资源:
dp, err := beku.NewDeployment().SetNamespaceAndName(&amp;quot;yulibaozi&amp;quot;, &amp;quot;http&amp;quot;). SetPodLabels(map[string]string{&amp;quot;app&amp;quot;: &amp;quot;http&amp;quot;}).SetContainer(&amp;quot;http&amp;quot;, &amp;quot;wucong60/kube-node-demo1:v1&amp;quot;, 8081).Finish() if err != nil { panic(err) }  接下来使用已有的Deployment的创建Service:
svc, err := beku.DeploymentToSvc(dp, beku.ServiceTypeNodePort, false) if err !</description>
			<content type="html"><![CDATA[

<h3 id="beku的概要">beku的概要</h3>

<p>众所周知，Kubernetes主要分为两大部分:部署时和运行时,Beku的定位在部署时,无需任何额外的心智负担,快速编写Kubernetes资源对象然后发布到到Kubernetes集群，这对那些开发PaaS平台的公司来说显得尤其重要,尤其是在前后端人员接口对接时和资安全保障。beku就是这样, 一个Golang人性化Kubernetes资源对象创建库,极简，无额外心智负担。</p>

<h3 id="beku的初衷">beku的初衷</h3>

<blockquote>
<p>部署应用时：</p>
</blockquote>

<p>beku期望同僚们在发布应用的时候,</p>

<ol>
<li>不用担心某个字段应该放在yaml或者json的那个层级(比如:imagePullSecrets这个字段我应该填在Pod里面还是Containers里,还是Deployment里,可能你一会记得,过一段时间你又不记得了);</li>
<li>也不用顾虑这个字段名是否填写正确,字段所对应的值是否符合Kubernetes要求（比如:imagepullsecrets应该是imagePullSecrets还是imagePullSecret呢;</li>
<li>更不用忧虑重复填写某字段不一致导致发布失败(比如:我们在使用Deployment的时候，通常会填写Pod的Labels,然后填写Deployment的Selectors,如果你需要Service时，还需要填写Service的Selectors,这可能会因为某个很小的错误填写导致发布不符合预期,引发这样的错误其实是很没必要的)等一些这些没必要担心的问题。</li>
</ol>

<blockquote>
<p>Paas平台研发时:</p>
</blockquote>

<p>beku期望同僚们在开发PaaS平台前后端对接时，后端开发者不用把Kubernetes资源对象的json和交给前端填写，由于在于Kubernetes资源对象的层级比较偏复杂，这需要时间成本沟通；在沟通层级的时候，还需要沟通相关的字段，这会分散前端开发者的精力和增加开发耗时；另一方面可能也会带来安全上面的问题，比如:某些存在不怀好意的人会尝试填写其他字段来试图攻击你的应用。如果使用beku，你可以很好的解决上面的问题，在解决和前端沟通成本和减少开发的时间成本时，安全也得到了保障，后端也增加了掌控能力,同时也达到了快速开发的目地,岂不美哉,
我们仍然举个示例来说明,假设我们在开发Deployment和Service的联合发布,前端可能只需要传入:</p>

<pre><code>    {
        
        &quot;name&quot;:&quot;httpd&quot;,
        &quot;namespace&quot;:&quot;yulibaozi&quot;,
        &quot;serviceType&quot;:&quot;NodePort&quot;,
        &quot;port&quot;:&quot;8080&quot;,
        &quot;replics&quot;:2
    }
</code></pre>

<p>前端只需要传入这些必要的字段,然后调用beku就可以发布Deployment和Service两个Kubernetes资源对象到Kubernetes了,是不是很惊叹太过简单了，你可能还会忧虑高阶需求能否得到满足,beku当然能满足,比如环境变量，挂载ceph和NFS等，不过这些都是选填,你可以很轻松的找到对他们的支持。</p>

<h3 id="beku的特性">beku的特性</h3>

<ul>
<li>自动发布资源对象到Kubernetes</li>
<li>灵活的自定义开发</li>
<li>极简的JSON和YAML输入/输出</li>
<li>自动判断资源对象的必要字段</li>
<li>人性化的资源对象关联发布</li>
<li>严谨的QOS等级设置</li>
<li>准确的字段自动填充</li>
<li>写意的链式调用</li>
</ul>

<h3 id="使用示例">使用示例</h3>

<p>接下来,我们以一个示例来说明如何使用beku? 这次示例的的目标是Deployment和Service的关联发布?</p>

<p>使用beku创建deployment资源:</p>

<pre><code>dp, err := beku.NewDeployment().SetNamespaceAndName(&quot;yulibaozi&quot;, &quot;http&quot;).
		SetPodLabels(map[string]string{&quot;app&quot;: &quot;http&quot;}).SetContainer(&quot;http&quot;, &quot;wucong60/kube-node-demo1:v1&quot;, 8081).Finish()
	if err != nil {
		panic(err)
	}
</code></pre>

<p>接下来使用已有的Deployment的创建Service:</p>

<pre><code>svc, err := beku.DeploymentToSvc(dp, beku.ServiceTypeNodePort, false)
	if err != nil {
		panic(err)
	}
</code></pre>

<p>上面两个代码就已经创建好了两个Kubernetes资源对象，只待发布，如果你需要自动发布,你需要向beku注册ApiServer,代码如下:</p>

<pre><code>err := beku.RegisterK8sClient(&quot;http://192.168.0.183&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;)
	if err != nil {
		panic(err)
	}
同时,把Finish()改成Release(),DeploymentToSvc(dp, beku.ServiceTypeNodePort, true)
</code></pre>

<p>修改好了这两个地方,就可自动发布了，是不是很简单,这只是beku一个最基本的示例，他还有更优秀和更高级的用法，只待你去探索。</p>

<h3 id="beku的使用习惯">beku的使用习惯</h3>

<ol>
<li>beku的使用习惯是以NewXXX()开始链式调用, 最终以调用Finish()表示调用结束, 从而得到完整的kubernetes资源对象配置信息。</li>
<li>所有的填写都以SetXXX()开头, 所有的获取都以GetXXX()开头。</li>
<li>在使用beku时, 尽量不使用强转的方式来满足函数所需要的变量类型, 这会引发未知错误。</li>
<li>在使用beku时, 如果有函数的参数不知道填什么, 实现函数的注释中有相关阐述。</li>
<li>在beku的应用场景中, Pod中的第一个container往往有至高地位, 拥有优先设置的权利, 随着序列的变大越显得平凡, 比如:第一次设置环境的时候, 只会为第一个container设置, 而不会为第二个设置, 当你第二次调用设置环境变量方法时, 才会设置第二container的环境变量, 以此类推。</li>
<li>如果某结构体存在<strong>union</strong>字符时, 那么说明会同时创建两个Kubernetes资源对象, 例如:Deployment和Service的联合,PersistentVolume和PersistentVolumeClaim的联合</li>
</ol>

<h3 id="文以终结-但艺不止步">文以终结,但艺不止步</h3>

<p>项目及更多示例查看:<a href="https://github.com/yulibaozi/beku">https://github.com/yulibaozi/beku</a>
 欢迎您的各种建议、吐槽,issue,pr!</p>

<h3 id="上文的示例-完整代码如下">上文的示例,完整代码如下:</h3>

<pre><code>
import (
	&quot;fmt&quot;

	&quot;github.com/yulibaozi/beku&quot;
	&quot;k8s.io/api/apps/v1&quot;
	corev1 &quot;k8s.io/api/core/v1&quot;
)

type DepAndSvc struct {
	Namespace   string
	Name        string
	Labels      map[string]string
	Port        int32
	Image       string
	ImageName   string
	ServiceType beku.ServiceType
}

func main() {
	depAndSvcInfo := &amp;DepAndSvc{
		Namespace:   &quot;yulibaozi&quot;,
		Name:        &quot;http&quot;,
		Labels:      map[string]string{&quot;app&quot;: &quot;http&quot;},
		Port:        8081,
		Image:       &quot;wucong60/kube-node-demo1:v1&quot;,
		ImageName:   &quot;http&quot;,
		ServiceType: &quot;NodePort&quot;,
	}
	//注册Kubernetes的ApiServer
	err := beku.RegisterK8sClient(&quot;http://192.168.0.183&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;)
	if err != nil {
		panic(err)
	}
	//发布应用
	dp, svc, err := UnionDepAndSvc(depAndSvcInfo)
	if err != nil {
		panic(err)
	}
	fmt.Printf(&quot;%+v&quot;, svc)
	fmt.Printf(&quot;%+v&quot;, dp)
}

func UnionDepAndSvc(info *DepAndSvc) (dp *v1.Deployment, svc *corev1.Service, err error) {
	dp, err = beku.NewDeployment().SetNamespaceAndName(info.Namespace, info.Name).
		SetPodLabels(info.Labels).SetContainer(info.ImageName, info.Image, info.Port).Release()
	if err != nil {
		return
	}
	svc, err = beku.DeploymentToSvc(dp, info.ServiceType, true)
	if err != nil {
		return
	}
	return
}


</code></pre>
]]></content>
		</item>
		
		<item>
			<title>收集kubernetes控制台日志及元数据(fluent-bit&#43;elasticsearch&#43;kibana搭建)</title>
			<link>https://yulibaozi.com/posts/kubernetes/deploy/2019-12-21-fluent-bit&#43;elasticsearch&#43;kibana/</link>
			<pubDate>Sat, 21 Dec 2019 20:29:20 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/deploy/2019-12-21-fluent-bit&#43;elasticsearch&#43;kibana/</guid>
			<description>#### 前要   kubectl version
 Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;9&amp;quot;, GitVersion:&amp;quot;v1.9.3&amp;quot;, GitCommit:&amp;quot;d2835416544f298c919e2ead3be3d0864b52323b&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-02-07T12:22:21Z&amp;quot;, GoVersion:&amp;quot;go1.9.2&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;} Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;9&amp;quot;, GitVersion:&amp;quot;v1.9.3&amp;quot;, GitCommit:&amp;quot;d2835416544f298c919e2ead3be3d0864b52323b&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-02-07T11:55:20Z&amp;quot;, GoVersion:&amp;quot;go1.9.2&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}   namespace
 如果需要部署在非kube-system命令下,需要该所有yaml的namespace,例如:我所有的资源部署在logging这个namespace下。
 下载相关代码(主要是yaml文件)
 《elasticsearch、kibana》 https://github.com/kubernetes/kubernetes.git
《fluent-bit》 https://github.com/fluent/fluent-bit-kubernetes-logging.git
首先部署elasticsearch yaml目录:cluster/addons/fluentd-elasticsearch
 部署statefulset
kubectl create -f es-statefulset.yaml  我部署的时候镜像版本是:k8s.gcr.io/elasticsearch:v6.2.4
  初始化镜像: alpine:3.6
 部署configmap
kubectl create -f fluentd-es-configmap.yaml  部署Service
kubectl create -f es-service.yaml  查看service的信息并记下来,下面要用</description>
			<content type="html"><![CDATA[

<pre><code>#### 前要
</code></pre>

<blockquote>
<p>kubectl version</p>
</blockquote>

<pre><code>Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.3&quot;, GitCommit:&quot;d2835416544f298c919e2ead3be3d0864b52323b&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-07T12:22:21Z&quot;, GoVersion:&quot;go1.9.2&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.3&quot;, GitCommit:&quot;d2835416544f298c919e2ead3be3d0864b52323b&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-02-07T11:55:20Z&quot;, GoVersion:&quot;go1.9.2&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
</code></pre>

<blockquote>
<p>namespace</p>
</blockquote>

<p>如果需要部署在非kube-system命令下,需要该所有yaml的namespace,例如:我所有的资源部署在logging这个namespace下。</p>

<blockquote>
<p>下载相关代码(主要是yaml文件)</p>
</blockquote>

<p>《elasticsearch、kibana》 <a href="https://github.com/kubernetes/kubernetes.git">https://github.com/kubernetes/kubernetes.git</a></p>

<p>《fluent-bit》 <a href="https://github.com/fluent/fluent-bit-kubernetes-logging.git">https://github.com/fluent/fluent-bit-kubernetes-logging.git</a></p>

<h3 id="首先部署elasticsearch">首先部署elasticsearch</h3>

<p><strong>yaml目录:cluster/addons/fluentd-elasticsearch</strong></p>

<ol>
<li><p>部署statefulset</p>

<pre><code>kubectl create -f es-statefulset.yaml
</code></pre>

<p>我部署的时候镜像版本是:<strong>k8s.gcr.io/elasticsearch:v6.2.4</strong></p></li>
</ol>

<p>初始化镜像: <strong>alpine:3.6</strong></p>

<ol>
<li><p>部署configmap</p>

<pre><code>kubectl create -f fluentd-es-configmap.yaml
</code></pre></li>

<li><p>部署Service</p>

<pre><code>kubectl create -f es-service.yaml
</code></pre></li>

<li><p>查看service的信息并记下来,下面要用</p>

<pre><code>[root@cicd-node01 es]# kubectl get svc -o wide  -n logging
NAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE       SELECTOR
elasticsearch-logging   NodePort   172.29.233.30   &lt;none&gt;        9200:30565/TCP   9h        k8s-app=elasticsearch-logging
</code></pre></li>
</ol>

<h3 id="再部署fluent-bit">再部署fluent-bit</h3>

<p><strong>yaml文件所在地址:fluent/fluent-bit-kubernetes-logging</strong></p>

<ol>
<li><p>创建相关资源</p>

<pre><code>kubectl create -f fluent-bit-role-binding.yaml
kubectl create -f fluent-bit-service-account.yaml 
kubectl create -f fluent-bit-role.yaml
</code></pre></li>

<li><p>创建DaemonSet</p></li>
</ol>

<p>进入output/elasticsearch目录</p>

<pre><code>kubectl create -f cluent-bit-configmap.yaml

</code></pre>

<p>修改fluent-bit-ds.yaml文件的这部分配置</p>

<pre><code>- name: FLUENT_ELASTICSEARCH_HOST
  value: &quot;10.151.33.87&quot;
- name: FLUENT_ELASTICSEARCH_PORT
  value: &quot;30565&quot;
</code></pre>

<p>这个地方修改elasticsearch的Host和Port的值(就是上面提到的)</p>

<pre><code>kubectl create -f fluent-bit-ds.yaml
</code></pre>

<p>我在使用的时候镜像版本是:<strong>fluent/fluent-bit:0.13.5</strong></p>

<h3 id="最后部署kibana">最后部署Kibana</h3>

<p>yaml所在目录: <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch</a></p>

<p>修改kibana-deployment.yaml的配置信息</p>

<pre><code>env:
  - name: ELASTICSEARCH_URL
    value: http://10.151.33.87:30565
    #  - name: SERVER_BASEPATH
    #value: /api/v1/namespaces/logging/services/kibana-logging/proxy
    
    ELASTICSEARCH_URL是elasticsearch的地址.
</code></pre>

<ol>
<li><p>创建deployment</p>

<pre><code>kubectl create -f kibana-deployment.yaml
</code></pre></li>

<li><p>创建kibana-service</p>

<pre><code>kubectl create -f kibana-service.yaml
</code></pre>

<p>我在创建的时候镜像版本是:<strong>docker.elastic.co/kibana/kibana-oss:6.2.4</strong></p></li>

<li><p>获取访问信息</p>

<pre><code>[root@cicd-node01 es]# kubectl get svc -n logging
NAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kibana-logging          NodePort   172.29.16.48    &lt;none&gt;        5601:31295/TCP   7h
</code></pre>

<p>然后以nodeIP+nodePort就能访问。</p></li>
</ol>

<h3 id="最后-需要的yaml文件">最后,需要的yaml文件</h3>

<p><a href="http://index.yulibaozi.com/b4e3c070-2fc5-4b90-977c-6dd69d07fee0deploy-FEK.zip.gz">http://index.yulibaozi.com/b4e3c070-2fc5-4b90-977c-6dd69d07fee0deploy-FEK.zip.gz</a></p>
]]></content>
		</item>
		
		<item>
			<title>自研程序如何快速对接kubernetes</title>
			<link>https://yulibaozi.com/posts/kubernetes/extend/2019-01-11-incluster-use-kubernetes/</link>
			<pubDate>Wed, 21 Aug 2019 20:33:22 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/extend/2019-01-11-incluster-use-kubernetes/</guid>
			<description>实践kubernetes版本:v1.11.3
伟大之路 伟大的程序想要跑起来都是要经过磨难的，像我这个伟大的程序想在kubernetes集群内(Pod内)访问kube-apiserver接口,出现错误:
Get Enpoints err:endpoints &amp;quot;kubernetes&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:yulibaozi:default&amp;quot; cannot get endpoints in the namespace &amp;quot;default&amp;quot;  而我伟大的程序是这样:
func main() { conf, err := rest.InClusterConfig() if err != nil { log.Fatalf(&amp;quot;InClusterConfig err:%v&amp;quot;, err) } client, err := kubernetes.NewForConfig(conf) if err != nil { log.Fatalf(&amp;quot;NewForConfig err:%v&amp;quot;, err) } for { ep, err := client.CoreV1().Endpoints(&amp;quot;default&amp;quot;).Get(&amp;quot;kubernetes&amp;quot;, metav1.GetOptions{}) if err != nil { log.Printf(&amp;quot;Get Enpoints err:%v&amp;quot;, err) } else { log.Printf(&amp;quot;%+v&amp;quot;, ep) } time.</description>
			<content type="html"><![CDATA[

<p><strong>实践kubernetes版本:v1.11.3</strong></p>

<h4 id="伟大之路">伟大之路</h4>

<p>伟大的程序想要跑起来都是要经过磨难的，像我这个伟大的程序想在kubernetes集群内(Pod内)访问kube-apiserver接口,出现错误:</p>

<pre><code>Get Enpoints err:endpoints &quot;kubernetes&quot; is forbidden: User &quot;system:serviceaccount:yulibaozi:default&quot; cannot get endpoints in the namespace &quot;default&quot;
</code></pre>

<p>而我伟大的程序是这样:</p>

<pre><code>func main() {
	conf, err := rest.InClusterConfig()
	if err != nil {
		log.Fatalf(&quot;InClusterConfig err:%v&quot;, err)
	}
	client, err := kubernetes.NewForConfig(conf)
	if err != nil {
		log.Fatalf(&quot;NewForConfig err:%v&quot;, err)
	}
	for {
		ep, err := client.CoreV1().Endpoints(&quot;default&quot;).Get(&quot;kubernetes&quot;, metav1.GetOptions{})
		if err != nil {
			log.Printf(&quot;Get Enpoints err:%v&quot;, err)
		} else {
			log.Printf(&quot;%+v&quot;, ep)
		}
		time.Sleep(5 * time.Second)
	}
}
</code></pre>

<h4 id="幡然醒悟">幡然醒悟</h4>

<p>看吧，伟大的程序想要运行起来总是饱经风霜，kube-apiserver访问控制导致的权限不足的问题自然而然的就呈现出来。这个时候，不！要！慌！咱们需要理清kubernetes的RBAC访问控制规则和使用关系:
<img src="http://arts.yulibaozi.com/image.png" alt="image" /></p>

<p>如上图所示，大体分成两部分，左边椭圆是角色，右边椭圆是用户和用户组，统称为使用者。中间的两条连接线作为绑定关系。为什么上图会画成这样，是因为RBAC本质就是角色赋权，然后使用者绑定角色，进而让使用者具有相应的权限，上图的大体分析到此。</p>

<p>接下来，我们就聊细节:
&gt; 左椭圆&mdash;角色:</p>

<p>在kubernetes中，角色分为两大类，Role和ClusterRole，它们两的访问规则配置是一样的，但有个巨大或者决定因素的不同是:
1. Role天然具有namespace的限制,也就说，Role的访问规则天然只能在同一namespace下生效，不能跨namespace
2. ClusterRole没有namespace的限制，他的访问规则是对整个集群生效，对，集群就是他的天下，但它不能跨集群。
3. 在yaml/json配置上的区别就是:Role需要配置namespace，而ClusterRole是不需要配置namespace的。</p>

<blockquote>
<p>右椭圆&mdash;使用者</p>
</blockquote>

<p>在kubernetes中，使用者分为3中，User，ServiceAccount(sa)，Group。
1. User是指用户，像我们的kubelet使用的就是kubelet-bootstrap这一用户名
2. Group是指用户组，这个没的说，同一用户组的所有用户拥有一致的访问权限。
3. ServiceAccount(sa)，也是一种账号，运行在Pod内的进程需要访问kube-apiserver的身份证明
4. 通常，好多伙计都会分不清楚User和ServiceAccount，User通常指kubernetes集群的用户(例如:系统管理员，运维人员，客户用户等)，不管他们在集群内还是集群外访问kube-apiserver都可以用到，也就是说个体是人，使用场景不限；而ServiceAccount是给运行在Pod上的程序用的，也就是说ServiceAccount的使用个体是程序，使用场景是集群内的Pod里。程序和人的差异还是很大的，相信一定能分得清。</p>

<blockquote>
<p>中间连线&mdash;绑定关系</p>
</blockquote>

<p>由于左边有两种角色，所以，Role使用RoleBinding来和右边的使用者绑定，而ClusterRole使用ClusterRoleBinding与右边使用者绑定，两种角色你走你的独木桥，我走我的阳关道但是最终都能达成目的。</p>

<h4 id="动手实践">动手实践</h4>

<p>由于Pod内的程序使用的是ServiceAccount(sa), 这个时候需要配置ServiceAccount(sa):</p>

<pre><code>apiVersion: v1
kind: ServiceAccount
metadata:
  name: kuclu
  namespace: yulibaozi
</code></pre>

<p>配置了sa后，一般情况下是管用的，但是你如果是跨namespace调用就会报错:</p>

<pre><code>Get Enpoints err:endpoints &quot;kubernetes&quot; is forbidden: User &quot;system:serviceaccount:yulibaozi:kuclu&quot; cannot get endpoints in the namespace &quot;default&quot;
</code></pre>

<p>上文的报错的意思是kuclu这个sa权限不注意访问default这个namespace下的endpoints资源。</p>

<p>可以看出，我们的权限不太够，进而导致我们的pod内的进程有力使不出。所以，</p>

<p>它急呀，</p>

<p>它一急就使劲报错，</p>

<p>不解决，</p>

<p>它就卯足劲报错，</p>

<p>受不了，得解决。</p>

<p>根据第二步的分析，我们得先配置一个角色,这个角色应该创建Role还是ClusterRole呢，程序所在的namespace在yulibaozi下，而我需要访问default这个namespace的资源对象，所以需要跨namespace访问，自然而然的选择了ClusterRole:</p>

<pre><code>kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  # &quot;namespace&quot; omitted since ClusterRoles are not namespaced
  name: root
rules:
- apiGroups: [&quot;*&quot;]
  resources: [&quot;*&quot;]
  verbs: [&quot;*&quot;]
</code></pre>

<p>对，这个名为root的角色能干任何事情，从测试的角度，我让这个角色能搞任何事情，但是，在生产环境，你可就悠着点。别出锅了。</p>

<p>根据第二步的分析，需要把ServiceAccount和ClusterRole绑定，由于我们使用的是ClusterRole这种角色，所以，我们得使用ClusterRoleBinding来绑定，自然而然的选择了ClusterRoleBinding，配置依然清秀简单:</p>

<pre><code>kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: root-to-kuclu
subjects:
- kind: ServiceAccount
  name: kuclu
  namespace: yulibaozi
 # apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: root
  apiGroup: rbac.authorization.k8s.io
</code></pre>

<p>配置完成后，程序也不用杀掉重启就能正常访问。</p>

<h4 id="这边的风景也很精彩">这边的风景也很精彩</h4>

<p><a href="https://jimmysong.io/kubernetes-handbook/guide/rbac.html">jimmysong-RBAC——基于角色的访问控制</a></p>

<p><a href="https://mritd.me/2017/07/17/kubernetes-rbac-chinese-translation/">默然&ndash;Kubernetes RBAC</a></p>
]]></content>
		</item>
		
		<item>
			<title>Deployment和StatefulSet的使用理念和区别</title>
			<link>https://yulibaozi.com/posts/kubernetes/extend/2019-07-23-deployment-vs-statefulset/</link>
			<pubDate>Tue, 23 Jul 2019 20:36:50 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/extend/2019-07-23-deployment-vs-statefulset/</guid>
			<description>### 理论上的区别  StatefulSet作为Kubernetes中部署应用的控制器之一,它被专门用于部署那些有状态和有持久化需求的应用,同时,它也遵循一些指定的规则。另一方面,我们熟知的Deployment也是做相似的事情。除了Kubernetes 文档中非常明确的指出:&amp;ldquo;如果应用不需要稳定的标识或者有序的部署,删除和缩放,你应该使用无状态的副本控制器部署你的应用,这些无状态的副本控制器(例如:Deployment,ReplicaSet)或许更适合你应用的无状态需求&amp;rdquo;。
If an application doesn’t require any stable identifiers or ordered deployment, deletion, or scaling, you should deploy your application with a controller that provides a set of stateless replicas. Controllers such as Deployment or Replica Set may be better suited to your stateless needs.
上面已经说明了StatefulSet和stateless replicas之间的区别,接下来,我们仔细看看:
   Deployment StatefulSet     可以扩展更多的副本集(ReplicaSet/RS)以支持更大的负载 有序,优雅的部署和扩展(缩放)   滚动更新是根据Deployment中的PodTemplateSpec配置完成的;当一个新的ReplicaSet(RS)被新建时，Deployment会以指定的速率将Pod从旧的RS升级到新的RS,直到更新完成 有序,自动滚动更新   回滚:可以回滚到上一个/更早的版本 没有回滚,只有删除和缩小副本数   有独立的网络服务 StatefulSets目前要求Headless Service负责Pod在网络中的唯一身份    从上面的表格可以非常有底气的假设两者之间最大的区别在于网络服务的要求和StatefulSet的持久化存的储示例文档。但是,眼睛看到的不一定就是全部。</description>
			<content type="html"><![CDATA[

<pre><code>### 理论上的区别
</code></pre>

<p>StatefulSet作为Kubernetes中部署应用的控制器之一,它被专门用于部署那些有状态和有持久化需求的应用,同时,它也遵循一些指定的规则。另一方面,我们熟知的Deployment也是做相似的事情。除了Kubernetes 文档中非常明确的指出:&ldquo;如果应用不需要稳定的标识或者有序的部署,删除和缩放,你应该使用无状态的副本控制器部署你的应用,这些无状态的副本控制器(例如:Deployment,ReplicaSet)或许更适合你应用的无状态需求&rdquo;。</p>

<p>If an application doesn’t require any stable identifiers or ordered deployment, deletion, or scaling, you should deploy your application with a controller that provides a set of stateless replicas. Controllers such as Deployment or Replica Set may be better suited to your stateless needs.</p>

<p>上面已经说明了StatefulSet和stateless replicas之间的区别,接下来,我们仔细看看:</p>

<table>
<thead>
<tr>
<th>Deployment</th>
<th>StatefulSet</th>
</tr>
</thead>

<tbody>
<tr>
<td>可以扩展更多的副本集(ReplicaSet/RS)以支持更大的负载</td>
<td>有序,优雅的部署和扩展(缩放)</td>
</tr>

<tr>
<td>滚动更新是根据Deployment中的PodTemplateSpec配置完成的;当一个新的ReplicaSet(RS)被新建时，Deployment会以指定的速率将Pod从旧的RS升级到新的RS,直到更新完成</td>
<td>有序,自动滚动更新</td>
</tr>

<tr>
<td>回滚:可以回滚到上一个/更早的版本</td>
<td>没有回滚,只有删除和缩小副本数</td>
</tr>

<tr>
<td>有独立的网络服务</td>
<td>StatefulSets目前要求Headless Service负责Pod在网络中的唯一身份</td>
</tr>
</tbody>
</table>

<p>从上面的表格可以非常有底气的假设两者之间最大的区别在于网络服务的要求和StatefulSet的持久化存的储示例文档。但是,眼睛看到的不一定就是全部。</p>

<h3 id="statefulsets-在实践中的总结">StatefulSets 在实践中的总结</h3>

<p>StatefulSets的要求是满足Pod所需要的网络身份,为达到这个要求,一个标准的Kubernetes svc 需要根据<strong>selector</strong>和<strong>app</strong>(用户在标签中定义的Key)来部署，以此来满足StatefulSet的逻辑映射(逻辑映射指的是svc通过selector来选择对应Label的Pod),接下来,我使用Postgres应用来作为示例。</p>

<p>以下Yaml文件定义了<strong>pgnet</strong>的Kubernetes svc,暴露的端口是:5432,定义的端口名字是:pgport,在这种情况下发布的类型是<strong>clusterIP</strong>,这意味着这个Postgres服务只能在Kubernetes集群内部访问。但是这个ClusterIP的值是:None,这意味着不会预定义IP,如果需要让外部也能访问到Postgres服务,我们可以<a href="https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services---service-types">查看文档</a>了解如何在Kubernetes之外和外部网络上公开访问它。这其中非常重要的部分是<strong>selector</strong>,它需要匹配被部署为StatefulSet的app的标签。</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
name: pgnet
labels:
app: pgnet
spec:
ports:
- port: 5432
name: pgport
clusterIP: None
selector:
app: postgres
</code></pre>

<p>StatefulSet的配置与Deployment非常相似,配置中的serviceName必须与上面创建的svc相匹配,容器也需要一致的端口,环境变量,volumeMounts等。</p>

<p>StatefulSet的配置与Deployment的配置相比,也有几处独特之处。
1. 从使用kubectl命令看到StatefulSet所管控的Pod的名字似乎遵从特有的规则,这点我们放在后面讨论.
2. 挂载卷部分和Deployment中的容器相似,容器上定义的volumeMounts的名字和volumeClaimTemplates存在逻辑映射(之间的映射是根据name,在下面的配置中,我们可以着重看volumeMounts和volumeClaimTemplates部分).
3. 鉴于Deployment可以使用StorageClass或者预定义好PersistentVolume,StatefulSet也可以预定义StorageClass实现动态挂载Volume,。我们可以在<a href="https://github.com/thecodeteam/vg-kubernetes/blob/master/scripts/examples/scaleio/storageclass.yaml">sc.yaml</a>看到相关示例。</p>

<pre><code>apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
name: pgdatabase
spec:
serviceName: &quot;postgres&quot;
replicas: 1
template:
metadata:
labels:
app: postgres
spec:
containers:
- name: postgres01
image: postgres
ports:
- containerPort: 5432
name: pgport
env:
- name: POSTGRES_PASSWORD
value: &quot;Password123!&quot;
volumeMounts:
- name: pgvolume
mountPath: /var/lib/postgresql/data
volumeClaimTemplates:
- metadata:
name: pgvolume
spec:
accessModes: [ &quot;ReadWriteOnce&quot; ]
storageClassName: sio-small
resources:
requests:
storage: 16Gi
</code></pre>

<p>一旦部署了StatefulSet,它会依照名称规则来显示名字,在这个Pod示例中,名称为pgdatabase的Pod会基于副本的数量有序排列(从小到大),而Deployment会以多个随机字符来保证Pod名称的唯一性,也就说Deployment控制的Pod名称是随机无序的,在名称上另一个区别是:当删除一个Pod后,Deployment会创建一个新Pod,且这个Pod的名字和之前的Pod名字是不一样的,是一个随机生成的新名字,相反,StatefulSet仍然沿用上一个Pod的名字和属性创建新的Pod,它以这种方式来保证所管控Pod的名字具有唯一性、有序性和一致性。</p>

<p><img src="http://links.yulibaozi.com/a9ae1f65-48db-41a2-8ea7-d4a884d8b02416-Aug-Blog-image-1.png" alt="image" /></p>

<p>另一个有趣的事情是删除Pod的时候,在Deployment中,Pod可能会重新调度在集群中的其他节点,至于会调度到那个节点,这个由调度器说了算;而在StatefulSet中,他会在之前运行的、相同的节点上重新创建(有点像落地生根),这个是有StatefulSet的默认属性决定的,这是因为StatefulSet认为没有任何理由需要从Node上强行删除Pod,Kubernetes假设以StatefulSet运行的应用程序需要稳定的网络标识和稳定的存储,并且同一Pod有多个实例可能会导致数据损坏;如果某个节点/主机需要维护而关闭或者网络不可达,那么Pod会进入 “Unknown” 或者 “Terminating” 状态,但即使在这种情况下,Pod也不会重新调度到其他节点上(是不是很头铁),除非以手动的方式删除Node对象或手动删除Pod对象且此时原本的Node不可调度或不可用,与此同时,不响应节点上的kubelet进程开始杀死Pod并在apiServer中删除对应的Pod条目,或者强行删除Pod,当然,强行删除会导致volume(挂载卷)未被正确卸载且重新调度Pod失败。</p>

<h3 id="他们之间的异同">他们之间的异同</h3>

<p>当阅读了StatefulSet的高可用性实践后,Deployment似乎能更好的完成工作,那为什么还推荐使用StatefulSet作为持久化应用的部署方法呢?是因为Kubernetes不希望Pod能够在另外一个节点上立即重启来增加<a href="https://en.wikipedia.org/wiki/Split-brain_(computing)">脑裂(split-brain)</a>情况的出现,如果存储提供者有能力的话,pod.Spec.TerminationGracePeriodSeconds这个配置项可以配置在StatefulSet中,同时,Controller能够在当前主机变为“Unknown”之前,在新的主机上自动重启一个Pod，持久卷的volumeClaimTemplates用来确保跨组件重新启动后仍然能够保证正确的状态,我们假设现在集群中正发生这样的灾难,因为Kubernetes支持本地存储(host),如果存储的数据未在节点间相互复制/同步或者集中于某一处,除非这个节点再次上线,否则你没有其他方法安全的还原数据。</p>

<h3 id="实验论证结论">实验论证结论</h3>

<h4 id="statefulset所管控的pod的名字遵循何种规则">StatefulSet所管控的Pod的名字遵循何种规则?</h4>

<p>StatefulSet所管控的Pod的名字遵循stsname-&lt;编号&gt;，同时保证唯一性,绝不重复,有序性,中间以-分割;名字中编号遵从如下规则:</p>

<pre><code>1. 是数字而不是字母,
2. 连续的(上一个Pod的编号是0,那么下一个Pod的编号一定是1,而不是2,3或者其他),
3. 递增的
</code></pre>

<p>另一方面,当你使用<strong>kubectl exec -it podname</strong> 进入容器后,再执行<strong>echo $HOSTNAME</strong>命令 你会发现hostname的值与pod name是完全一致的。</p>

<h4 id="为什么说statefulset是有序-优雅的部署和扩展-缩放">为什么说StatefulSet是有序,优雅的部署和扩展(缩放)?</h4>

<p>StatefulSet在做任何操作的时候,都会遵循有序这个前提。</p>

<ol>
<li><p>在部署的时候,他会遵从顺序一个接一个的部署,如果中途有一个Pod没有达到Running状态,他也会hang在哪儿,直到当前Pod达到Running状态,才会创建下一个Pod。</p></li>

<li><p>在扩容的时候,编号小的Pod没有达到Running的状态,它不会增加下一个Pod,也就说它会死等。</p>

<pre><code>我们把elasticsearch-logging的副本数从1修改为4(replicas:1-&gt; replicas:4)
    
elasticsearch-logging-1   0/1       Pending   0         0s      &lt;none&gt;
elasticsearch-logging-1   0/1       Pending   0         0s      kube02
elasticsearch-logging-1   0/1       Init:0/1  0         0s      kube02
elasticsearch-logging-1   0/1       PodInitializing   0 6s      kube02
elasticsearch-logging-1   1/1       Running   0         7s      kube02
elasticsearch-logging-2   0/1       Pending   0         0s      &lt;none&gt;
elasticsearch-logging-2   0/1       Pending   0         0s      kube01
elasticsearch-logging-2   0/1       Init:0/1   0        1s      kube01
elasticsearch-logging-2   0/1       PodInitializing   0 3s      kube01
elasticsearch-logging-2   0/1       ErrImagePull   0    4s      kube01
elasticsearch-logging-2   0/1       ImagePullBackOff  0 19s     kube01
    
我们看到elasticsearch-logging-1状态变为Running后,才会创建elasticsearch-logging-2。
我们的elasticsearch-logging-2遭遇了ImagePullBackOff错误,所以它会死等elasticsearch-logging-2状态变为Running后才会创建elasticsearch-logging-3。
</code></pre></li>

<li><p>在缩容的时候,是编号值大的Pod会先被删除,接下来才会删除编号小的Pod,直到达到期望值才停止。</p>

<pre><code>我们把elasticsearch-logging的副本数从4修改为1(replicas:4-&gt; replicas:1),由于asticsearch-logging-2没有拉取到镜像,所以没有创建asticsearch-logging-3,它就从编号最大的Pod杀死Pod.
    
elasticsearch-logging-2   0/1       Terminating   0         6m   kube01
elasticsearch-logging-2   0/1       Terminating   0         6m   kube01
elasticsearch-logging-2   0/1       Terminating   0         6m   kube01
elasticsearch-logging-2   0/1       Terminating   0         6m   kube01
elasticsearch-logging-1   1/1       Terminating   0         7m   kube02
elasticsearch-logging-1   0/1       Terminating   0         7m   kube02
    
在实验中,我们会看到会从编号最大的Pod开始执行杀死策略,且它是从大到小,依次执行杀死策略的。
    
</code></pre></li>

<li><p>再次扩容到3个Pod会发现,相同Pod的名字仍然会绑定到相同的Node节点</p>

<pre><code>[root@kube03 ~]# kubectl get pod -n logging -o wide
    
elasticsearch-logging-0           1/1       Running        1       17d   kube04
elasticsearch-logging-1           1/1       Running        0       39s   kube02
elasticsearch-logging-2           0/1       ErrImagePull   0       14s   kube01
    
从扩容到缩容的一系列操作中,我们会发现elasticsearch-logging-0始终在名为kube04的节点上。elasticsearch-logging-1一直在名为kube02的Node上,elasticsearch-logging-2也一直在名为kube01的节点上。
</code></pre></li>
</ol>

<p>通过以上实验证明了StatefulSet是如何有序,优雅的进行部署和扩展。</p>

<h4 id="headless-service是如何工作的">Headless Service是如何工作的？</h4>

<p>如我们所知道的那样,Headless并不会分配虚拟IP(VIP),而是以DNS的方式直接解析到对应的Pod IP,这是Headless Service相对于其他svc的主要区别。 DNS的结构遵循<strong>my-svc.my-namespace.svc.cluste.local</strong>,同时,StatefulSet的配置中有个serviceName字段,这个字段是用来维持Headless service和StatefulSet的映射关系的,通常这个字段不用手动填写,你只需要做好service和statefulSet之间的label映射关系,Kubernetes会自动将service的名称填充到StatefulSet的serviceName字段。</p>

<h4 id="statefulset是如何处理持久化存储的">StatefulSet是如何处理持久化存储的?</h4>

<p>在StatefulSet中我们有多种挂载数据卷来达到持久化存储的目地,例如:</p>

<pre><code>1. 舍弃PV或PVC资源对象直接&quot;硬挂&quot;,你需要填入后端RBD或者NFS相关的配置信息。而这部信息往往是集群管理人员掌握的,你需要和他们沟通才能索要到这些信息,这也导致了信息的过度暴露。
2. 使用PVC,他的优点在于你不需要配置后端存储的任何信息,你只是把你存储需求配置好就可以直接用了,比如你需要多大的容量,你想要以什么方式访问(accessMode)。至于后面挂载到PV事情你就可以不用关心了,把这些事情交给集群管理员处理或者自动实现。
</code></pre>

<p>接下来,我们主要是对第二种方式进行探讨:</p>

<p>在StatefulSet的配置模板中,有项关于<strong>volumeClaimTemplates</strong>的配置,这个配置是后续创建pvc的模板,在模板中,我们通常会声明模板名字(例如:mysql),但是在PVC真正被创建时,被创建的pvc的名称和pvc模板名字有所区别,真正创建pvc的名字遵循如下规则:</p>

<pre><code>    被创建的pvc名字=pvc模板名字-StatefulSet名字-编号 
    或
    被创建的pvc名字=pvc模板名字-StatefulSet所管控的Pod名字
    
    上面所表达的两种规则，其实是同一规则,生成的结果是一样的,只是表述不一样。
</code></pre>

<p>例如pvc模板名字是mysql,statefulset的名称是master,那么Statefulset所管控的pod名字是master-0,而生成的pvc名字是mysql-master-0。
另一方面值得注意的是:如果pod和pvc绑定在了一起,那么他们编号往往是相同的(例如名为<strong>master-0</strong>的Pod绑定的pvc名字一定是<strong>mysql-master-0</strong>),且即使pod被删除重建后,他们俩仍然会绑定在一起(颇有不离不弃,生死相依的感觉),出现这一情况的原因是:被创建的pod名字仍然是上一个被删除Pod的名字,Pod的名字保持了不变,那么该Pod选择的pvc名字也就不会改变,也就是说名字,是<strong>名字</strong>的选择让他们在一起了,另外,他们看似生死相依,但是有一点例外,当Statefulset所管控的Pod被删除后且还没来及重建,在这段时间差里,刚好,有另外一个Pod绑定了这个名为:mysql-master-0的pvc,且这个pvc不支持多个Pod/host绑定,这会发生什么呢？想想都刺激。对于pvc有哪几种方式绑定pv的呢,我们可以留在后面再说。</p>

<p>参考文档:</p>

<p>[1] Technical Dive into StatefulSets and Deployments in Kubernetes <a href="https://blog.thecodeteam.com/2017/08/16/technical-dive-statefulsets-deployments-kubernetes/">https://blog.thecodeteam.com/2017/08/16/technical-dive-statefulsets-deployments-kubernetes/</a></p>
]]></content>
		</item>
		
		<item>
			<title>CronJob 原理和源码分析</title>
			<link>https://yulibaozi.com/posts/kubernetes/extend/2019-04-23-cronjob/</link>
			<pubDate>Sun, 21 Apr 2019 20:35:24 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/extend/2019-04-23-cronjob/</guid>
			<description>CronJob是Kubernetes提供的定时任务功能，CronJob可以根据你指定的cron策略来完成任务。  我们在使用CronJob的时候，我们发现，当创建一个CronJob的时候，只会创建一个CronJob，当到指定时间时，会创建一个job和一个pod,随着时间的推移，我们会发现，越来越多的job和pod，甚至是满屏的job和pod，我们在调用API删除这个cronjob的时候，发现只会删除cronjob这个资源，而不会删除对应的已有的job和pod，然后，我们尝试删除job，发现删除job，会把对应的pod删除掉，为什么好多的操作不符合心理预期呢？心里冒着十万个为什么
迷雾沼泽  cronjob是怎么定时创建job的呢？
 为什么删除的时候，只是删除了cronjob，而没有删除对应的job和pod呢？
 cronjob、job和pod之间的关系是怎么样的呢？
 job和pod的关系是怎么对应上的呢？
  探索一(从命令行结果开始探索)  kubectl describe cronjob hello
 我们看到上图的Events，看到了cronjob-controller,且Message 里面创建了job，我们猜测是cronjob通过controller-manager下的cronjob-controller来实现对job的创建工作
 kubectl describe job jobname
 我们看到这个job是被CronJob/hello创建，而并非我们上文猜测的cronjob-controller,但是想来应该存在某种关系的，然后看到下面的Events,我们看到通过job-controller创建了pod.
 kubectl describe pod podname
 看到这个pod是由job/hello-1528459440创建,似乎和上文猜测的job-controller看上去并无直接关系。
到这里，我们看似发现了许多的蛛丝马迹，但是细细想来，还是不清不楚，不明不白,是时候读源码了。
探索二（源码探索） 注意:会删除一些无关代码
源码位于:github.com/kubernetes/kubernetes/pkg/controller/cronjob/cronjob_controller.go  回答问题：cronjob和job是如何维护关系的呢？ // syncAll lists all the CronJobs and Jobs and reconciles them. func (jm *CronJobController) syncAll() { ...... //列出所有的job jl, err := jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(metav1.ListOptions{}) if err != nil { utilruntime.</description>
			<content type="html"><![CDATA[

<pre><code>CronJob是Kubernetes提供的定时任务功能，CronJob可以根据你指定的cron策略来完成任务。
</code></pre>

<p>我们在使用CronJob的时候，我们发现，当创建一个CronJob的时候，只会创建一个CronJob，当到指定时间时，会创建一个job和一个pod,随着时间的推移，我们会发现，越来越多的job和pod，甚至是满屏的job和pod，我们在调用API删除这个cronjob的时候，发现只会删除cronjob这个资源，而不会删除对应的已有的job和pod，然后，我们尝试删除job，发现删除job，会把对应的pod删除掉，为什么好多的操作不符合心理预期呢？心里冒着十万个为什么</p>

<h3 id="迷雾沼泽">迷雾沼泽</h3>

<ol>
<li><p>cronjob是怎么定时创建job的呢？</p></li>

<li><p>为什么删除的时候，只是删除了cronjob，而没有删除对应的job和pod呢？</p></li>

<li><p>cronjob、job和pod之间的关系是怎么样的呢？</p></li>

<li><p>job和pod的关系是怎么对应上的呢？</p></li>
</ol>

<h3 id="探索一-从命令行结果开始探索">探索一(从命令行结果开始探索)</h3>

<blockquote>
<p>kubectl describe cronjob hello</p>
</blockquote>

<p><img src="http://index.yulibaozi.com/4334dcd0-ca6d-41b7-b2c8-fd26408bbc6f1.png" alt="image" /></p>

<p>我们看到上图的Events，看到了cronjob-controller,且Message 里面创建了job，我们猜测是cronjob通过controller-manager下的cronjob-controller来实现对job的创建工作</p>

<blockquote>
<p>kubectl describe job jobname</p>
</blockquote>

<p><img src="http://index.yulibaozi.com/8e09ea27-d72f-4baa-9324-e4eb9b82e14e2.png" alt="image" /></p>

<p>我们看到这个job是被CronJob/hello创建，而并非我们上文猜测的cronjob-controller,但是想来应该存在某种关系的，然后看到下面的Events,我们看到通过job-controller创建了pod.</p>

<blockquote>
<p>kubectl describe pod podname</p>
</blockquote>

<p><img src="http://index.yulibaozi.com/f76dd4de-7262-4119-8807-473a394d06904.png" alt="image" /></p>

<p>看到这个pod是由job/hello-1528459440创建,似乎和上文猜测的job-controller看上去并无直接关系。</p>

<p>到这里，我们看似发现了许多的蛛丝马迹，但是细细想来，还是不清不楚，不明不白,是时候读源码了。</p>

<h3 id="探索二-源码探索">探索二（源码探索）</h3>

<p><em>注意:会删除一些无关代码</em></p>

<pre><code>源码位于:github.com/kubernetes/kubernetes/pkg/controller/cronjob/cronjob_controller.go
</code></pre>

<h4 id="回答问题-cronjob和job是如何维护关系的呢">回答问题：cronjob和job是如何维护关系的呢？</h4>

<pre><code>// syncAll lists all the CronJobs and Jobs and reconciles them.
func (jm *CronJobController) syncAll() {
	......
	//列出所有的job
	jl, err := jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(metav1.ListOptions{})
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&quot;can't list Jobs: %v&quot;, err))
		return
	}
	js := jl.Items //所有的jobs(指的是已经创建了的)
	glog.V(4).Infof(&quot;Found %d jobs&quot;, len(js))
	//列出所有cronjob
	sjl, err := jm.kubeClient.BatchV1beta1().CronJobs(metav1.NamespaceAll).List(metav1.ListOptions{})
	...
	sjs := sjl.Items //所有的定时任务
	glog.V(4).Infof(&quot;Found %d cronjobs&quot;, len(sjs))

	//为上面所有的job分组，本质是个map，同一个cronjob是一组，是否为同一组使用types.UID来判断
	jobsBySj := groupJobsByParent(js)
	glog.V(4).Infof(&quot;Found %d groups&quot;, len(jobsBySj))

	for _, sj := range sjs { //遍历直接查询出来的cronjob
		// cronjob
		syncOne(&amp;sj, jobsBySj[sj.UID], time.Now(), jm.jobControl, jm.sjControl, jm.podControl, jm.recorder)
		cleanupFinishedJobs(&amp;sj, jobsBySj[sj.UID], jm.jobControl, jm.sjControl, jm.podControl, jm.recorder)
	}
}
</code></pre>

<p>A: cronjob和job关系,使用了types.UID,来判断的，每个cronjob拥有唯一的UID,然后列出所有的jobs，使用遍历job的策略来判断job应该属于哪个cronjob。</p>

<h4 id="回答问题-job和pod的关系是怎么样维护的呢">回答问题：job和pod的关系是怎么样维护的呢？</h4>

<pre><code>源代码：380行

selector, _ := metav1.LabelSelectorAsSelector(job.Spec.Selector)
	options := metav1.ListOptions{LabelSelector: selector.String()}
	podList, err := pc.ListPods(job.Namespace, options)
	if err != nil {
		recorder.Eventf(sj, v1.EventTypeWarning, &quot;FailedList&quot;, &quot;List job-pods: %v&quot;, err)
		return false
	}
</code></pre>

<p>从代码看出，我们先拿到了job中的Selector,然后，根据选择器来获取对应的pods,从截图也能发现这个这一点。</p>

<h4 id="回答问题-cronjob是怎么定时创建job的呢">回答问题：cronjob是怎么定时创建job的呢？</h4>

<pre><code>func (jm *CronJobController) Run(stopCh &lt;-chan struct{}) {
	defer utilruntime.HandleCrash()
	glog.Infof(&quot;Starting CronJob Manager&quot;)
	// Check things every 10 second.
	go wait.Until(jm.syncAll, 10*time.Second, stopCh)
	&lt;-stopCh
	glog.Infof(&quot;Shutting down CronJob Manager&quot;)
}

// syncAll lists all the CronJobs and Jobs and reconciles them.
func (jm *CronJobController) syncAll() {
	...
	jl, err := jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(metav1.ListOptions{})
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&quot;can't list Jobs: %v&quot;, err))
		return
	}
	js := jl.Items
	glog.V(4).Infof(&quot;Found %d jobs&quot;, len(js))

	sjl, err := jm.kubeClient.BatchV1beta1().CronJobs(metav1.NamespaceAll).List(metav1.ListOptions{})
	if err != nil {
		utilruntime.HandleError(fmt.Errorf(&quot;can't list CronJobs: %v&quot;, err))
		return
	}
	sjs := sjl.Items
	glog.V(4).Infof(&quot;Found %d cronjobs&quot;, len(sjs))

	jobsBySj := groupJobsByParent(js)
	glog.V(4).Infof(&quot;Found %d groups&quot;, len(jobsBySj))

	for _, sj := range sjs {
		syncOne(&amp;sj, jobsBySj[sj.UID], time.Now(), jm.jobControl, jm.sjControl, jm.podControl, jm.recorder)
		cleanupFinishedJobs(&amp;sj, jobsBySj[sj.UID], jm.jobControl, jm.sjControl, jm.podControl, jm.recorder)
	}
}

func syncOne(sj *batchv1beta1.CronJob, js []batchv1.Job, now time.Time, jc jobControlInterface, sjc sjControlInterface, pc podControlInterface, recorder record.EventRecorder) {
    ...
    jobReq, err := getJobFromTemplate(sj, scheduledTime)
	if err != nil {
		glog.Errorf(&quot;Unable to make Job from template in %s: %v&quot;, nameForLog, err)
		return
	}
    //创建job
	jobResp, err := jc.CreateJob(sj.Namespace, jobReq)
	if err != nil {
		recorder.Eventf(sj, v1.EventTypeWarning, &quot;FailedCreate&quot;, &quot;Error creating job: %v&quot;, err)
		return
	}
	glog.V(4).Infof(&quot;Created Job %s for %s&quot;, jobResp.Name, nameForLog)
	recorder.Eventf(sj, v1.EventTypeNormal, &quot;SuccessfulCreate&quot;, &quot;Created job %v&quot;, jobResp.Name)
	...
	ref, err := getRef(jobResp)
	if err != nil {
		glog.V(2).Infof(&quot;Unable to make object reference for job for %s&quot;, nameForLog)
	} else {
		sj.Status.Active = append(sj.Status.Active, *ref)
	}
	sj.Status.LastScheduleTime = &amp;metav1.Time{Time: scheduledTime}
	if _, err := sjc.UpdateStatus(sj); err != nil {
		glog.Infof(&quot;Unable to update status for %s (rv = %s): %v&quot;, nameForLog, sj.ResourceVersion, err)
	}

	return
}

</code></pre>

<p>A: 通过代码发现，cronjob会在后台启动一个go程，后台一直在处理cronjob,job和pod，同时也在维护cornjob的Active列表数据的正确性，同时，我们也在syncOne()函数中找到了他后台周期期创建job的操作。同时，我们也解决了上文我们猜测job是cronjob-controller创建的问题，我们的猜测是正确的，逻辑是，当创建一个cronjob后，会把这个cronjob交接到cronjob-controller下的goroutine,然后就返回了，正在创建任务的是controller下的goroutine。</p>

<h4 id="回答问题-删除是cronjob-为何没有删除对应的job和pod">回答问题:删除是cronjob,为何没有删除对应的job和pod?</h4>

<pre><code>源代码 354行

func deleteJob(sj *batchv1beta1.CronJob, job *batchv1.Job, jc jobControlInterface,
	pc podControlInterface, recorder record.EventRecorder, reason string) bool {
</code></pre>

<p>A: 删除的时候，会首先检查并发job的并发策略，如果不为0,会设置为0,并更新这个job的状态,然后找到对应的pod,删除完pod后再删除job本身,如果有一个pod删除失败，会直接退出而不会job。删除完job后，再从cronjob的Active列表中移除job(通过上文我们提到的UID)。</p>

<p>对于删除cronjob,可能没有立即删除job和pod，是因为删除的时候会有三种策略，即：</p>

<pre><code>DeletionPropagation string=&quot;Orphan&quot;,&quot;Foreground&quot;(默认),&quot;Background&quot;
</code></pre>

<p>Orphan:GC自动触发；
Background:垃圾回收器会在后台执行删除（手动触发，立即后台处理）。
Foreground:API调用后会设置删除的过期时间，并把他放入到要删除的队列，在没删除之前，前台一直可见（手动触发，可能不立即处理）。</p>

<p>两种数据结构:</p>

<pre><code>//立即执行的数据结构
type realJobControl struct {
	KubeClient clientset.Interface
	Recorder   record.EventRecorder
}

func (r realJobControl) DeleteJob(namespace string, name string) error {
	return r.KubeClient.BatchV1().Jobs(namespace).Delete(name, nil)
}

// 假执行的数据结构,把要操作的数据放入到列表就算完成执行了，而正在的执行操作并不它。
type fakeJobControl struct {
	sync.Mutex
	Job           *batchv1.Job
	Jobs          []batchv1.Job
	DeleteJobName []string
	Err           error
	UpdateJobName []string
	PatchJobName  []string
	Patches       [][]byte
}

func (f *fakeJobControl) DeleteJob(namespace string, name string) error {
	f.Lock()
	defer f.Unlock()
	if f.Err != nil {
		return f.Err
	}
	f.DeleteJobName = append(f.DeleteJobName, name)
	return nil
}
</code></pre>

<p>当真正执执行Delete的时候</p>

<pre><code>源代码：github.com/kubernetes/kubernetes/pkg/controller/garbagecollector/operations.go

func (gc *GarbageCollector) deleteObject(item objectReference, policy *metav1.DeletionPropagation) error {
	resource, namespaced, err := gc.apiResource(item.APIVersion, item.Kind)
	if err != nil {
		return err
	}
	uid := item.UID
	preconditions := metav1.Preconditions{UID: &amp;uid}
	deleteOptions := metav1.DeleteOptions{Preconditions: &amp;preconditions, PropagationPolicy: policy}
	return gc.dynamicClient.Resource(resource).Namespace(resourceDefaultNamespace(namespaced, item.Namespace)).Delete(item.Name, &amp;deleteOptions)
}
</code></pre>

<p>根据资源获取获取resource,然后重新组装deleteOptions策略，然后删除。</p>

<p>本次cronjob原理和源码分析结束。</p>
]]></content>
		</item>
		
		<item>
			<title>Golang json 的进阶用法</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-09-24-go-and-json/</link>
			<pubDate>Mon, 24 Sep 2018 22:19:50 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-09-24-go-and-json/</guid>
			<description>Golang json 的进阶用法 痛点  你是否遇到过json中某个字段填入某种类型都适合而陷入两难境地? (例如：定义了一个port字段，你却不知道是填入8080,还是** &amp;ldquo;8080&amp;rdquo; **的尴尬局面) 你是否遇到过json反解析报错是因为填入字段的类型不匹配导致的？例如：
json: cannot unmarshal number into Go struct field Host.port of type string  你是否有json某字段兼容2种或者多种的数据结构的需求?
 你是否想让程序更优雅，更具有适配性，而不在被这些小细节头痛？
  如果你有或者你想，获取你可以看看这篇文章。
重现问题 我们给了用户一个json如下：
{ &amp;quot;name&amp;quot;:&amp;quot;yulibaozi&amp;quot;, &amp;quot;port&amp;quot;:8080 }  但是，业务方却误填了&amp;rdquo;8080&amp;rdquo;,结果我们程序反解析报错，导致业务失败。
json: cannot unmarshal number into Go struct field Host2.port of type string  或许你认为这是业务方的问题，但我认为我们可以更优雅的解决这个问题。
如何解决问题 我们先定义了一个结构体
type Host struct { Name string `json:&amp;quot;name&amp;quot;` Port Port `json:&amp;quot;port&amp;quot;` }  心细的你会发现，Port既不是int也不是string类型，而是Port类型，而Port类型是：
type Type int const ( Int Type = iota String ) type Port struct { Type Type IntVal int StrVal string }  在Port结构体中，我们发现了Type类型， 而Type类型包括了int,string两种类型。接下来就非常重要了，我们需要实现</description>
			<content type="html"><![CDATA[

<h2 id="golang-json-的进阶用法">Golang json 的进阶用法</h2>

<h4 id="痛点">痛点</h4>

<ol>
<li>你是否遇到过json中某个字段填入某种类型都适合而陷入两难境地? (例如：定义了一个port字段，你却不知道是填入<strong>8080</strong>,还是** &ldquo;8080&rdquo; **的尴尬局面)</li>

<li><p>你是否遇到过json反解析报错是因为填入字段的类型不匹配导致的？例如：</p>

<pre><code>json: cannot unmarshal number into Go struct field Host.port of type string
</code></pre></li>

<li><p>你是否有json某字段兼容2种或者多种的数据结构的需求?</p></li>

<li><p>你是否想让程序更优雅，更具有适配性，而不在被这些小细节头痛？</p></li>
</ol>

<p>如果你有或者你想，获取你可以看看这篇文章。</p>

<h4 id="重现问题">重现问题</h4>

<p>我们给了用户一个json如下：</p>

<pre><code>{
    &quot;name&quot;:&quot;yulibaozi&quot;,
    &quot;port&quot;:8080
}
</code></pre>

<p>但是，业务方却误填了&rdquo;8080&rdquo;,结果我们程序反解析报错，导致业务失败。</p>

<pre><code>json: cannot unmarshal number into Go struct field Host2.port of type string
</code></pre>

<p>或许你认为这是业务方的问题，但我认为我们可以更优雅的解决这个问题。</p>

<h4 id="如何解决问题">如何解决问题</h4>

<p>我们先定义了一个结构体</p>

<pre><code>type Host struct {
	Name string `json:&quot;name&quot;`
	Port Port   `json:&quot;port&quot;`
}
</code></pre>

<p>心细的你会发现，Port既不是int也不是string类型，而是Port类型，而Port类型是：</p>

<pre><code>type Type int

const (
	Int Type = iota
	String
)

type Port struct {
	Type   Type
	IntVal int
	StrVal string
}
</code></pre>

<p>在Port结构体中，我们发现了Type类型， 而Type类型包括了int,string两种类型。接下来就非常重要了，我们需要实现</p>

<pre><code>json.Unmarshaller interface
json.Marshaller interface
</code></pre>

<p>实现代码如下：</p>

<pre><code>type Port struct {
	Type   Type
	IntVal int
	StrVal string
}

// 实现 json.Unmarshaller 接口
func (port *Port) UnmarshalJSON(value []byte) error {
	if value[0] == '&quot;' {
		port.Type = String
		return json.Unmarshal(value, &amp;port.StrVal)
	}
	port.Type = Int
	return json.Unmarshal(value, &amp;port.IntVal)
}

// 实现 json.Marshaller 接口
func (port Port) MarshalJSON() ([]byte, error) {
	switch port.Type {
	case Int:
		return json.Marshal(port.IntVal)
	case String:
		return json.Marshal(port.StrVal)
	default:
		return []byte{}, fmt.Errorf(&quot;impossible Port.Type&quot;)
	}
}
</code></pre>

<p>接下来测试：</p>

<h4 id="测试反解析">测试反解析：</h4>

<ol>
<li><p>测试反解析int
给出json数据：</p>

<pre><code>{&quot;name&quot;:&quot;yulibaozi&quot;,&quot;port&quot;:8090}
</code></pre>

<p>反解析得到的结构体数据如下：</p>

<pre><code>&amp;{Name:yulibaozi Port:{Type:0 IntVal:8090 StrVal:}}
</code></pre></li>

<li><p>测试反解析string:
给出json数据：</p>

<pre><code>{&quot;name&quot;:&quot;yulibaozi&quot;,&quot;port&quot;:&quot;8090&quot;}
</code></pre>

<p>反解析得到的结构体数据如下：</p>

<pre><code>&amp;{Name:yulibaozi Port:{Type:1 IntVal:0 StrVal:8090}}
</code></pre>

<h4 id="测试编码的json">测试编码的json:</h4></li>

<li><p>测试编码int的结构体如下:</p>

<pre><code>host := &amp;Host{
    Name: &quot;yulibaozi&quot;,
    Port: Port{
        Type:   Int,
        IntVal: 8080,
    },
}
</code></pre>

<p>编码后的json如下：</p>

<pre><code>{&quot;name&quot;:&quot;yulibaozi&quot;,&quot;port&quot;:8080}
</code></pre></li>

<li><p>测试编码string的结构体如下：</p>

<pre><code>host := &amp;Host{
    Name: &quot;yulibaozi&quot;,
    Port: Port{
        Type:   String,
        StrVal: &quot;8080&quot;,
    },
}
</code></pre>

<p>编码后的json数据如下：</p>

<pre><code>{&quot;name&quot;:&quot;yulibaozi&quot;,&quot;port&quot;:&quot;8080&quot;}
</code></pre></li>
</ol>

<p>在反编码测试中，你会发现当json填入的类型不同时，会编码到结构体中对应的字段中。</p>

<p>在编码测试中, 具体编码那个数据是由Type来确定的。</p>

<h4 id="总结">总结</h4>

<p>其实,这篇文章只是分享了下json中使用的小技巧，他打破了在使用json时，需要呆板的数据结构的印象，转而走向了多变，灵活跳脱的风格，其实，这这个小tips的核心在于实现Unmarshaller,Marshaller这两个结构体,他们的实现是这个分享的关键,当然，你可以实现如开篇所说的那样，json某字段兼容2种及以上结构，当然，你也可以对yaml,toml等进行折腾,都会得到你想要的答案。</p>

<h4 id="最后">最后</h4>

<p>祝大家中秋快乐。哈哈，回忆起远赴他乡工作的时候，那时心中一股将要独闯天涯的英雄气，亦有此地一别孤蓬万里征的伤感，还有自己那种挥挥手不带走半片云彩的洒脱……</p>
]]></content>
		</item>
		
		<item>
			<title>在 Kubernetes 使用 ceph 快速部署mysql主从复制集群</title>
			<link>https://yulibaozi.com/posts/kubernetes/deploy/2018-09-06-k8s-ceph-mysql-master-slave/</link>
			<pubDate>Sun, 16 Sep 2018 01:03:46 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/deploy/2018-09-06-k8s-ceph-mysql-master-slave/</guid>
			<description>血与泪的感慨 我的天空星星都亮了，一扫之前的阴霾，喜上眉梢，多日的折磨成了幻影，程序员就是这样，痛并快乐着。接下来，开始我们伟大的分享之路，分享如何在kubernetes中搭建mysql主从集群,不踩坑的快速搭建master主从集群,让你们早点回家，多陪陪女友，愿码友们不终日与十姊妹为伍。
千里之行始于足下  集群版本信息
[root@kube03 ~]# kubectl version Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;9&amp;quot;, GitVersion:&amp;quot;v1.9.6&amp;quot;, BuildDate:&amp;quot;2018-03-21T15:21:50Z&amp;quot;, GoVersion:&amp;quot;go1.9.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;} Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;9&amp;quot;, GitVersion:&amp;quot;v1.9.6&amp;quot;, BuildDate:&amp;quot;2018-03-21T15:13:31Z&amp;quot;, GoVersion:&amp;quot;go1.9.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}  需要ceph的相关配置:key,rbdImage名字(需要找ceph管理员)
 需要基础镜像
镜像仓库: docker.io/yulibaozi master: yulibaozi/mysql-master:201807261706 sha256:65ec774cd3a5e71bae1864dffbb1b37b34a2832cce1b8eb6c87f5b1e24b54267 slave: yulibaozi/mysql-slave:201807261706 sha256:712359cbe130bf282876ea7df85fb7f67d8843ab64e54f96a73fee72cef67d87  需要定义k8s相关的资源(svc,sts,secret)
  相关资源的解释  为什么需要secret？
secret是来存储ceph的配置信息中的key的,一定要记得在存入secret之前进行base64编码。如果管理员提供给你了一个key，2个rbdImage，那么只需要一个secret，如果给了你两个key,那么需要两个secret
 为什么需要sts？
sts和deployment类似，只不过deployment用于无状态的应用而sts应用于有状态的应用，master和slave各一个sts
 为什么需要svc？
svc是为外部提供服务的，master一个svc，slave一个svc。
  千里之行 创建secret  假设管理员给你的ceph信息如下:
key = AQAqfzNaofxHLBAAS7qY64uE/ddqWLOMVDhkAA== rbdImagename = k8s_image01 user = admin（假设默认是admin,如果不是就问ceph管理员要）  执行命令把ceph的Key使用base64编码</description>
			<content type="html"><![CDATA[

<h3 id="血与泪的感慨">血与泪的感慨</h3>

<p>我的天空星星都亮了，一扫之前的阴霾，喜上眉梢，多日的折磨成了幻影，程序员就是这样，痛并快乐着。接下来，开始我们伟大的分享之路，分享如何在kubernetes中搭建mysql主从集群,不踩坑的快速搭建master主从集群,让你们早点回家，多陪陪女友，愿码友们不终日与十姊妹为伍。</p>

<h3 id="千里之行始于足下">千里之行始于足下</h3>

<ol>
<li><p>集群版本信息</p>

<pre><code>[root@kube03 ~]# kubectl version

Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, BuildDate:&quot;2018-03-21T15:21:50Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.6&quot;, BuildDate:&quot;2018-03-21T15:13:31Z&quot;, GoVersion:&quot;go1.9.3&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}
</code></pre></li>

<li><p>需要ceph的相关配置:key,rbdImage名字(需要找ceph管理员)</p></li>

<li><p>需要基础镜像</p>

<pre><code>镜像仓库: docker.io/yulibaozi
master: yulibaozi/mysql-master:201807261706 sha256:65ec774cd3a5e71bae1864dffbb1b37b34a2832cce1b8eb6c87f5b1e24b54267
slave: yulibaozi/mysql-slave:201807261706   sha256:712359cbe130bf282876ea7df85fb7f67d8843ab64e54f96a73fee72cef67d87
</code></pre></li>

<li><p>需要定义k8s相关的资源(svc,sts,secret)</p></li>
</ol>

<h4 id="相关资源的解释">相关资源的解释</h4>

<ol>
<li><p>为什么需要secret？</p>

<p>secret是来存储ceph的配置信息中的key的,一定要记得在存入secret之前进行base64编码。如果管理员提供给你了一个key，2个rbdImage，那么只需要一个secret，如果给了你两个key,那么需要两个secret</p></li>

<li><p>为什么需要sts？</p>

<p>sts和deployment类似，只不过deployment用于无状态的应用而sts应用于有状态的应用，master和slave各一个sts</p></li>

<li><p>为什么需要svc？</p>

<p>svc是为外部提供服务的，master一个svc，slave一个svc。</p></li>
</ol>

<h3 id="千里之行">千里之行</h3>

<h4 id="创建secret">创建secret</h4>

<ol>
<li><p>假设管理员给你的ceph信息如下:</p>

<pre><code>key = AQAqfzNaofxHLBAAS7qY64uE/ddqWLOMVDhkAA==
rbdImagename = k8s_image01
user = admin（假设默认是admin,如果不是就问ceph管理员要）
</code></pre></li>

<li><p>执行命令把ceph的Key使用base64编码</p>

<pre><code>[root@kube03 yuli]# echo &quot;AQAqfzNaofxHLBAAS7qY64uE/ddqWLOMVDhkAA==&quot; | base64
QVFBcWZ6TmFvZnhITEJBQVM3cVk2NHVFL2RkcVdMT01WRGhrQUE9PQo=
</code></pre></li>

<li><p>创建secret文件</p>

<pre><code>[root@kube03 sts]# cat 1ceph-secret.json

{
&quot;apiVersion&quot;: &quot;v1&quot;,
&quot;data&quot;: {
    &quot;key&quot;: &quot;QVFBcWZ6TmFvZnhITEJBQVM3cVk2NHVFL2RkcVdMT01WRGhrQUE9PQo=&quot;
},
&quot;kind&quot;: &quot;Secret&quot;,
&quot;metadata&quot;: {
    &quot;name&quot;: &quot;ceph-secret&quot;,
    &quot;namespace&quot;: &quot;yulibaozi&quot;
},
&quot;type&quot;: &quot;Opaque&quot;
}
</code></pre></li>
</ol>

<p>（可选）如果没有namespace,需要创建,创建namespace的命令如下:</p>

<pre><code>kubectl create namespace yulibaozi
</code></pre>

<p>执行命令创建secret</p>

<pre><code>kubectl create -f 1ceph-secret.json
</code></pre>

<p>secret 创建完毕了</p>

<h4 id="创建master的sts">创建master的sts</h4>

<p>master的sts如下:</p>

<pre><code>[root@kube03 sts]# cat 2master-sts.json
{
    &quot;apiVersion&quot;: &quot;apps/v1&quot;,
    &quot;kind&quot;: &quot;StatefulSet&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;name&quot;: &quot;mysql-master&quot;
        },
        &quot;name&quot;: &quot;mysql-master&quot;,
        &quot;namespace&quot;: &quot;yulibaozi&quot;
    },
    &quot;spec&quot;: {
        &quot;podManagementPolicy&quot;: &quot;OrderedReady&quot;,
        &quot;replicas&quot;: 1,
        &quot;revisionHistoryLimit&quot;: 10,
        &quot;selector&quot;: {
            &quot;matchLabels&quot;: {
                &quot;app&quot;: &quot;mysql-master&quot;
            }
        },
        &quot;serviceName&quot;: &quot;mysql-master&quot;,
        &quot;template&quot;: {
            &quot;metadata&quot;: {
                &quot;labels&quot;: {
                    &quot;app&quot;: &quot;mysql-master&quot;
                }
            },
            &quot;spec&quot;: {
                &quot;containers&quot;: [
                    {
                        &quot;env&quot;: [
                            {
                                &quot;name&quot;: &quot;MYSQL_ROOT_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_USER&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_PASSWORD&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            }
                        ],
                        &quot;image&quot;: &quot;yulibaozi/mysql-master:201807261706&quot;,
                        &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;,
                        &quot;name&quot;: &quot;mysql-master&quot;,
                        &quot;ports&quot;: [
                            {
                                &quot;containerPort&quot;: 3306,
                                &quot;protocol&quot;: &quot;TCP&quot;
                            }
                        ],
                        &quot;resources&quot;: {},
                        &quot;terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
                        &quot;terminationMessagePolicy&quot;: &quot;File&quot;,
                        &quot;volumeMounts&quot;: [
                            {
            ③                    &quot;mountPath&quot;: &quot;/var/lib/mysql&quot;,
                                &quot;name&quot;: &quot;data&quot;
                            }
                        ]
                    }
                ],
                &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
                &quot;restartPolicy&quot;: &quot;Always&quot;,
                &quot;schedulerName&quot;: &quot;default-scheduler&quot;,
                &quot;securityContext&quot;: {},
                &quot;terminationGracePeriodSeconds&quot;: 10,
                &quot;volumes&quot;: [
                    {
                        &quot;name&quot;: &quot;data&quot;,
                        &quot;rbd&quot;: {
                            &quot;fsType&quot;: &quot;xfs&quot;,
            ②               &quot;image&quot;: &quot;k8s_image01&quot;,
                            &quot;keyring&quot;: &quot;/etc/ceph/keyring&quot;,
                            &quot;monitors&quot;: [
            ⑤                   &quot;192.168.0.199:6789&quot;
                            ],
                            &quot;pool&quot;: &quot;rbd&quot;,
                            &quot;secretRef&quot;: {
            ①                   &quot;name&quot;: &quot;ceph-secret&quot;
                            },
            ④               &quot;user&quot;: &quot;admin&quot;
                        }
                    }
                ]
            }
        },
        &quot;updateStrategy&quot;: {
            &quot;rollingUpdate&quot;: {
                &quot;partition&quot;: 0
            },
            &quot;type&quot;: &quot;RollingUpdate&quot;
        }
    },
    &quot;status&quot;: {
    }
}

相关解释：
①: secret的名字,就是上文中创建的secret名字
②：ceph管理员给你的块名字
③：容器中挂载的目录,这么目录就是mysql存数据的目录,如果不知道怎么做就不要改
④：ceph的用户名，问下ceph管理员是什么，如果不是admin就得改
⑤：monitors:ceph的monitor地址，找ceph管理员要，可以是一个,也可以是多个

</code></pre>

<p>确认了4点后向下看
创建master-sts:</p>

<pre><code>kubectl create -f 2master-sts.json
</code></pre>

<p>查看状态:</p>

<pre><code>[root@kube03 sts]# kubectl get pod -n yulibaozi

NAME             READY     STATUS    RESTARTS   AGE
mysql-master-0   1/1       Running   0          2h

如果状态一直是CreateContaining，等10分钟左右还不是Running说明有问题,愿上天眷顾你。如果的确有问题，那么查问题。
</code></pre>

<p>master的sts创建完毕。</p>

<h4 id="创建slave的sts">创建slave的sts</h4>

<p>slave的sts配置如下：</p>

<pre><code>[root@kube03 sts]# cat 3slave-sts.json

{
    &quot;apiVersion&quot;: &quot;apps/v1&quot;,
    &quot;kind&quot;: &quot;StatefulSet&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;name&quot;: &quot;mysql-slave&quot;
        },
        &quot;name&quot;: &quot;mysql-slave&quot;,
        &quot;namespace&quot;: &quot;yulibaozi&quot;
    },
    &quot;spec&quot;: {
        &quot;podManagementPolicy&quot;: &quot;OrderedReady&quot;,
        &quot;replicas&quot;: 1,
        &quot;revisionHistoryLimit&quot;: 10,
        &quot;selector&quot;: {
            &quot;matchLabels&quot;: {
                &quot;name&quot;: &quot;mysql-slave&quot;
            }
        },
        &quot;serviceName&quot;: &quot;mysql-slave&quot;,
        &quot;template&quot;: {
            &quot;metadata&quot;: {
                &quot;creationTimestamp&quot;: null,
                &quot;labels&quot;: {
                    &quot;name&quot;: &quot;mysql-slave&quot;
                }
            },
            &quot;spec&quot;: {
                &quot;containers&quot;: [
                    {
                        &quot;env&quot;: [
                            {
                                &quot;name&quot;: &quot;MYSQL_ROOT_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_PASSWORD&quot;,
                                &quot;value&quot;: &quot;root&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_USER&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MYSQL_REPLICATION_PASSWORD&quot;,
                                &quot;value&quot;: &quot;repl&quot;
                            },
                            {
                                &quot;name&quot;: &quot;MASTER_HOST&quot;,
        ①                        &quot;value&quot;: &quot;mysql-slave.yulibaozi&quot;
                            }
                        ],
                        &quot;image&quot;: &quot;yulibaozi/mysql-slave:201807261706&quot;,
                        &quot;imagePullPolicy&quot;: &quot;IfNotPresent&quot;,
                        &quot;name&quot;: &quot;mysql-slave&quot;,
                        &quot;ports&quot;: [
                            {
                                &quot;containerPort&quot;: 3306,
                                &quot;protocol&quot;: &quot;TCP&quot;
                            }
                        ],
                        &quot;resources&quot;: {},
                        &quot;terminationMessagePath&quot;: &quot;/dev/termination-log&quot;,
                        &quot;terminationMessagePolicy&quot;: &quot;File&quot;,
                        &quot;volumeMounts&quot;: [
                            {
        ②                       &quot;mountPath&quot;: &quot;/var/lib/mysql&quot;,
                                &quot;name&quot;: &quot;data&quot;
                            }
                        ]
                    }
                ],
                &quot;dnsPolicy&quot;: &quot;ClusterFirst&quot;,
                &quot;restartPolicy&quot;: &quot;Always&quot;,
                &quot;schedulerName&quot;: &quot;default-scheduler&quot;,
                &quot;securityContext&quot;: {},
                &quot;terminationGracePeriodSeconds&quot;: 10,
                &quot;volumes&quot;: [
                    {
                        &quot;name&quot;: &quot;data&quot;,
                        &quot;rbd&quot;: {
                            &quot;fsType&quot;: &quot;xfs&quot;,
        ③                   &quot;image&quot;: &quot;k8s_image02&quot;,
                            &quot;keyring&quot;: &quot;/etc/ceph/keyring&quot;,
        ⑥                   &quot;monitors&quot;: [
                                &quot;192.168.0.199:6789&quot;
                            ],
                            &quot;pool&quot;: &quot;rbd&quot;,
                            &quot;secretRef&quot;: {
        ④                       &quot;name&quot;: &quot;ceph-secret&quot;
                            },
                            &quot;user&quot;: &quot;admin&quot;
        ⑤               }
                    }
                ]
            }
        },
        &quot;updateStrategy&quot;: {
            &quot;rollingUpdate&quot;: {
                &quot;partition&quot;: 0
            },
            &quot;type&quot;: &quot;RollingUpdate&quot;
        }
    },
    &quot;status&quot;: {
    }
}

相关解释：
①：如果你的namespace不是yulubaozi,需要把yulibaozi替换成你的命名空间名字
②：需要配置mysql的数据目录，可以选择不修改，默认是它
③：ceph管理分配给你的rbdImage名字，注意:不得与master是同一个名字（同一个名字是指:同一个key，同一个rbdImage名字）
④：secret的名字，secret里面的Key和rbdImage相对应，我这里master和slave使用的相同的key，而rbdImage名字不相同，所以master可以和slave使用一样的secret；如果你的key和rbdImage是不同的两套，那么需要创建两个secret,master一个,slave一个,secret的创建方式在上面提到了。
⑥：monitors:ceph的monitor地址，找ceph管理员要，可以是一个,也可以是多个,可以和master一致也可和master不一致
</code></pre>

<p>上述确认完毕后
执行命令创建slave的sts:</p>

<pre><code>kubectl create -f 3slave-sts.json
</code></pre>

<p>查看sts的pod状态:</p>

<pre><code>[root@kube03 sts]# kubectl get pod -n yulibaozi

NAME             READY     STATUS    RESTARTS   AGE
mysql-master-0   1/1       Running   0          2h
mysql-slave-0    1/1       Running   0          1h

如果等待15分钟不是Running，可能有问题，你可以选择耐心等待下，你可以选择采用和master一样的解决办法。
</code></pre>

<h4 id="创建master的svc">创建master的svc：</h4>

<p>master的svc配置如下：</p>

<pre><code>[root@kube03 sts]# cat 4master-svc.json

{
    &quot;apiVersion&quot;: &quot;v1&quot;,
    &quot;kind&quot;: &quot;Service&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;app&quot;: &quot;mysql-master&quot;
        },
        &quot;name&quot;: &quot;mysql-master&quot;,
        &quot;namespace&quot;: &quot;yulibaozi&quot;
    },
    &quot;spec&quot;: {
        &quot;ports&quot;: [
            {
                &quot;nodePort&quot;: 33306,
                &quot;port&quot;: 3306,
                &quot;protocol&quot;: &quot;TCP&quot;,
                &quot;targetPort&quot;: 3306
            }
        ],
        &quot;selector&quot;: {
            &quot;app&quot;: &quot;mysql-master&quot;
        },
        &quot;sessionAffinity&quot;: &quot;None&quot;,
        &quot;type&quot;: &quot;NodePort&quot;
    },
    &quot;status&quot;: {
        &quot;loadBalancer&quot;: {}
    }
}

如上,我们暴露的NodePort是33306端口，假设我们的宿主机IP:192.168.0.198

</code></pre>

<p>执行命令创建master的svc：</p>

<pre><code>kubectl create -f 4master-svc.json
</code></pre>

<p>访问master数据库验证是否可以连接：</p>

<ol>
<li><p>telnet的方式查看是否能连接</p>

<pre><code>telnet 192.168.0.198 33306 
</code></pre></li>

<li><p>进入mysql</p>

<pre><code>mysql -h 192.168.0.198 -P 33306 -uroot -proot
</code></pre></li>
</ol>

<p>如果能够进入mysql的master节点，执行：</p>

<pre><code>mysql&gt; show master status;

+------------+----------+--------------+------------------+-------------------------------------------+
| File       | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set                         |
+------------+----------+--------------+------------------+-------------------------------------------+
| bin.000005 |      194 |              |                  | 1f14ce9c-b1a8-11e8-becd-02420aa83b04:1-12 |
+------------+----------+--------------+------------------+-------------------------------------------+
1 row in set (0.00 sec)
</code></pre>

<p>一定记住其中的</p>

<pre><code>File
Position
</code></pre>

<p>能走到此处，至少说明master可用。</p>

<h4 id="创建mysql的slave节点">创建mysql的slave节点</h4>

<p>mysql的slave节点配置如下:</p>

<pre><code>[root@kube03 sts]# cat 5slave-svc.json

{
    &quot;apiVersion&quot;: &quot;v1&quot;,
    &quot;kind&quot;: &quot;Service&quot;,
    &quot;metadata&quot;: {
        &quot;labels&quot;: {
            &quot;name&quot;: &quot;mysql-slave&quot;
        },
        &quot;name&quot;: &quot;mysql-slave&quot;,
        &quot;namespace&quot;: &quot;yuli&quot;
    },
    &quot;spec&quot;: {
        &quot;ports&quot;: [
            {
                &quot;nodePort&quot;:33307,
                &quot;port&quot;: 3306,
                &quot;protocol&quot;: &quot;TCP&quot;,
                &quot;targetPort&quot;: 3306
            }
        ],
        &quot;selector&quot;: {
            &quot;name&quot;: &quot;mysql-slave&quot;
        },
        &quot;sessionAffinity&quot;: &quot;None&quot;,
        &quot;type&quot;: &quot;NodePort&quot;
    },
    &quot;status&quot;: {
        &quot;loadBalancer&quot;: {}
    }
}

如上,我们暴露的NodePort是33307端口，假设我们的宿主机IP:192.168.0.198
</code></pre>

<p>执行命令创建slave的svc:</p>

<pre><code>kubectl create -f 5slave-svc.json
</code></pre>

<p>验证是否可用的方式和master节点一样</p>

<p>slave配置步骤：
执行命令:</p>

<pre><code>mysql -h 192.168.0.198 -P 33307 -uroot -proot
</code></pre>

<p>在slave的数据库中配置master：</p>

<pre><code>mysql&gt; change master to master_host='192.168.0.198', master_user='root', master_password='root', master_port=33306, master_log_file='bin.000005', master_log_pos=194, master_connect_retry=30;

Query OK, 0 rows affected, 2 warnings (0.19 sec)

</code></pre>

<p>可能你需要修改的是：</p>

<pre><code> master_host  
 master_user
 master_password
 master_port 
 master_log_file 上面让你记住的File
 master_log_pos  上面让你记住的Position
</code></pre>

<p>查看slave的复制状态：</p>

<pre><code>mysql&gt; show slave status \G;

*************************** 1. row ***************************
               Slave_IO_State:
                  Master_Host: 10.151.30.141
                  Master_User: root
                  Master_Port: 33306
                Connect_Retry: 30
              Master_Log_File: bin.000005
          Read_Master_Log_Pos: 194
               Relay_Log_File: rrelay.000001
                Relay_Log_Pos: 4
        Relay_Master_Log_File: bin.000005
             Slave_IO_Running: No
            Slave_SQL_Running: No
                            .
                            .
                            .
1 row in set (0.00 sec)
</code></pre>

<p>以下两个字段一定要注意，关键操作:</p>

<pre><code>  Slave_IO_Running: No
  Slave_SQL_Running: No
  
  这两个结果一定得是YES,如果不是,先不管，向下执行
</code></pre>

<p>执行名字开始主从复制:</p>

<pre><code>mysql&gt; start slave;

ERROR 1872 (HY000): Slave failed to initialize relay log info structure from the repository
</code></pre>

<p>报错了，问题出在mysql的配置文件mysqld.cnf中</p>

<p>执行命令进入slave容器:</p>

<pre><code>kubectl exec -it mysql-slave-0 -n yulibaozi /bin/bash
</code></pre>

<p>进入以下路径修改配置文件：</p>

<pre><code>路径在: /etc/mysql/mysql.conf.d

修改配置:
relay_log=rrelay.log =&gt; relay_log=relay.log
</code></pre>

<p>修改完毕后,执行:</p>

<pre><code>mysql&gt; reset slave;

Query OK, 0 rows affected (0.23 sec)
</code></pre>

<p>再次查看主从状态:</p>

<pre><code>mysql&gt; show slave status \G;
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: 10.151.30.141
                  Master_User: root
                  Master_Port: 33306
                Connect_Retry: 30
              Master_Log_File: bin.000005
          Read_Master_Log_Pos: 194
               Relay_Log_File: rrelay.000002
                Relay_Log_Pos: 314
        Relay_Master_Log_File: bin.000005
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
                            .
                            .
                            .
1 row in set (0.00 sec)

</code></pre>

<h4 id="黎明已到">黎明已到</h4>

<p>然后创建数据库，创建表，写入数据，验证主从是否可用，同时删除pod，在启动判断数据是否持久化。基于ceph的mysql主从搭建完毕。</p>
]]></content>
		</item>
		
		<item>
			<title>Go func作为类型妙用提高代码的抽象粒度</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-08-20-go-funcs/</link>
			<pubDate>Mon, 20 Aug 2018 22:25:59 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-08-20-go-funcs/</guid>
			<description>#### 前提  这种实现方式的前提是: 把对象的行为作为了字段传输，如果一个对象有一个操作那么就需要一个字段，有N个操作则有N个字段,例如下方的Div操作,通常是用于同方法需要不同的实现方式。
优点: 1. 在设计上并不关心具体的实现方式，只是指明了具体的入参和出参。 2. 在具体实现上，我们把具体实现归还给了编码者,他做他需要做的操作以及错误处理，这样我们把实现的权利交还给了编码者，提高了系统的灵活性和扩展度。  和结构体方法实现的对比: 1. 更轻松实现统一的日志处理，校验参数，转换操作。  假定使用场景: 1. 需要自实现同一类操作类型(例如:节点优选策略）但有不同的侧重实现方式(例如有些要求cpu的权重高些，有些要求memory多些，有些要求GPU要求多些)  实现思路:  明确是做同一类操作,即是实现节点的优选策略,但是不同的场景有不同的侧重点或者他不满足当前的实现方式，他可以自实现 虽然他可以有不同的实现方式，但是我们需要规范的他的入参和出参。所以我们需要定义函数类型，即：
// arithmetic 定义一个通用的计算模型 type arithmetic func(a, b int64) (int64, error)  定义统一的函数名字，并实现:
// arithmetic 定义一个通用的计算模型 type arithmetic func(a, b int64) (int64, error) func (ar arithmetic) Div(a, b int64) (int64, error) { fmt.Println(&amp;quot;print div log&amp;quot;) return ar(a, b) } // Diver 集合 type Diver interface { Div(a, b int64) (int64, error) } var _ Diver = arithmetic(nil  定义不同的角色（当然也可以使用同一角色）</description>
			<content type="html"><![CDATA[

<pre><code>#### 前提
</code></pre>

<p>这种实现方式的前提是:
    把对象的行为作为了字段传输，如果一个对象有一个操作那么就需要一个字段，有N个操作则有N个字段,例如下方的Div操作,通常是用于同方法需要不同的实现方式。</p>

<h4 id="优点">优点:</h4>

<pre><code>1. 在设计上并不关心具体的实现方式，只是指明了具体的入参和出参。
2. 在具体实现上，我们把具体实现归还给了编码者,他做他需要做的操作以及错误处理，这样我们把实现的权利交还给了编码者，提高了系统的灵活性和扩展度。
</code></pre>

<h4 id="和结构体方法实现的对比">和结构体方法实现的对比:</h4>

<pre><code>1. 更轻松实现统一的日志处理，校验参数，转换操作。
</code></pre>

<h4 id="假定使用场景">假定使用场景:</h4>

<pre><code>1. 需要自实现同一类操作类型(例如:节点优选策略）但有不同的侧重实现方式(例如有些要求cpu的权重高些，有些要求memory多些，有些要求GPU要求多些)
</code></pre>

<h4 id="实现思路">实现思路:</h4>

<ol>
<li>明确是做同一类操作,即是实现节点的优选策略,但是不同的场景有不同的侧重点或者他不满足当前的实现方式，他可以自实现</li>

<li><p>虽然他可以有不同的实现方式，但是我们需要规范的他的入参和出参。所以我们需要定义函数类型，即：</p>

<pre><code>// arithmetic 定义一个通用的计算模型
type arithmetic func(a, b int64) (int64, error)
</code></pre></li>

<li><p>定义统一的函数名字，并实现:</p>

<pre><code>// arithmetic 定义一个通用的计算模型
type arithmetic func(a, b int64) (int64, error)

func (ar arithmetic) Div(a, b int64) (int64, error) {
    fmt.Println(&quot;print div log&quot;)
    return ar(a, b)
}

// Diver 集合
type Diver interface {
    Div(a, b int64) (int64, error)
}

var _ Diver = arithmetic(nil
</code></pre></li>

<li><p>定义不同的角色（当然也可以使用同一角色）</p>

<pre><code>type Roc struct {
    d Diver
}
    
type Yulibaozi struct {
    d Diver
}
    
</code></pre></li>

<li><p>在声明和创建角色时，指定其实现策略：</p>

<pre><code>yulibaozi := &amp;Yulibaozi{
        d: arithmetic(func(a, b int64) (divr int64, err error) {
            defer func() {
                if err1 := recover(); err1 != nil {
                    err = fmt.Errorf(&quot;%v&quot;, err1)
                }
            }()
            divr = a / b
            return
        }),
    }
    fmt.Println(yulibaozi.d.Div(2, 0))
</code></pre>

<p>5.1 当然,你也可以使用另外一种方式(注册的方式)：</p>

<pre><code>var (
    roc *Roc
)
    
func register(fun arithmetic) {
    roc = &amp;Roc{
        d: arithmetic(fun),
    }
}
    
</code></pre></li>
</ol>

<h4 id="完整代码">完整代码</h4>

<pre><code>
var (
	roc *Roc
)

func register(fun arithmetic) {
	roc = &amp;Roc{
		d: arithmetic(fun),
	}
}

func main() {
	fmt.Println(&quot;====第一次执行====&quot;)
	roc = &amp;Roc{d: arithmetic(
		func(a, b int64) (int64, error) {
			if b == 0 {
				b = 1
			}
			return a / b, nil
		},
	)}
	fmt.Println(roc.d.Div(2, 4))
	fmt.Println(&quot;====第二次执行,覆盖了第一次的实现方式====&quot;)
	fun := func(a, b int64) (int64, error) {
		if b == 0 {
			b = 1
		}
		return a * 100 / b, nil
	}
	register(fun)
	fmt.Println(roc.d.Div(2, 4))
	yulibaozi := &amp;Yulibaozi{
		d: arithmetic(func(a, b int64) (divr int64, err error) {
			defer func() {
				if err1 := recover(); err1 != nil {
					err = fmt.Errorf(&quot;%v&quot;, err1)
				}
			}()
			divr = a / b
			return
		}),
	}
	fmt.Println(yulibaozi.d.Div(2, 0))
}

type Roc struct {
	d Diver
}

type Yulibaozi struct {
	d Diver
}

// arithmetic 定义一个通用的计算模型
type arithmetic func(a, b int64) (int64, error)

func (ar arithmetic) Div(a, b int64) (int64, error) {
	fmt.Println(&quot;print div log&quot;)
	return ar(a, b)
}

// Diver 集合
type Diver interface {
	Div(a, b int64) (int64, error)
}

var _ Diver = arithmetic(nil)
</code></pre>
]]></content>
		</item>
		
		<item>
			<title>kubernetes中请求网络链路的分析与总结</title>
			<link>https://yulibaozi.com/posts/kubernetes/knowledge/2018-07-22-proxy-network/</link>
			<pubDate>Sun, 22 Jul 2018 20:54:11 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/knowledge/2018-07-22-proxy-network/</guid>
			<description>k8s中请求网络链路的分析与总结  前要 这篇文章，你能明白那些问题？  从集群外部请求的时候，整个请求的走向是怎么样的？ 在service中port,targetPort和nodePort的区别是什么？ iptables是怎样工作的？ 谁来维护了数据的正确性，及时性和稳定性？  基础知识: port指当使用ClusterIP访问这个服务所使用的port targetPort 指当使用PodIP访问后端Pod使用的port nodePort 指当在外部(nodeport)访问这个服务使用的Port  正文 前段时间,请求发生了time out或者connection refused这类问题,从k8s的角度来说,我并不能提出有效的猜想更不能解决这个问题是催生这篇文章的关键所在。于是,我对自己提出了上面的那些问题，能够让我清楚的记住一个请求的走向。
请求的走向是：
外部(nodeport)/clusterIP &amp;ndash;&amp;gt; iptables &amp;ndash;&amp;gt; 到endpoints(pod的Ip地址)
当一个请求到来时,首先会进入iptables,iptables来做转发,那么iptables怎么转发的呢？当一个service被创建的时候,会在iptables上创建一个转发规则(实际是kube-proxy来创建和维护这个规则的)
下面以集群中某个服务为例：
基本信息: PODIP 172.22.50.18 172.22.36.51 PORT: port: 8080, targetPort: 8080, nodePort&amp;quot;: 32080 CLUSTERIP: 172.50.71.49  iptables中的规则:
root@node-2:~# iptables-save | grep yce/yce-svc:http -A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;yce/yce-svc:http&amp;quot; -m tcp --dport 32080 -j KUBE-MARK-MASQ -A KUBE-NODEPORTS -p tcp -m comment --comment &amp;quot;yce/yce-svc:http&amp;quot; -m tcp --dport 32080 -j KUBE-SVC-33TZ5W3QVRYDY6UM -A KUBE-SEP-JIUUF7F4K5DSHT3L -s 172.</description>
			<content type="html"><![CDATA[

<pre><code>k8s中请求网络链路的分析与总结
</code></pre>

<hr />

<h4 id="前要">前要</h4>

<h4 id="这篇文章-你能明白那些问题">这篇文章，你能明白那些问题？</h4>

<ol>
<li>从集群外部请求的时候，整个请求的走向是怎么样的？</li>
<li>在service中port,targetPort和nodePort的区别是什么？</li>
<li>iptables是怎样工作的？</li>
<li>谁来维护了数据的正确性，及时性和稳定性？</li>
</ol>

<h4 id="基础知识">基础知识:</h4>

<pre><code>port指当使用ClusterIP访问这个服务所使用的port
targetPort 指当使用PodIP访问后端Pod使用的port
nodePort 指当在外部(nodeport)访问这个服务使用的Port
</code></pre>

<h4 id="正文">正文</h4>

<p>前段时间,请求发生了time out或者connection refused这类问题,从k8s的角度来说,我并不能提出有效的猜想更不能解决这个问题是催生这篇文章的关键所在。于是,我对自己提出了上面的那些问题，能够让我清楚的记住一个请求的走向。</p>

<p>请求的走向是：</p>

<p>外部(nodeport)/clusterIP &ndash;&gt; iptables &ndash;&gt; 到endpoints(pod的Ip地址)</p>

<p>当一个请求到来时,首先会进入iptables,iptables来做转发,那么iptables怎么转发的呢？当一个service被创建的时候,会在iptables上创建一个转发规则(实际是kube-proxy来创建和维护这个规则的)</p>

<p>下面以集群中某个服务为例：</p>

<pre><code>基本信息:
PODIP 
    172.22.50.18
    172.22.36.51
PORT:
    port: 8080,
    targetPort: 8080,
    nodePort&quot;: 32080
CLUSTERIP:
    172.50.71.49

</code></pre>

<p>iptables中的规则:</p>

<pre><code>root@node-2:~# iptables-save | grep yce/yce-svc:http
-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp --dport 32080 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp --dport 32080 -j KUBE-SVC-33TZ5W3QVRYDY6UM
-A KUBE-SEP-JIUUF7F4K5DSHT3L -s 172.22.36.51/32 -m comment --comment &quot;yce/yce-svc:http&quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-JIUUF7F4K5DSHT3L -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp -j DNAT --to-destination 172.22.36.51:8080
-A KUBE-SEP-WAKCSVYOKXRU3L56 -s 172.22.50.18/32 -m comment --comment &quot;yce/yce-svc:http&quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-WAKCSVYOKXRU3L56 -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp -j DNAT --to-destination 172.22.50.18:8080
-A KUBE-SERVICES -d 172.50.71.49/32 -p tcp -m comment --comment &quot;yce/yce-svc:http cluster IP&quot; -m tcp --dport 8080 -j KUBE-SVC-33TZ5W3QVRYDY6UM
-A KUBE-SVC-33TZ5W3QVRYDY6UM -m comment --comment &quot;yce/yce-svc:http&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-JIUUF7F4K5DSHT3L
-A KUBE-SVC-33TZ5W3QVRYDY6UM -m comment --comment &quot;yce/yce-svc:http&quot; -j KUBE-SEP-WAKCSVYOKXRU3L56
</code></pre>

<p>当以nodeport访问时,会进入:</p>

<pre><code>-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp --dport 32080 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp --dport 32080 -j KUBE-SVC-33TZ5W3QVRYDY6UM
</code></pre>

<p>然后会跳转到KUBE-SVC-33TZ5W3QVRYDY6UM:</p>

<pre><code>-A KUBE-SVC-33TZ5W3QVRYDY6UM -m comment --comment &quot;yce/yce-svc:http&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-JIUUF7F4K5DSHT3L
-A KUBE-SVC-33TZ5W3QVRYDY6UM -m comment --comment &quot;yce/yce-svc:http&quot; -j KUBE-SEP-WAKCSVYOKXRU3L56
</code></pre>

<p>在33TZ5W3QVRYDY6UM中,我们看到了0.5的概率跳转到KUBE-SEP-JIUUF7F4K5DSHT3L链,还有0.5的概率跳转到WAKCSVYOKXRU3L56链</p>

<p>在JIUUF7F4K5DSHT3L中，通过修改目的地址(DNAT)指向到172.22.36.51:8080，这个地址是我们后端podIP之一。</p>

<pre><code>-A KUBE-SEP-JIUUF7F4K5DSHT3L -s 172.22.36.51/32 -m comment --comment &quot;yce/yce-svc:http&quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-JIUUF7F4K5DSHT3L -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp -j DNAT --to-destination 172.22.36.51:8080
</code></pre>

<p>在WAKCSVYOKXRU3L56中也是通过DNAT指向到172.22.50.18:8080,这个地址也是我们后端的podIP之一。</p>

<pre><code>-A KUBE-SEP-WAKCSVYOKXRU3L56 -s 172.22.50.18/32 -m comment --comment &quot;yce/yce-svc:http&quot; -j KUBE-MARK-MASQ
-A KUBE-SEP-WAKCSVYOKXRU3L56 -p tcp -m comment --comment &quot;yce/yce-svc:http&quot; -m tcp -j DNAT --to-destination 172.22.50.18:8080
</code></pre>

<p>在通过使用clusterIP访问时,即目标IP:172.50.71.49和目标PORT是8080,协议是tcp时,会跳转到KUBE-SVC-33TZ5W3QVRYDY6UM,后续跳转链和上面分析一致。</p>

<pre><code>-A KUBE-SERVICES -d 172.50.71.49/32 -p tcp -m comment --comment &quot;yce/yce-svc:http cluster IP&quot; -m tcp --dport 8080 -j KUBE-SVC-33TZ5W3QVRYDY6UM
</code></pre>

<p>上面的分析中,我们知道了通过clusterIP和NodePort请求时,整个请求的跳转走向，那么维护iptables的数据正确性呢？当svc被删除,被修改,当pod被重新调度,被杀掉,不可用的时候，谁来保证iptables的正确性呢？</p>

<p>答案是kube-proxy,kube-proxy在幕后默默的维护了这一切。感谢这哥们吧。
源码地址: <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go">https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go</a></p>

<p>定义的链:</p>

<pre><code>	// the services chain
	kubeServicesChain utiliptables.Chain = &quot;KUBE-SERVICES&quot;
	// the external services chain
	kubeExternalServicesChain utiliptables.Chain = &quot;KUBE-EXTERNAL-SERVICES&quot;
	// the nodeports chain
	kubeNodePortsChain utiliptables.Chain = &quot;KUBE-NODEPORTS&quot;
	// the kubernetes postrouting chain
	kubePostroutingChain utiliptables.Chain = &quot;KUBE-POSTROUTING&quot;
	// the mark-for-masquerade chain
	KubeMarkMasqChain utiliptables.Chain = &quot;KUBE-MARK-MASQ&quot;
	// the mark-for-drop chain
	KubeMarkDropChain utiliptables.Chain = &quot;KUBE-MARK-DROP&quot;
	// the kubernetes forward chain
	kubeForwardChain utiliptables.Chain = &quot;KUBE-FORWARD&quot;
</code></pre>

<p>计算概率:</p>

<pre><code>func computeProbability(n int) string {
	return fmt.Sprintf(&quot;%0.5f&quot;, 1.0/float64(n))
}
</code></pre>

<p>在代码的144-156行:</p>

<pre><code>func (ep *endpointsInfo) Cleanup() {
	Log(ep, &quot;Endpoint Cleanup&quot;, 3)
	ep.refCount--
	// Remove the remote hns endpoint, if no service is referring it
	// Never delete a Local Endpoint. Local Endpoints are already created by other entities.
	// Remove only remote endpoints created by this service
	if ep.refCount &lt;= 0 &amp;&amp; !ep.isLocal {
		glog.V(4).Infof(&quot;Removing endpoints for %v, since no one is referencing it&quot;, ep)
		deleteHnsEndpoint(ep.hnsID)
		ep.hnsID = &quot;&quot;
	}

}
</code></pre>

<p>在这里真正删除不可用的endpoint。</p>

<p>在代码的1207-1221行:</p>

<pre><code>	// Delete chains no longer in use.
	for chain := range existingNATChains {
		if !activeNATChains[chain] {
			chainString := string(chain)
			if !strings.HasPrefix(chainString, &quot;KUBE-SVC-&quot;) &amp;&amp; !strings.HasPrefix(chainString, &quot;KUBE-SEP-&quot;) &amp;&amp; !strings.HasPrefix(chainString, &quot;KUBE-FW-&quot;) &amp;&amp; !strings.HasPrefix(chainString, &quot;KUBE-XLB-&quot;) {
				// Ignore chains that aren't ours.
				continue
			}
			// We must (as per iptables) write a chain-line for it, which has
			// the nice effect of flushing the chain.  Then we can remove the
			// chain.
			writeLine(proxier.natChains, existingNATChains[chain])
			writeLine(proxier.natRules, &quot;-X&quot;, chainString)
		}
	}
</code></pre>

<p>删除不在使用的chain,遍历所有的chain,proxy会维护一个活跃的chain Map,如果chain不存在这个Map中就会被删除。</p>

<p>在1304-1311行:</p>

<pre><code>err = proxier.iptables.RestoreAll(proxier.iptablesData.Bytes(), utiliptables.NoFlushTables, utiliptables.RestoreCounters)
	if err != nil {
		glog.Errorf(&quot;Failed to execute iptables-restore: %v&quot;, err)
		// Revert new local ports.
		glog.V(2).Infof(&quot;Closing local ports after iptables-restore failure&quot;)
		utilproxy.RevertPorts(replacementPortsMap, proxier.portsMap)
		return
	}
</code></pre>

<p>把新一轮同步iptables的规则同步到node的iptables中去.</p>

<p>在1105-1113行:</p>

<pre><code>	// Update healthchecks.  The endpoints list might include services that are
	// not &quot;OnlyLocal&quot;, but the services list will not, and the healthChecker
	// will just drop those endpoints.
	if err := proxier.healthChecker.SyncServices(serviceUpdateResult.hcServices); err != nil {
		glog.Errorf(&quot;Error syncing healtcheck services: %v&quot;, err)
	}
	if err := proxier.healthChecker.SyncEndpoints(endpointUpdateResult.hcEndpoints); err != nil {
		glog.Errorf(&quot;Error syncing healthcheck endoints: %v&quot;, err)
	}

</code></pre>

<p>我们看到了proxy与apiserver通信来实现service和Endpoint的同步。</p>

<h4 id="另外">另外</h4>

<p><strong>在之前一个版本:</strong>
kube-proxy不但要维护iptables的数据,kube-proxy还负责真正的负载均衡,使用Round Robin算法(循环列表逐个选取endpoint,一轮结束,从头开始下一轮),另一方面,为了支持session保持,还增加了affinityState查询(即:kube-proxy会从本地查找是否存在该IP的affinityState对象，如果存在且Session没有超时,那么这个请求就会指向affinityState对象所指向的那个pod,否则还是使用Round Robin算法。)</p>

<p>在之后的版本:kube-proxy只是单纯维护iptables的数据,而不管负载均衡，iptables来做负载均衡</p>
]]></content>
		</item>
		
		<item>
			<title>Kubernetes 1.11新特性,欢呼！</title>
			<link>https://yulibaozi.com/posts/kubernetes/knowledge/2018-06-26-kubernetes-up-to-1.11/</link>
			<pubDate>Tue, 26 Jun 2018 20:52:40 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/knowledge/2018-06-26-kubernetes-up-to-1.11/</guid>
			<description>kubernetes 1.11新特性 重要 文章同步微信公众号：容器时代(https://mp.weixin.qq.com/s/8bW-Et9WHVgQ8O2JwJarUw)
前言 Kubernetes 社区已经在6月21号放出了Kubernetes 1.11的第一个rc版本，这意味着现在你可以尝试着去运用一些新的特性和功能, 并在正式版上线前为开发组提供珍贵的反馈信息。正式版计划将在2018年6月26日上线，其中包含了六个全新的alpha功能和十八个更加成熟稳定的功能。
你可以从以下链接当中下载最新的Kubernetes1.11版本 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md
如果你希望将你的使用情况反馈给开发社区，可向Kubernetes 1.11 milestone提交一个新的issue(https://github.com/kubernetes/kubernetes/issues/new?milestone=v1.11)，并标记相应的SIG团队
SIG-AUTH  Provide RunAsGroup feature for Containers in a Pod(Alpha):
目前，我们可以控制容器的UID同时也可以补充组，但是，无法控制那些运行的容器的GID总是0（root）。这个功能能够以非root运行容器，并且帮助我们提高容器的安全级别。(https://github.com/kubernetes/features/issues/213)
 TokenRequest API and Kubelet integration(Alpha):
TokenRequest API 提供了对service account token基础的改进。这个功能将允许生成Secret APIs之外、针对特定对象的（例如external secretstores）、可以调整有效期的以及可以绑定于特定Pods 的tokens(https://github.com/kubernetes/features/issues/542)
 Limit node access to API(beta):
可以对节点上的API调用进行限制，限制节点的作用域，比如：只能自己节点上的Node API Objects，只能修改绑定在自己节点上的Pod Objects，只能获取自己节点上的secrets和configmaps 等。(https://github.com/kubernetes/features/issues/279)
 ClusterRole Aggregation（Stable）:
为了CustomResources and ExtensionAPIServers更容易支持 RBAC，我们可以让API extenders为其添加一些权限，比如修改，查看等。(https://github.com/kubernetes/features/issues/502)
  SIG-API-MACHINERY  Custom Resource Definition Versioning(beta):  CRD因其提高了Kubernetes的扩展性而被广泛接纳，为CRD增加版本，这样CRD的作者可以定义不同的资源版本，同时添加一种版本间的转换机制。目前，每个CR只有一个版本，每当更新的时候，只能重新增加一个CRD，然后以手动的方式同步。 * 支持API级别的版本控制 * 支持不同版本之间的转换 * 支持修改版本库 * 所有版本支持Validation/OpenAPI 模式 (https://github.</description>
			<content type="html"><![CDATA[

<h2 id="kubernetes-1-11新特性">kubernetes 1.11新特性</h2>

<h3 id="重要">重要</h3>

<p>文章同步微信公众号：容器时代(<a href="https://mp.weixin.qq.com/s/8bW-Et9WHVgQ8O2JwJarUw">https://mp.weixin.qq.com/s/8bW-Et9WHVgQ8O2JwJarUw</a>)</p>

<h3 id="前言">前言</h3>

<p>Kubernetes 社区已经在6月21号放出了Kubernetes 1.11的第一个rc版本，这意味着现在你可以尝试着去运用一些新的特性和功能, 并在正式版上线前为开发组提供珍贵的反馈信息。正式版计划将在2018年6月26日上线，其中包含了六个全新的alpha功能和十八个更加成熟稳定的功能。</p>

<p>你可以从以下链接当中下载最新的Kubernetes1.11版本
<a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md">https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md</a></p>

<p>如果你希望将你的使用情况反馈给开发社区，可向Kubernetes 1.11 milestone提交一个新的issue(<a href="https://github.com/kubernetes/kubernetes/issues/new?milestone=v1.11)，并标记相应的SIG团队">https://github.com/kubernetes/kubernetes/issues/new?milestone=v1.11)，并标记相应的SIG团队</a></p>

<h3 id="sig-auth">SIG-AUTH</h3>

<ol>
<li><p>Provide RunAsGroup feature for Containers in a Pod(Alpha):</p>

<p>目前，我们可以控制容器的UID同时也可以补充组，但是，无法控制那些运行的容器的GID总是0（root）。这个功能能够以非root运行容器，并且帮助我们提高容器的安全级别。(<a href="https://github.com/kubernetes/features/issues/213">https://github.com/kubernetes/features/issues/213</a>)</p></li>

<li><p>TokenRequest API and Kubelet integration(Alpha):<br />
TokenRequest API 提供了对service account token基础的改进。这个功能将允许生成Secret APIs之外、针对特定对象的（例如external secretstores）、可以调整有效期的以及可以绑定于特定Pods 的tokens(<a href="https://github.com/kubernetes/features/issues/542">https://github.com/kubernetes/features/issues/542</a>)</p></li>

<li><p>Limit node access to API(beta):</p>

<p>可以对节点上的API调用进行限制，限制节点的作用域，比如：只能自己节点上的Node API Objects，只能修改绑定在自己节点上的Pod Objects，只能获取自己节点上的secrets和configmaps 等。(<a href="https://github.com/kubernetes/features/issues/279">https://github.com/kubernetes/features/issues/279</a>)</p></li>

<li><p>ClusterRole Aggregation（Stable）:</p>

<p>为了CustomResources and ExtensionAPIServers更容易支持 RBAC，我们可以让API extenders为其添加一些权限，比如修改，查看等。(<a href="https://github.com/kubernetes/features/issues/502">https://github.com/kubernetes/features/issues/502</a>)</p></li>
</ol>

<h3 id="sig-api-machinery">SIG-API-MACHINERY</h3>

<ol>
<li>Custom Resource Definition Versioning(beta):</li>
</ol>

<p>CRD因其提高了Kubernetes的扩展性而被广泛接纳，为CRD增加版本，这样CRD的作者可以定义不同的资源版本，同时添加一种版本间的转换机制。目前，每个CR只有一个版本，每当更新的时候，只能重新增加一个CRD，然后以手动的方式同步。
*   支持API级别的版本控制
*   支持不同版本之间的转换
*   支持修改版本库
*   所有版本支持Validation/OpenAPI 模式
(<a href="https://github.com/kubernetes/features/issues/544">https://github.com/kubernetes/features/issues/544</a>)</p>

<ol>
<li>Subresources for Custom Resources(beta):</li>
</ol>

<p>为CR引入子资源(Subresources)。而此前CR并不支持子资源。在本版本中添加了/status 和 /scale子资源。
* 支持独立的Status和Spec定义
    * 在resource主访问点上忽略Status字段的变化
    * 提供/status路径提供对Status字段变化的访问
    * metadata.Generation只有当Spec字段改变的时候才递增
* 支持/scale子资源
* 保持向后兼容性
* 如果一个CR早已构建了Spec或Status，它可以轻而易举地转为使用/status和/scale进行访问
* 与JSON Schema Validation无缝对接
    (<a href="https://github.com/kubernetes/features/issues/571">https://github.com/kubernetes/features/issues/571</a>)</p>

<h3 id="sig-azure">SIG-AZURE</h3>

<ol>
<li><p>Add support for Azure Virtual Machine Scale Sets（Beta）:</p>

<p>添加对VMSS的支持，具体包含同时支持VMSS和标准VM，LB规则、探测以及PVs
(<a href="https://github.com/kubernetes/features/issues/513">https://github.com/kubernetes/features/issues/513</a>)</p></li>

<li><p>Add Azure support to cluster-autoscaler (Beta):</p>

<p>在弹性伸缩中，支持Azure VMSS，支持Availability Set，支持节点组以及相关单元测试和文档 (<a href="https://github.com/kubernetes/features/issues/514">https://github.com/kubernetes/features/issues/514</a>)</p></li>
</ol>

<h3 id="sig-cloud-provider">SIG-cloud-provider</h3>

<ol>
<li><p>Support out-of-process and out-of-tree cloud providers(Beta):</p>

<p>支持可插拔云提供商。云服务商的依赖需要从Kubernetes二进制中移除。(<a href="https://github.com/kubernetes/features/issues/88">https://github.com/kubernetes/features/issues/88</a>)</p></li>
</ol>

<h3 id="sig-network">SIG-NETWORK</h3>

<ol>
<li><p>Implement IPVS-based in-cluster service load balancing(Stable):</p>

<p>实现基于IPVS的负载均衡,IPVS基于内核哈希表,与iptables相比,IPVS还支持更复杂的负载均衡算法（最少负载，最少连接数，局部性，加权）以及其他有用的功能（例如健康检查，重试等）。(<a href="https://github.com/kubernetes/features/issues/265">https://github.com/kubernetes/features/issues/265</a>)</p></li>

<li><p>Enable CoreDNS as a DNS plugin for Kubernetes(Stable):</p>

<p>启用CoreDns作为k8s的DNS插件。(<a href="https://github.com/kubernetes/features/issues/427">https://github.com/kubernetes/features/issues/427</a>)</p></li>
</ol>

<h3 id="sig-node">SIG-NODE</h3>

<ol>
<li><p>Dynamic Kubelet Configuration（Beta）:</p>

<p>目前，Kubelet通过命令行的标志位来设置，这意味着需要改变启动配置，同时还有可能修改了之前的配置，这非常繁琐也可能容易出错，所以，支持动态修改配置能够减轻这些负担。(<a href="https://github.com/kubernetes/features/issues/281">https://github.com/kubernetes/features/issues/281</a>)</p></li>

<li><p>Add sysctl support（Beta）:</p>

<p>扩展pod的规范，让pod能够支持内核上的命名空间隔离。(<a href="https://github.com/kubernetes/features/issues/34">https://github.com/kubernetes/features/issues/34</a>)</p></li>

<li><p>Add support for Windows Container Configuration in CRI（Beta）:</p>

<p>在CRI上添加对Windows容器配置的支持,例如：cpu/memory限制。(<a href="https://github.com/kubernetes/features/issues/547">https://github.com/kubernetes/features/issues/547</a>)</p></li>

<li><p>CRI: logging, stats, and more（Beta）:</p>

<p>实现CRI容器日志的滚动收集；添加对CRI容器的统计功能。(<a href="https://github.com/kubernetes/features/issues/552">https://github.com/kubernetes/features/issues/552</a>)</p></li>

<li><p>CRI validation test suite（Stable）:</p>

<p>为Kubelet CRI提供通用的验证/测试工具。(<a href="https://github.com/kubernetes/features/issues/292">https://github.com/kubernetes/features/issues/292</a>)</p></li>
</ol>

<h3 id="sig-scheduling">SIG-scheduling</h3>

<ol>
<li><p>Schedule DaemonSet Pods by kube-scheduler(Alpha):</p>

<p>目前DaemonSet由DaemonSet Controller完成调度，这会带来一些问题，比如DaemonSet调度时会忽略节点资源变化、DaemonSet忽略Pod亲和性和反亲和性的影响、以及对DaemonSet调度故障的排查等。使用通用的调度器要比使用自己的调度器更好。
(<a href="https://github.com/kubernetes/features/issues/548">https://github.com/kubernetes/features/issues/548</a>)</p></li>

<li><p>Add pod priority and preemption(Beta):</p>

<p>运行拥有不同优先级的工作负载通常可以提高中大型集群资源使用率。工作负载的重要程度可以由优先级、QoS或者其他集群特定的指标进行评估。超卖在云上很常见，为Pod添加优先级和抢占式策略，一些Pod承担的工作可能重要一些，一些Pod承担的工作可能要次要一些，在集群中，可能会遇到资源紧张或者某pod优先使用某资源的情况，我们可以通过设置pod的优先级或者抢占方式来保证承担重要任务的pod能够工作。(<a href="https://github.com/kubernetes/features/issues/564">https://github.com/kubernetes/features/issues/564</a>)</p></li>
</ol>

<h3 id="sig-storage">SIG-storage</h3>

<ol>
<li><p>Add support for online resizing of PVs（Alpha）:</p>

<p>支持在线（正在被Pod使用的PV）调整pv的大小。(<a href="https://github.com/kubernetes/features/issues/531">https://github.com/kubernetes/features/issues/531</a>)</p></li>

<li><p>Dynamic Maximum volume count（Alpha）:</p>

<p>支持动态统计node上的挂载数量，可以避免挂载到已经挂满的节点上。(<a href="https://github.com/kubernetes/features/issues/554">https://github.com/kubernetes/features/issues/554</a>)</p></li>

<li><p>Provide environment variables expansion in sub path mount(Alpha):
当挂载卷的时候根据环境变量动态生成宿主目录，subPath可以按需创建目录，但分配给这些目录的名字是静态的。
(<a href="https://github.com/kubernetes/features/issues/559">https://github.com/kubernetes/features/issues/559</a>)</p></li>

<li><p>Add support for resizing PVs(Beta):</p>

<p>支持调整 PVs (<a href="https://github.com/kubernetes/features/issues/284">https://github.com/kubernetes/features/issues/284</a>)</p></li>

<li><p>StorageObjectInUseProtection (Stable):</p>

<p>提供PVC保护，如果PVC在Pod的使用的情况下，不允许删除PVC。(<a href="https://github.com/kubernetes/features/issues/498">https://github.com/kubernetes/features/issues/498</a>)</p></li>

<li><p>StorageObjectInUseProtection(Stable):
提供PV保护，在有Pod使用该PV的情况下,PV不允许被删除。(<a href="https://github.com/kubernetes/features/issues/499">https://github.com/kubernetes/features/issues/499</a>)</p></li>
</ol>
]]></content>
		</item>
		
		<item>
			<title>安装Weave Scope</title>
			<link>https://yulibaozi.com/posts/kubernetes/deploy/2018-05-16-deploy-weavescope/</link>
			<pubDate>Wed, 16 May 2018 19:58:52 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/kubernetes/deploy/2018-05-16-deploy-weavescope/</guid>
			<description>#### 获取k8s版本并使用base64编码   kubectl apply &amp;ndash;namespace kube-system -f &amp;ldquo;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 |tr -d &amp;lsquo;\n&amp;rsquo;)&amp;rdquo;
 如果出现
error: the namespace from the provided object &amp;quot;weave&amp;quot; does not match the namespace &amp;quot;kube-system&amp;quot;. You must pass &#39;--namespace=weave&#39; to perform this operation.  执行 &amp;gt; kubectl apply &amp;ndash;namespace=weave -f &amp;ldquo;https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl version | base64 |tr -d &amp;lsquo;\n&amp;rsquo;)&amp;rdquo;
查看是否部署成功  kubectl get &amp;ndash;namespace=weave daemonset weave-scope-agent
 NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE weave-scope-agent 3 3 2 3 0 &amp;lt;none&amp;gt; 4m   kubectl get &amp;ndash;namespace=weave deployment weave-scope-app</description>
			<content type="html"><![CDATA[

<pre><code>#### 获取k8s版本并使用base64编码
</code></pre>

<blockquote>
<p>kubectl apply &ndash;namespace kube-system -f &ldquo;<a href="https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl">https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl</a> version | base64 |tr -d &lsquo;\n&rsquo;)&rdquo;</p>
</blockquote>

<p>如果出现</p>

<pre><code>error: the namespace from the provided object &quot;weave&quot; does not match the namespace &quot;kube-system&quot;. You must pass '--namespace=weave' to perform this operation.
</code></pre>

<p>执行
&gt; kubectl apply &ndash;namespace=weave -f &ldquo;<a href="https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl">https://cloud.weave.works/k8s/scope.yaml?k8s-version=$(kubectl</a> version | base64 |tr -d &lsquo;\n&rsquo;)&rdquo;</p>

<h4 id="查看是否部署成功">查看是否部署成功</h4>

<blockquote>
<p>kubectl get &ndash;namespace=weave daemonset weave-scope-agent</p>
</blockquote>

<pre><code>NAME                DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
weave-scope-agent   3         3         2         3            0           &lt;none&gt;          4m
</code></pre>

<blockquote>
<p>kubectl  get &ndash;namespace=weave deployment weave-scope-app</p>
</blockquote>

<pre><code>NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
weave-scope-app   1         1         1            1           1m

</code></pre>

<blockquote>
<p>kubectl get &ndash;namespace=weave svc weave-scope-app</p>
</blockquote>

<pre><code>NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
weave-scope-app   ClusterIP   xxxx   &lt;none&gt;        80/TCP    7m
</code></pre>

<blockquote>
<p>kubectl get &ndash;namespace=weave pod | grep weave</p>
</blockquote>

<pre><code>weave-scope-agent-brltx            1/1       Running   0          5m
weave-scope-agent-cdxn7            1/1       Running   0          5m
weave-scope-agent-rw5hg            0/1       Pending   0          5m
weave-scope-app-57566f67f6-2dnzf   1/1       Running   0          5m
</code></pre>

<h4 id="然后修改为外部可以访问并暴露端口">然后修改为外部可以访问并暴露端口</h4>

<blockquote>
<p>kubectl &ndash;namespace=weave edit svc weave-scope-app</p>
</blockquote>

<pre><code>type:NodePort
</code></pre>

<h4 id="查看暴露的端口">查看暴露的端口</h4>

<blockquote>
<p>kubectl &ndash;namespace=weave get svc weave-scope-app</p>
</blockquote>

<h4 id="打开浏览器你会看到精美的界面">打开浏览器你会看到精美的界面</h4>

<p><img src="http://index.yulibaozi.com/1d55eb89-780b-43d2-9bdc-8761ddcaac17weave.png" alt="image" /></p>
]]></content>
		</item>
		
		<item>
			<title>进程间通信的6种方式</title>
			<link>https://yulibaozi.com/posts/system/2018-02-24-interprocess-communication/</link>
			<pubDate>Sat, 21 Apr 2018 20:11:21 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/system/2018-02-24-interprocess-communication/</guid>
			<description>第一种： 无名管道( 管道pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 高级管道(popen)：将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程，这种方式我们成为高级管道方式。
第二种：消息队列 消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。
第三种：信号量 信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
第四种：信号 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。
第五种：共享内存 共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。
第六种：套接字 套接字( socket ) ： 套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。
进程间通信的方式中访问速度最快的是:共享内存</description>
			<content type="html"><![CDATA[<p>第一种：
无名管道( 管道pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。
有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。
高级管道(popen)：将另一个程序当做一个新的进程在当前程序进程中启动，则它算是当前程序的子进程，这种方式我们成为高级管道方式。</p>

<p>第二种：消息队列
消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。</p>

<p>第三种：信号量
信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。</p>

<p>第四种：信号
信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。</p>

<p>第五种：共享内存
共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。</p>

<p>第六种：套接字
套接字( socket ) ： 套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。</p>

<p><strong>进程间通信的方式中访问速度最快的是:共享内存</strong></p>
]]></content>
		</item>
		
		<item>
			<title>[黑魔法] Go 获取 gid 源码详解</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-04-18-get-gid/</link>
			<pubDate>Wed, 18 Apr 2018 22:11:01 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-04-18-get-gid/</guid>
			<description>1. 应不应该拿到goroutine 怎么获取goid？  原则下是不能获取的。
 原因一:官方担心滥用GID实现goroutine local storage(类似java的&amp;rdquo;thread-local&amp;rdquo; storage)， 因为goroutine local storage很难进行垃圾回收。
Please don&amp;rsquo;t use goroutine local storage. It&amp;rsquo;s highly discouraged. In fact, IIRC, we used to expose Goid, but it is hidden since we don&amp;rsquo;t want people to do this. Potential problems include:
 when goroutine goes away, its goroutine local storage won&amp;rsquo;t be GCed. (you can get goid for the current goroutine, but you can&amp;rsquo;t get a list of all running goroutines)</description>
			<content type="html"><![CDATA[

<h4 id="1-应不应该拿到goroutine-怎么获取goid">1. 应不应该拿到goroutine 怎么获取goid？</h4>

<blockquote>
<p>原则下是不能获取的。</p>
</blockquote>

<p>原因一:官方担心滥用GID实现goroutine local storage(类似java的&rdquo;thread-local&rdquo; storage)， 因为goroutine local storage很难进行垃圾回收。</p>

<p>Please don&rsquo;t use goroutine local storage. It&rsquo;s highly discouraged. In fact, IIRC, we used to
expose Goid, but it is hidden since we don&rsquo;t want people to do this.
Potential problems include:</p>

<ol>
<li><p>when goroutine goes away, its goroutine local storage won&rsquo;t be GCed. (you can get goid for
the current goroutine, but you can&rsquo;t get a list of all running goroutines)</p></li>

<li><p>what if handler spawns goroutine itself? the new goroutine suddenly loses access to your
goroutine local storage. You can guarantee that your own code won&rsquo;t spawn other goroutines,
but in general you can&rsquo;t make sure the standard library or any 3rd party code won&rsquo;t do that.
thread local storage is invented to help reuse bad/legacy code that assumes global state, Go
doesn&rsquo;t have legacy code like that, and you really should design your code so that state is passed
explicitly and not as global (e.g. resort to goroutine local storage)</p></li>
</ol>

<p>原因二:拿GOID会使用runtime.Stack()来获取当前go程的调用栈踪迹。而这个又是消耗性能的,如果在debug下,我们可以用来追查异常是可以的。</p>

<blockquote>
<p>如何调用</p>
</blockquote>

<pre><code>主要代码:
    var buf [64]byte
	n := runtime.Stack(buf[:], false) //这个时候GID已经在buf里面了
	idField := strings.Fields(strings.TrimPrefix(string(buf[:n]), &quot;goroutine &quot;))[0]
	id, err := strconv.Atoi(idField) //此时的id就是GOID

所有代码:

func GoID() int {
	var buf [64]byte
	n := runtime.Stack(buf[:], false)
	idField := strings.Fields(strings.TrimPrefix(string(buf[:n]), &quot;goroutine &quot;))[0]
	id, err := strconv.Atoi(idField)
	if err != nil {
		panic(fmt.Sprintf(&quot;cannot get goroutine id: %v&quot;, err))
	}
	return id
}
func main() {
	fmt.Println(&quot;main&quot;, GoID())
	var wg sync.WaitGroup
	for i := 0; i &lt; 5; i++ {
		i := i
		wg.Add(1)
		go func() {
			defer wg.Done()
			fmt.Println(i, GoID())
		}()
	}
	wg.Wait()
}
</code></pre>

<p>其中buf里面的数据是这个样子:</p>

<pre><code>goroutine 1 [running]: 
main.GoID(0xc420041f30)
        /Users/yulibaozi
main 1
使用strings字符分离&lt;goroutine  &gt;就能获取到Goid
</code></pre>

<blockquote>
<p>深度解析背后的原理</p>
</blockquote>

<pre><code>func Stack(buf []byte, all bool) int {
	if all { //如果需要需要获取这一刻所有go程的信息,暂停所有操作,保留现场
		stopTheWorld(&quot;stack trace&quot;)
	}

	n := 0
	if len(buf) &gt; 0 {
		gp := getg()                            //每个 goroutine 都有一个自己的结构体 g
		sp := getcallersp(unsafe.Pointer(&amp;buf)) //当前的堆栈寄存器SP
		pc := getcallerpc()                     //返回当前当前go层的程序计数器
		systemstack(func() {                    //在系统堆栈中获得详细的堆栈信息
			g0 := getg()
			// Force traceback=1 to override GOTRACEBACK setting,
			// so that Stack's results are consistent.
			// GOTRACEBACK is only about crash dumps.
			g0.m.traceback = 1
			g0.writebuf = buf[0:0:len(buf)]
			goroutineheader(gp)
			traceback(pc, sp, 0, gp) //获取当前 goroutine 的堆栈信息
			if all {
				tracebackothers(gp) //获取其他 goroutine 的堆栈信息
			}
			g0.m.traceback = 0
			n = len(g0.writebuf)
			g0.writebuf = nil
		})
	}

	if all { //恢复如初,正常执行
		startTheWorld()
	}
	return n
}
</code></pre>

<p>参数说明:
1. buf 保存信息的结构体
2. all是否获取这一刻所有go程的信息。
3. 返回值int表示数据的大小</p>

<blockquote>
<p>参考资料</p>
</blockquote>

<p><a href="https://groups.google.com/forum/#!topic/golang-nuts/Nt0hVV_nqHE">Google论坛</a>
<a href="http://colobu.com/2016/04/01/how-to-get-goroutine-id/">鸟窝博客</a></p>
]]></content>
		</item>
		
		<item>
			<title>Go性能测试手册</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-04-12-go-pprof/</link>
			<pubDate>Thu, 12 Apr 2018 22:23:36 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-04-12-go-pprof/</guid>
			<description> Go性能测试总结 参考地址  go-torch Go代码调优利器-火焰图 不能使用的解决方案  使用命令 1. hey -n 50000 -c 1000 http://127.0.0.1:8003/v1/report/pc/hotreport   在1的同时使用:
go-torch -u http://127.0.0.1:8003 -t 30  3.自动生成了火焰图,然后去$PATH找torch.svg文件，我的在 /usr/local/bin下面
  4.使用beego的路由时，需要添加默认路由
 beego.Handler(&amp;quot;/debug/pprof/&amp;quot;, http.HandlerFunc(pprof.Index)) beego.Handler(&amp;quot;/debug/pprof/cmdline&amp;quot;, http.HandlerFunc(pprof.Cmdline)) beego.Handler(&amp;quot;/debug/pprof/profile&amp;quot;, http.HandlerFunc(pprof.Profile)) beego.Handler(&amp;quot;/debug/pprof/symbol&amp;quot;, http.HandlerFunc(pprof.Symbol)) beego.Handler(&amp;quot;/debug/pprof/trace&amp;quot;, http.HandlerFunc(pprof.Trace))   接下来是分析   </description>
			<content type="html"><![CDATA[

<h2 id="go性能测试总结">Go性能测试总结</h2>

<h3 id="参考地址">参考地址</h3>

<ol>
<li><a href="https://github.com/uber/go-torch">go-torch</a></li>
<li><a href="http://lihaoquan.me/2017/1/1/Profiling-and-Optimizing-Go-using-go-torch.html">Go代码调优利器-火焰图</a></li>
<li><a href="https://stackoverflow.com/questions/30560859/cant-use-go-tool-pprof-with-an-existing-server">不能使用的解决方案</a></li>
</ol>

<h3 id="使用命令">使用命令</h3>

<pre><code>1. hey  -n 50000 -c 1000 http://127.0.0.1:8003/v1/report/pc/hotreport
</code></pre>

<ol>
<li><p>在1的同时使用:</p>

<pre><code>go-torch -u http://127.0.0.1:8003 -t 30
</code></pre>

<p>3.自动生成了火焰图,然后去$PATH找torch.svg文件，我的在 /usr/local/bin下面</p></li>
</ol>

<p>4.使用beego的路由时，需要添加默认路由</p>

<pre><code>    beego.Handler(&quot;/debug/pprof/&quot;, http.HandlerFunc(pprof.Index))
	beego.Handler(&quot;/debug/pprof/cmdline&quot;, http.HandlerFunc(pprof.Cmdline))
	beego.Handler(&quot;/debug/pprof/profile&quot;, http.HandlerFunc(pprof.Profile))
	beego.Handler(&quot;/debug/pprof/symbol&quot;, http.HandlerFunc(pprof.Symbol))
	beego.Handler(&quot;/debug/pprof/trace&quot;, http.HandlerFunc(pprof.Trace))
</code></pre>

<ol>
<li>接下来是分析

<br /></li>
</ol>
]]></content>
		</item>
		
		<item>
			<title>Golang Channel源码分析</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-04-12-go-and-channel/</link>
			<pubDate>Thu, 12 Apr 2018 22:22:05 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-04-12-go-and-channel/</guid>
			<description>Channel的数据结构及解释 代码地址:runtime/chan.go
基本结构
type hchan struct { qcount uint // 在队列中的数据总个数 dataqsiz uint // 当前channel的容量 buf unsafe.Pointer //指向存放数据的环形数组 elemsize uint16 // Channel中数据类型的大小 closed uint32 //这个Channel是否被关闭 elemtype *_type // 元素的数据类型 sendx uint // send index recvx uint // receive index recvq waitq // 存放阻塞在&amp;lt;- ch动作的go程队列 sendq waitq // 存放阻塞在ch &amp;lt;-动作的go程队列 lock mutex // }  waitq :内部实现是一个队列
type waitq struct { first *sudog last *sudog }  sudog
type sudog struct { // The following fields are protected by the hchan.</description>
			<content type="html"><![CDATA[

<h3 id="channel的数据结构及解释">Channel的数据结构及解释</h3>

<p>代码地址:runtime/chan.go</p>

<p>基本结构</p>

<pre><code>type hchan struct {
	qcount   uint          // 在队列中的数据总个数
	dataqsiz uint          // 当前channel的容量
	buf      unsafe.Pointer  //指向存放数据的环形数组
	elemsize uint16  // Channel中数据类型的大小
	closed   uint32 //这个Channel是否被关闭
	elemtype *_type // 元素的数据类型
	sendx    uint   // send index
	recvx    uint   // receive index
	recvq    waitq  // 存放阻塞在&lt;- ch动作的go程队列
	sendq    waitq  // 存放阻塞在ch &lt;-动作的go程队列
	lock mutex //
}

</code></pre>

<p>waitq :内部实现是一个队列</p>

<pre><code>type waitq struct {
	first *sudog
	last  *sudog
}
</code></pre>

<p>sudog</p>

<pre><code>type sudog struct {
	// The following fields are protected by the hchan.lock of the
	// channel this sudog is blocking on. shrinkstack depends on
	// this for sudogs involved in channel ops.

	g *g

	// isSelect indicates g is participating in a select, so
	// g.selectDone must be CAS'd to win the wake-up race.
	isSelect bool
	next     *sudog
	prev     *sudog
	elem     unsafe.Pointer // data element (may point to stack)

	// The following fields are never accessed concurrently.
	// For channels, waitlink is only accessed by g.
	// For semaphores, all fields (including the ones above)
	// are only accessed when holding a semaRoot lock.

	acquiretime int64
	releasetime int64
	ticket      uint32
	parent      *sudog // semaRoot binary tree
	waitlink    *sudog // g.waiting list or semaRoot
	waittail    *sudog // semaRoot
	c           *hchan // channel
}
</code></pre>

<h3 id="创建channel做了什么">创建Channel做了什么?</h3>

<h4 id="源码">源码</h4>

<pre><code>func makechan64(t *chantype, size int64) *hchan {
	if int64(int(size)) != size { // ①
		panic(plainError(&quot;makechan: size out of range&quot;))
	}

	return makechan(t, int(size))
}

func makechan(t *chantype, size int) *hchan {
	elem := t.elem

	// compiler checks this but be safe.
	if elem.size &gt;= 1&lt;&lt;16 { // ②
		throw(&quot;makechan: invalid channel element type&quot;)
	}
	if hchanSize%maxAlign != 0 || elem.align &gt; maxAlign { // ③
		throw(&quot;makechan: bad alignment&quot;)
	}

	if size &lt; 0 || uintptr(size) &gt; maxSliceCap(elem.size) || uintptr(size)*elem.size &gt; _MaxMem-hchanSize { //④
		panic(plainError(&quot;makechan: size out of range&quot;))
	}

	// Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers.
	// buf points into the same allocation, elemtype is persistent.
	// SudoG's are referenced from their owning thread so they can't be collected.
	// TODO(dvyukov,rlh): Rethink when collector can move allocated objects.
	var c *hchan
	switch {
	case size == 0 || elem.size == 0: //⑤
		// Queue or element size is zero.
		c = (*hchan)(mallocgc(hchanSize, nil, true))
		// Race detector uses this location for synchronization.
		c.buf = unsafe.Pointer(c)
	case elem.kind&amp;kindNoPointers != 0: //⑥
		// Elements do not contain pointers.
		// Allocate hchan and buf in one call.
		c = (*hchan)(mallocgc(hchanSize+uintptr(size)*elem.size, nil, true))
		c.buf = add(unsafe.Pointer(c), hchanSize)
	default: //⑦
		// Elements contain pointers.
		c = new(hchan)
		c.buf = mallocgc(uintptr(size)*elem.size, elem, true)
	}

	c.elemsize = uint16(elem.size)
	c.elemtype = elem
	c.dataqsiz = uint(size)

	if debugChan {
		print(&quot;makechan: chan=&quot;, c, &quot;; elemsize=&quot;, elem.size, &quot;; elemalg=&quot;, elem.alg, &quot;; dataqsiz=&quot;, size, &quot;\n&quot;)
	}
	return c
}
</code></pre>

<h4 id="理解">理解</h4>

<p>①:判断size是否超出上下边界</p>

<p>②:判断元素类型大小是否安全</p>

<p>③:判断对齐限制</p>

<p>④:判断size是否过小(小于0),是否过大(大于系统为堆arena分配的最大值-1)</p>

<p>⑤:第一个switch:如果是一个非缓冲通道,不分配空间</p>

<p>⑥:当类型不包含在指针的时候,那么分配在连续的内存空间上</p>

<p>⑦:否则就分配一个指定大小的对象</p>

<p>最后返回这个*chan</p>

<blockquote>
<p><strong>值得注意的是:如何分配对象的?</strong></p>
</blockquote>

<pre><code>// Allocate an object of size bytes.
// Small objects are allocated from the per-P cache's free lists.
// Large objects (&gt; 32 kB) are allocated straight from the heap.
func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {
</code></pre>

<p>简单的说就是:
1. 当是一个小对象(&lt;=32KB)的时候就从 per-P的空闲列表上分配
2. 如果是个大对象(&gt;32KB)的时候就在堆上直接分配。</p>

<h3 id="我们在操作-ch-时做了什么-写入">我们在操作 ch&lt;-时做了什么(写入)？</h3>

<blockquote>
<p>源码:</p>
</blockquote>

<pre><code>func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool {
	if c == nil {
		if !block {
			return false
		}
		gopark(nil, nil, &quot;chan send (nil chan)&quot;, traceEvGoStop, 2)
		throw(&quot;unreachable&quot;)
	}

	if debugChan {
		print(&quot;chansend: chan=&quot;, c, &quot;\n&quot;)
	}

	if !block &amp;&amp; c.closed == 0 &amp;&amp; ((c.dataqsiz == 0 &amp;&amp; c.recvq.first == nil) ||
		(c.dataqsiz &gt; 0 &amp;&amp; c.qcount == c.dataqsiz)) {
		return false
	}

	var t0 int64
	if blockprofilerate &gt; 0 {
		t0 = cputicks()
	}

	lock(&amp;c.lock)

	if c.closed != 0 {
		unlock(&amp;c.lock)
		panic(plainError(&quot;send on closed channel&quot;))
	}
.....
</code></pre>

<blockquote>
<p>理解</p>
</blockquote>

<ol>
<li><p>判断了是否是nil channel</p></li>

<li><p>是否是否缓存通道但是数据已经满,</p></li>

<li><p>是否是非缓冲通道但是头结点为nil;</p></li>

<li><p>当channel为nil的情况时，gopark会将当前goroutine休眠，然后通过unlockf来唤醒，这时传入的unlockf是nil，也就是向nil channel发送数据的goroutine会一直休眠，Golang中会一直检测运行情况，遇到这样的情况就会报出throw (&ldquo;all goroutines are asleep - deadlock!&rdquo;) 的异常。</p></li>

<li><p>再判断当前的channel是否关闭了，如果是已经关闭了的，则直接panic：panic(plainError(&ldquo;send on closed channel&rdquo;))。</p></li>
</ol>

<p>正式写的时候分为带缓冲和不带缓冲的channel，二者主要区别在于什么时候阻塞读写，总的来说：</p>

<pre><code>当goroutine阻塞在channel上。如果hchan.buf为空，从当前channel的等待队列中取出等待的goroutine，然后调用send方法.

如果hchan.buf还有可用空间，将数据放到buffer里面；

如果hchan.buf已满，则阻塞当前channel。
</code></pre>

<h3 id="我们在操作-ch-时做了什么-读取">我们在操作 &lt;-ch 时做了什么(读取)？</h3>

<p>读取和写入基本一致,区别是写入是对recvq队列操作,读取是对send队列操作。</p>

<h3 id="关闭通道时">关闭通道时</h3>

<p>源码</p>

<pre><code>func closechan(c *hchan) {
	if c == nil {
		panic(plainError(&quot;close of nil channel&quot;))
	}

	lock(&amp;c.lock)
	if c.closed != 0 {
		unlock(&amp;c.lock)
		panic(plainError(&quot;close of closed channel&quot;))
	}

    (取消掉冗余代码)...

	c.closed = 1

	var glist *g

	// release all readers
	for {
		sg := c.recvq.dequeue()
		if sg == nil {
			break
		}
		if sg.elem != nil {
			typedmemclr(c.elemtype, sg.elem)
			sg.elem = nil
		}
		if sg.releasetime != 0 {
			sg.releasetime = cputicks()
		}
		gp := sg.g
		gp.param = nil
		if raceenabled {
			raceacquireg(gp, unsafe.Pointer(c))
		}
		gp.schedlink.set(glist)
		glist = gp
	}

	// release all writers (they will panic)
	for {
		sg := c.sendq.dequeue()
		if sg == nil {
			break
		}
		sg.elem = nil
		if sg.releasetime != 0 {
			sg.releasetime = cputicks()
		}
		gp := sg.g
		gp.param = nil
		if raceenabled {
			raceacquireg(gp, unsafe.Pointer(c))
		}
		gp.schedlink.set(glist)
		glist = gp
	}
	unlock(&amp;c.lock)

	// Ready all Gs now that we've dropped the channel lock.
	for glist != nil {
		gp := glist
		glist = glist.schedlink.ptr()
		gp.schedlink = 0
		goready(gp, 3)
	}
}
</code></pre>

<p>理解</p>

<pre><code>如果通道已经被关闭再次关闭会panic

清空阻塞在recvq队列中的goroutine;

清空阻塞在sendq队列中的gorountine;
</code></pre>

<p>实现方式是:以for循环的方式遍历这上述两种队列,然后把所有的gorountine放在glist里面，然后调用gready唤醒这里面的gorountine。</p>

<h3 id="总结">总结</h3>

<p>channel其实是一个加锁的队列.使用锁保护好hchan中的数据同步,这个锁比较轻量级。其中 recvq 是读操作阻塞在 channel 的 goroutine 列表，sendq 是写操作阻塞在 channel 的 goroutine 列表。列表的实现是 sudog，其实就是一个对 g 的结构的封装。</p>

<blockquote>
<p>关于锁:</p>
</blockquote>

<p>锁的实现是:先自选,超时使用型号量。因为锁的内部实现是自旋锁,如果超时就使用信号量,自旋锁之所以轻量级是因为他在短时间空转并不会让出时间片,所以不会引起上下文切换。
参考及推荐链接:</p>

<p><a href="http://legendtkl.com/2017/08/06/golang-channel-implement/">Go Channel 源码剖析</a></p>

<p><a href="http://www.opscoder.info/golang_channel.html">深入Golang之channel</a></p>

<p>Golang版本</p>

<pre><code>➜  ~ go version
go version go1.10 darwin/amd64
</code></pre>
]]></content>
		</item>
		
		<item>
			<title>Golang的websocket开发与实践</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-03-26-go-and-websocket/</link>
			<pubDate>Mon, 26 Mar 2018 22:16:12 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-03-26-go-and-websocket/</guid>
			<description>进入正题(以一个聊天室入手,层层剖析):
#### websocket核心要义： 浏览器通过js向服务器端发起建立websocket连接,连接建立以后，客户端和服务器端就可以通过 TCP 连接直接交换数据。因为WebSocket连接本质上就是一个 TCP 连接，也就是说首次建立链接后,后续就直接交换数据。
#### 是如何建立websocket链接的呢? 请求的时候他仍然是个普通请求,比如GET,在服务端的接口中把这个请求升级成websocket; &amp;gt; 具体的代码:
 var upgrader = websocket.Upgrader{ ReadBufferSize: 1024, WriteBufferSize: 1024, } 控制器中: conn, err := upgrader.Upgrade(base.Ctx.ResponseWriter, base.Ctx.Request, nil) if _, ok := err.(websocket.HandshakeError); ok { http.Error(base.Ctx.ResponseWriter, &amp;quot;不是合法的WebSocket请求&amp;quot;, 400) return }  这样就把这个链接升级成了websocket。这个就是websocket的核心代码之一。
#### 聊天室的结构设计:聊天室结构详解:
 Room:
 某一个单独的聊天室,也就是某一个房间,创建一个房间的时候,需要为这个房间开个go程监听这个房间的一切事情,比如某人进入了房间,某人离开了房间,某人说了些什么,我们称之为营业,这是必须的。
 Client:
 相对于Room来说,Client就是用户的角色。当用户进入到某个房间称之为注册,当然用户得能说会道。所以,Client得有两个功能:监听房间里面的消息,能在房间里面说话,所以,我们考虑开两个Go程读取和发送消息是合理的。
 用户和Client的真实关系。
 相对于服务端来说,Client的角色就是用户,其实不然,真实用户和Client之间其实还有个中间介质,那就是websocket,真实用户通过websocket发送消息到Client,Client接收房间里面然后通过websocket发送给真实用户。Client是用户在服务端的一个代理身份,如果理清了这个关系,将极大的帮助你理解代码。
 聊天室的消息广播是怎么回事?
 其实,这个东西非常简单,不要想得那么复杂。 1. 如果是个演示项目的话: 其实是循环向每个客户端发送这条消息,也就是代码里面的for,看完你将哑然失笑。 2. 如果你想专业一点的话,你可以使用消息队列,把用户发送的这条消息写入到消息队列(比如nats等)中,然后所有订阅此topic的客户端都将收到这条消息。
HTML5中Wesocket的使用有哪些?  onopen() 与服务端建立链接 onclose() 关闭与服务端的链接 onerror() 突发错误。 onmessage() 接收来自服务端的消息。 onsend() 向服务端发送消息。  展示聊天室代码和详细注释: yulibaozi/chatroom</description>
			<content type="html"><![CDATA[

<p>进入正题(以一个聊天室入手,层层剖析):</p>

<p>#### websocket核心要义：
 浏览器通过js向服务器端发起建立websocket连接,连接建立以后，客户端和服务器端就可以通过 TCP 连接直接交换数据。因为WebSocket连接本质上就是一个 TCP 连接，也就是说首次建立链接后,后续就直接交换数据。</p>

<p>#### 是如何建立websocket链接的呢?
 请求的时候他仍然是个普通请求,比如GET,在服务端的接口中把这个请求升级成websocket;
 &gt; 具体的代码:</p>

<pre><code> var upgrader = websocket.Upgrader{
	ReadBufferSize:  1024,
	WriteBufferSize: 1024,
}

控制器中:

conn, err := upgrader.Upgrade(base.Ctx.ResponseWriter, base.Ctx.Request, nil)
	if _, ok := err.(websocket.HandshakeError); ok {
		http.Error(base.Ctx.ResponseWriter, &quot;不是合法的WebSocket请求&quot;, 400)
		return
	} 
</code></pre>

<p>这样就把这个链接升级成了websocket。这个就是websocket的核心代码之一。</p>

<p>#### 聊天室的结构设计:<img src="http://index.yulibaozi.com/653028c9-6ae4-4907-bbc5-3eb26cdb1e5dQQ20180326-0.jpg" alt="image" /></p>

<p>聊天室结构详解:</p>

<blockquote>
<p>Room:</p>
</blockquote>

<p>某一个单独的聊天室,也就是某一个房间,创建一个房间的时候,需要为这个房间开个go程监听这个房间的一切事情,比如某人进入了房间,某人离开了房间,某人说了些什么,我们称之为营业,这是必须的。</p>

<blockquote>
<p>Client:</p>
</blockquote>

<p>相对于Room来说,Client就是用户的角色。当用户进入到某个房间称之为注册,当然用户得能说会道。所以,Client得有两个功能:监听房间里面的消息,能在房间里面说话,所以,我们考虑开两个Go程读取和发送消息是合理的。</p>

<blockquote>
<p>用户和Client的真实关系。</p>
</blockquote>

<p>相对于服务端来说,Client的角色就是用户,其实不然,真实用户和Client之间其实还有个中间介质,那就是websocket,真实用户通过websocket发送消息到Client,Client接收房间里面然后通过websocket发送给真实用户。Client是用户在服务端的一个代理身份,如果理清了这个关系,将极大的帮助你理解代码。</p>

<blockquote>
<p>聊天室的消息广播是怎么回事?</p>
</blockquote>

<p>其实,这个东西非常简单,不要想得那么复杂。
1. 如果是个演示项目的话:
其实是循环向每个客户端发送这条消息,也就是代码里面的for,看完你将哑然失笑。
2. 如果你想专业一点的话,你可以使用消息队列,把用户发送的这条消息写入到消息队列(比如nats等)中,然后所有订阅此topic的客户端都将收到这条消息。</p>

<h4 id="html5中wesocket的使用有哪些">HTML5中Wesocket的使用有哪些?</h4>

<ol>
<li>onopen() 与服务端建立链接</li>
<li>onclose() 关闭与服务端的链接</li>
<li>onerror() 突发错误。</li>
<li>onmessage() 接收来自服务端的消息。</li>
<li>onsend() 向服务端发送消息。</li>
</ol>

<p>展示聊天室代码和详细注释:
<a href="https://github.com/yulibaozi/chatroom">yulibaozi/chatroom</a></p>

<p>参考文档及项目:</p>

<p><a href="https://github.com/gorilla/websocket">gorilla/websocket</a></p>

<p><a href="https://www.ibm.com/developerworks/cn/web/1112_huangxa_websocket/index.html">使用 HTML5 WebSocket 构建实时 Web 应用</a></p>
]]></content>
		</item>
		
		<item>
			<title>Golang实现二叉查找树</title>
			<link>https://yulibaozi.com/posts/go/algorithm/2018-02-23-binary-tree/</link>
			<pubDate>Fri, 23 Mar 2018 21:00:47 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/algorithm/2018-02-23-binary-tree/</guid>
			<description>为简单起见，键值均为整型。  定义接口(tree.go)：
type Tree interface { Put(k, v int) //新增或修改 	Get(k int) int //查询 	Delete(k int) //删除 	Size() int //树的大小 	Min() int //最小键 	DeleteMin() //删除最小键 } 二叉查找树(binary_tree.go)：
//二叉查找树 type BinaryTree struct { root *node n int } //创建节点 func newNode(k, v int) *node { return &amp;amp;node{k: k, v: v, sz: 1} } //创建二叉查找树 func NewBinaryTree() *BinaryTree { return &amp;amp;BinaryTree{} } //增加或修改 func (bt *BinaryTree) Put(k, v int) { bt.</description>
			<content type="html"><![CDATA[<pre><code>为简单起见，键值均为整型。
</code></pre>

<p>定义接口(tree.go)：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">type</span> <span class="nx">Tree</span> <span class="kd">interface</span> <span class="p">{</span>
	<span class="nf">Put</span><span class="p">(</span><span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="kt">int</span><span class="p">)</span>  <span class="c1">//新增或修改
</span><span class="c1"></span>	<span class="nf">Get</span><span class="p">(</span><span class="nx">k</span> <span class="kt">int</span><span class="p">)</span> <span class="kt">int</span> <span class="c1">//查询
</span><span class="c1"></span>	<span class="nf">Delete</span><span class="p">(</span><span class="nx">k</span> <span class="kt">int</span><span class="p">)</span>  <span class="c1">//删除
</span><span class="c1"></span>	<span class="nf">Size</span><span class="p">()</span> <span class="kt">int</span>     <span class="c1">//树的大小
</span><span class="c1"></span>	<span class="nf">Min</span><span class="p">()</span> <span class="kt">int</span>      <span class="c1">//最小键
</span><span class="c1"></span>	<span class="nf">DeleteMin</span><span class="p">()</span>    <span class="c1">//删除最小键
</span><span class="c1"></span><span class="p">}</span></code></pre></div>
<p>二叉查找树(binary_tree.go)：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="c1">//二叉查找树
</span><span class="c1"></span><span class="kd">type</span> <span class="nx">BinaryTree</span> <span class="kd">struct</span> <span class="p">{</span>
	<span class="nx">root</span> <span class="o">*</span><span class="nx">node</span>
	<span class="nx">n</span>    <span class="kt">int</span>
<span class="p">}</span>
 
<span class="c1">//创建节点
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">newNode</span><span class="p">(</span><span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="kt">int</span><span class="p">)</span> <span class="o">*</span><span class="nx">node</span> <span class="p">{</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="nx">node</span><span class="p">{</span><span class="nx">k</span><span class="p">:</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">:</span> <span class="nx">v</span><span class="p">,</span> <span class="nx">sz</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="p">}</span>
 
<span class="c1">//创建二叉查找树
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">NewBinaryTree</span><span class="p">()</span> <span class="o">*</span><span class="nx">BinaryTree</span> <span class="p">{</span>
	<span class="k">return</span> <span class="o">&amp;</span><span class="nx">BinaryTree</span><span class="p">{}</span>
<span class="p">}</span>
 
<span class="c1">//增加或修改
</span><span class="c1"></span><span class="kd">func</span> <span class="p">(</span><span class="nx">bt</span> <span class="o">*</span><span class="nx">BinaryTree</span><span class="p">)</span> <span class="nf">Put</span><span class="p">(</span><span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="kt">int</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">bt</span><span class="p">.</span><span class="nx">root</span><span class="p">,</span> <span class="nx">_</span> <span class="p">=</span> <span class="nf">put</span><span class="p">(</span><span class="nx">bt</span><span class="p">.</span><span class="nx">root</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">)</span>
<span class="p">}</span>
 
<span class="c1">//查找
</span><span class="c1"></span><span class="kd">func</span> <span class="p">(</span><span class="nx">bt</span> <span class="o">*</span><span class="nx">BinaryTree</span><span class="p">)</span> <span class="nf">Get</span><span class="p">(</span><span class="nx">k</span> <span class="kt">int</span><span class="p">)</span> <span class="kt">int</span> <span class="p">{</span>
	<span class="k">return</span> <span class="nf">get</span><span class="p">(</span><span class="nx">bt</span><span class="p">.</span><span class="nx">root</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
<span class="p">}</span>
 
<span class="c1">//树的大小
</span><span class="c1"></span><span class="kd">func</span> <span class="p">(</span><span class="nx">bt</span> <span class="o">*</span><span class="nx">BinaryTree</span><span class="p">)</span> <span class="nf">Size</span><span class="p">()</span> <span class="kt">int</span> <span class="p">{</span>
	<span class="k">return</span> <span class="nf">size</span><span class="p">(</span><span class="nx">bt</span><span class="p">.</span><span class="nx">root</span><span class="p">)</span>
<span class="p">}</span>
 
<span class="c1">//选出最小键
</span><span class="c1"></span><span class="kd">func</span> <span class="p">(</span><span class="nx">bt</span> <span class="o">*</span><span class="nx">BinaryTree</span><span class="p">)</span> <span class="nf">Min</span><span class="p">()</span> <span class="kt">int</span> <span class="p">{</span>
	<span class="k">return</span> <span class="nf">min</span><span class="p">(</span><span class="nx">bt</span><span class="p">.</span><span class="nx">root</span><span class="p">).</span><span class="nx">k</span>
<span class="p">}</span>
 
<span class="c1">//删除最小键
</span><span class="c1"></span><span class="kd">func</span> <span class="p">(</span><span class="nx">bt</span> <span class="o">*</span><span class="nx">BinaryTree</span><span class="p">)</span> <span class="nf">DeleteMin</span><span class="p">()</span> <span class="p">{</span>
	<span class="nx">bt</span><span class="p">.</span><span class="nx">root</span> <span class="p">=</span> <span class="nf">deleteMin</span><span class="p">(</span><span class="nx">bt</span><span class="p">.</span><span class="nx">root</span><span class="p">)</span>
<span class="p">}</span>
 
<span class="c1">//删除
</span><span class="c1"></span><span class="kd">func</span> <span class="p">(</span><span class="nx">bt</span> <span class="o">*</span><span class="nx">BinaryTree</span><span class="p">)</span> <span class="nf">Delete</span><span class="p">(</span><span class="nx">k</span> <span class="kt">int</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">bt</span><span class="p">.</span><span class="nx">root</span> <span class="p">=</span> <span class="nb">delete</span><span class="p">(</span><span class="nx">bt</span><span class="p">.</span><span class="nx">root</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
<span class="p">}</span></code></pre></div>
<p>节点(node.go)：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="c1">//节点
</span><span class="c1"></span><span class="kd">type</span> <span class="nx">node</span> <span class="kd">struct</span> <span class="p">{</span>
	<span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">,</span> <span class="nx">sz</span>    <span class="kt">int</span>   <span class="c1">//键，值，大小
</span><span class="c1"></span>	<span class="nx">left</span><span class="p">,</span> <span class="nx">right</span> <span class="o">*</span><span class="nx">node</span> <span class="c1">//左右子节点
</span><span class="c1"></span><span class="p">}</span>

<span class="c1">//在以nd为根节点的树下增加或修改一个节点
</span><span class="c1">//如果创建了新节点，第二个参数返回true，
</span><span class="c1">//如果只是修改，第二个参数返回false
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">put</span><span class="p">(</span><span class="nx">nd</span> <span class="o">*</span><span class="nx">node</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span> <span class="kt">int</span><span class="p">)</span> <span class="p">(</span><span class="o">*</span><span class="nx">node</span><span class="p">,</span> <span class="kt">bool</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">nd</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span> <span class="nf">newNode</span><span class="p">(</span><span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">),</span> <span class="kc">true</span>
	<span class="p">}</span>
	<span class="nx">hasNew</span> <span class="o">:=</span> <span class="kc">false</span> <span class="c1">//检测是否创建了新节点
</span><span class="c1"></span>	<span class="k">if</span> <span class="nx">k</span> <span class="p">&lt;</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">k</span> <span class="p">{</span>
		<span class="nx">nd</span><span class="p">.</span><span class="nx">left</span><span class="p">,</span> <span class="nx">hasNew</span> <span class="p">=</span> <span class="nf">put</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">left</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">)</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="nx">k</span> <span class="p">&gt;</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">k</span> <span class="p">{</span>
		<span class="nx">nd</span><span class="p">.</span><span class="nx">right</span><span class="p">,</span> <span class="nx">hasNew</span> <span class="p">=</span> <span class="nf">put</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">right</span><span class="p">,</span> <span class="nx">k</span><span class="p">,</span> <span class="nx">v</span><span class="p">)</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="nx">nd</span><span class="p">.</span><span class="nx">v</span> <span class="p">=</span> <span class="nx">v</span> <span class="c1">//仅修改，不会增加增加节点，就不更新节点大小
</span><span class="c1"></span>	<span class="p">}</span>
	<span class="k">if</span> <span class="nx">hasNew</span> <span class="p">{</span> <span class="c1">//如果创建了新节点就更新树的大小
</span><span class="c1"></span>		<span class="nf">updateSize</span><span class="p">(</span><span class="nx">nd</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="nx">nd</span><span class="p">,</span> <span class="nx">hasNew</span>
<span class="p">}</span>
 
<span class="c1">//在以nd为根节点的树中获取键为k的值
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">get</span><span class="p">(</span><span class="nx">nd</span> <span class="o">*</span><span class="nx">node</span><span class="p">,</span> <span class="nx">k</span> <span class="kt">int</span><span class="p">)</span> <span class="kt">int</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">nd</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="nx">k</span> <span class="p">&lt;</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">k</span> <span class="p">{</span>
		<span class="k">return</span> <span class="nf">get</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">left</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="nx">k</span> <span class="p">&gt;</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">k</span> <span class="p">{</span>
		<span class="k">return</span> <span class="nf">get</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">right</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
		<span class="k">return</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">v</span>
	<span class="p">}</span>
<span class="p">}</span>
 
<span class="c1">//获取以nd为根节点的树的大小
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">size</span><span class="p">(</span><span class="nx">nd</span> <span class="o">*</span><span class="nx">node</span><span class="p">)</span> <span class="kt">int</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">nd</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span> <span class="mi">0</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">sz</span>
<span class="p">}</span>
 
<span class="c1">//更新以nd为根节点的树的大小
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">updateSize</span><span class="p">(</span><span class="nx">nd</span> <span class="o">*</span><span class="nx">node</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">nd</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span>
	<span class="p">}</span>
	<span class="nx">nd</span><span class="p">.</span><span class="nx">sz</span> <span class="p">=</span> <span class="nf">size</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">left</span><span class="p">)</span> <span class="o">+</span> <span class="nf">size</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">right</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="p">}</span>
 
<span class="c1">//选出以nd为根节点的树的最小键节点
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">min</span><span class="p">(</span><span class="nx">nd</span> <span class="o">*</span><span class="nx">node</span><span class="p">)</span> <span class="o">*</span><span class="nx">node</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">nd</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span> <span class="kc">nil</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">left</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span> <span class="nf">min</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">left</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="nx">nd</span>
<span class="p">}</span>
 
<span class="c1">//删除以nd为根节点的树的最小键节点
</span><span class="c1">//返回被删除的节点
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">deleteMin</span><span class="p">(</span><span class="nx">nd</span> <span class="o">*</span><span class="nx">node</span><span class="p">)</span> <span class="o">*</span><span class="nx">node</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">nd</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span> <span class="kc">nil</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">left</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span> <span class="c1">//找到最小节点
</span><span class="c1"></span>		<span class="nx">nd</span> <span class="p">=</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">right</span> <span class="c1">//用右子节点代替自己
</span><span class="c1"></span>	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span> <span class="c1">//还有更小的
</span><span class="c1"></span>		<span class="nx">nd</span><span class="p">.</span><span class="nx">left</span> <span class="p">=</span> <span class="nf">deleteMin</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">left</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="nf">updateSize</span><span class="p">(</span><span class="nx">nd</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">nd</span>
<span class="p">}</span>
 
<span class="c1">//删除以nd为根节点的树并且键为k的节点
</span><span class="c1"></span><span class="kd">func</span> <span class="nb">delete</span><span class="p">(</span><span class="nx">nd</span> <span class="o">*</span><span class="nx">node</span><span class="p">,</span> <span class="nx">k</span> <span class="kt">int</span><span class="p">)</span> <span class="o">*</span><span class="nx">node</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">nd</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="k">return</span> <span class="kc">nil</span>
	<span class="p">}</span>
	<span class="k">if</span> <span class="nx">k</span> <span class="p">&lt;</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">k</span> <span class="p">{</span>
		<span class="nx">nd</span><span class="p">.</span><span class="nx">left</span> <span class="p">=</span> <span class="nb">delete</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">left</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
	<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="nx">k</span> <span class="p">&gt;</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">k</span> <span class="p">{</span>
		<span class="nx">nd</span><span class="p">.</span><span class="nx">right</span> <span class="p">=</span> <span class="nb">delete</span><span class="p">(</span><span class="nx">nd</span><span class="p">.</span><span class="nx">right</span><span class="p">,</span> <span class="nx">k</span><span class="p">)</span>
	<span class="p">}</span> <span class="k">else</span> <span class="p">{</span> <span class="c1">//命中要删除的节点
</span><span class="c1"></span>		<span class="c1">//只有一个或零个子节点
</span><span class="c1"></span>		<span class="k">if</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">right</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
			<span class="k">return</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">left</span>
		<span class="p">}</span>
		<span class="k">if</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">left</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
			<span class="k">return</span> <span class="nx">nd</span><span class="p">.</span><span class="nx">right</span>
		<span class="p">}</span>
		<span class="c1">//同时具有两个子节点
</span><span class="c1"></span>		<span class="c1">//先找出大于本节点的最小节点作为后继节点
</span><span class="c1"></span>		<span class="nx">t</span> <span class="o">:=</span> <span class="nx">nd</span>
		<span class="nx">nd</span><span class="p">.</span><span class="nx">k</span> <span class="p">=</span> <span class="nf">min</span><span class="p">(</span><span class="nx">t</span><span class="p">.</span><span class="nx">right</span><span class="p">).</span><span class="nx">k</span>
		<span class="c1">//删除
</span><span class="c1"></span>		<span class="nf">deleteMin</span><span class="p">(</span><span class="nx">t</span><span class="p">.</span><span class="nx">right</span><span class="p">)</span>
		<span class="c1">//用后继节点代替本节点
</span><span class="c1"></span>		<span class="nx">nd</span><span class="p">.</span><span class="nx">left</span> <span class="p">=</span> <span class="nx">t</span><span class="p">.</span><span class="nx">left</span>
	<span class="p">}</span>
	<span class="nf">updateSize</span><span class="p">(</span><span class="nx">nd</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">nd</span>
<span class="p">}</span></code></pre></div>]]></content>
		</item>
		
		<item>
			<title>Redis开发与优化总结</title>
			<link>https://yulibaozi.com/posts/redis/know/2018-03-19-redis/</link>
			<pubDate>Mon, 19 Mar 2018 20:40:09 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/redis/know/2018-03-19-redis/</guid>
			<description>Redis中常用类型的数据结构优化：  string string内部有三种数据结构： 1. raw 2. int 3. embstr
 当值大于39个字节时使用raw;当值小于等于39个字节时使用embstr;当是长整型时使用int,可以是整数,浮点数（实际是字符串）,简单或复杂的字符串,也可以是二进制。但是值的最大值不超过512MB。 4. 如何判断某值所占字节数和查看编码.
redis&amp;gt; set yulibaozi 一个阳光的男孩 OK redis&amp;gt; strlen yulibaozi 21 （因为一个汉字占三个字节） redis&amp;gt; object encoding yulibaozi embstr  字符串如何优化: 1. 如果是整数,直接存储数据。如果是个字符串且长度&amp;gt;39个字节的数据,那么可以考虑其他的数据额结构或者将长度控制在39个字节以内。 原因:因为redisObject 对象的*ptr字段:与对象的数据内容有关,如果是整数它就直接存了数据,否则表示指向数据的指针。同样对值对象是字符串且长度&amp;lt;=39个字节的数据,内部编码是embstr类型的,字符串sds和redisObject一起分配,从而值需要一次内存操作。 2.缩减键值长度:可以去掉一些业务对象中不必要的属性来缩小数据大小。比如User对象有个注册时间。但是在我们的业务中并没有用到这个属性。那我们可以在Redis中舍弃这个属性。 3. 选择一些压缩工具来降低内存占用。比如把使用GZIP压缩JSON后可以节约约60%的空间。当然,你也需要考虑压缩/解压的速度和计算效率,因为这个他注定是频繁的。
 Hash hash内部有两种数据结构: 1. hashtable(散列表) 2. ziplist(压缩列表,双向列表)
 当值最大字节数&amp;lt;=hash-max-ziplist-value（默认是64个）且field个数&amp;lt;=hash-max-ziplist-entries(默认512个)时使用ziplist;否则使用hashtable
ziplist相比于hashtable更节约内存,但是ziplist的读写效率相对于hashtable有所降低。同样hashtable会消耗更对的内存。 ziplist（压缩链表）,他是一个连续的内存空间的压缩链表,他比较节约内存,压缩双链表节省前驱和后驱指针的空间（8b）
ziplist使用连续的内存;删除操作会导致内存重新分配和释放,加大了操作的复杂性;读写操作设计复杂的指针移动,最坏达到O(N2)的时间复杂度。所以ziplist适合存储小对象和长度有限的数据。
优化建议: 在针对性能较高的场景下时,保持使用ziplist数据结构,建议长度不要超过1000,每个元素大小控制在512个字节以内。
 list list内部有三种数据结构: 1. ziplist(压缩列表) 2. linkedlist(双向列表) 3. quicklist（快速列表）
 当值最大字节数&amp;lt;=list-max-ziplist-value（默认是64个）且field个数&amp;lt;=list-max-ziplist-entries(默认512个)时使用ziplist;不满足上述任意一个条件使用linkedlist;
 set set内部有两种数据结构: 1. hashtable 2. intset</description>
			<content type="html"><![CDATA[

<h3 id="redis中常用类型的数据结构优化">Redis中常用类型的数据结构优化：</h3>

<blockquote>
<h4 id="string">string</h4>

<p>string内部有三种数据结构：
1. raw
2. int
3. embstr</p>
</blockquote>

<p>当值大于39个字节时使用raw;当值小于等于39个字节时使用embstr;当是长整型时使用int,可以是整数,浮点数（实际是字符串）,简单或复杂的字符串,也可以是二进制。但是值的最大值不超过512MB。
4. 如何判断某值所占字节数和查看编码.</p>

<pre><code>redis&gt; set yulibaozi 一个阳光的男孩
OK
redis&gt; strlen yulibaozi
21 （因为一个汉字占三个字节）
redis&gt; object encoding yulibaozi
embstr
</code></pre>

<p>字符串如何优化:
1. 如果是整数,直接存储数据。如果是个字符串且长度&gt;39个字节的数据,那么可以考虑其他的数据额结构或者将长度控制在39个字节以内。
原因:因为redisObject 对象的*ptr字段:与对象的数据内容有关,如果是整数它就直接存了数据,否则表示指向数据的指针。同样对值对象是字符串且长度&lt;=39个字节的数据,内部编码是embstr类型的,字符串sds和redisObject一起分配,从而值需要一次内存操作。
2.缩减键值长度:可以去掉一些业务对象中不必要的属性来缩小数据大小。比如User对象有个注册时间。但是在我们的业务中并没有用到这个属性。那我们可以在Redis中舍弃这个属性。
3. 选择一些压缩工具来降低内存占用。比如把使用GZIP压缩JSON后可以节约约60%的空间。当然,你也需要考虑压缩/解压的速度和计算效率,因为这个他注定是频繁的。</p>

<blockquote>
<h4 id="hash">Hash</h4>

<p>hash内部有两种数据结构:
1. hashtable(散列表)
2. ziplist(压缩列表,双向列表)</p>
</blockquote>

<p>当值最大字节数&lt;=hash-max-ziplist-value（默认是64个）且field个数&lt;=hash-max-ziplist-entries(默认512个)时使用ziplist;否则使用hashtable</p>

<p>ziplist相比于hashtable更节约内存,但是ziplist的读写效率相对于hashtable有所降低。同样hashtable会消耗更对的内存。
ziplist（压缩链表）,他是一个连续的内存空间的压缩链表,他比较节约内存,压缩双链表节省前驱和后驱指针的空间（8b）</p>

<p>ziplist使用连续的内存;删除操作会导致内存重新分配和释放,加大了操作的复杂性;读写操作设计复杂的指针移动,最坏达到O(N2)的时间复杂度。所以ziplist适合存储小对象和长度有限的数据。</p>

<p>优化建议:
在针对性能较高的场景下时,保持使用ziplist数据结构,建议长度不要超过1000,每个元素大小控制在512个字节以内。</p>

<blockquote>
<h4 id="list">list</h4>

<p>list内部有三种数据结构:
1. ziplist(压缩列表)
2. linkedlist(双向列表)
3. quicklist（快速列表）</p>
</blockquote>

<p>当值最大字节数&lt;=list-max-ziplist-value（默认是64个）且field个数&lt;=list-max-ziplist-entries(默认512个)时使用ziplist;不满足上述任意一个条件使用linkedlist;</p>

<blockquote>
<h4 id="set">set</h4>

<p>set内部有两种数据结构:
1. hashtable
2. intset</p>
</blockquote>

<p>当元素全部是整数且集合长度小于等于&lt;=set-max-intset-entries（默认是:512个）时使用intset否则使用hashtable</p>

<blockquote>
<h4 id="zset">zset</h4>

<p>zset内部有两种数据结构:
1. skiplist(跳表)
2. ziplist</p>
</blockquote>

<p>当值最大字节数小于&lt;=zset-max-ziplist-value（默认64个字节）且有序集合长度小于等于&lt;=zset-max-ziplist-entries(默认128个)时使用ziplist,否则使用skiplist</p>

<h3 id="redis键解析与关系设计">Redis键解析与关系设计</h3>

<ol>
<li>在明确键意不和其他模型冲突有歧义的情况的下,键的长度尽量简短,清晰。比如user:1,如果整个项目只有它是以u打头的,我们可以使用u:1。</li>
<li>在用键来表达关系时,我们可以使用<strong>模型:ID:关系</strong>来设计。比如:某用户下的粉丝列表可以用:<strong>u:1:fans</strong>。</li>
<li>因为redis是个key/value数据库,在处理模型关系上没有关系数据库那么方便,所以设计之初就需要理清模型之间的关系和需要使用的redis数据结构。且在设计的时候尽可能的回避对redis阻塞命令的使用。比如keys等。</li>
</ol>

<h3 id="redis不推荐命令与替换">Redis不推荐命令与替换</h3>

<ol>
<li>append 命令</li>
</ol>

<p>在值后追加数据。因为redis的字符串结构是自定义的。简单动态字符串(sds).
在新插入一个值时,预备的字节数为0,总占用=时间占用+1个字节。1字节是&rsquo;\0&rsquo;结尾符。如果使用append最佳字符串数据,会导致这个对象预分配一倍的容量作为预备空间,同时会导致内存重分配。造成内存碎片率。预留空间分配规律如下:第一次创建不分配。追加后,如果实际数据小于1M且之间的预留空间不够，预分配一倍容量。比如原来有60byte,free=0，追加了60bytes,那么预分配120bytes,总空间就是(60+60+120+1)byte;如果数据大于1MB且预留空间不够,那么每次预分配1MB。综上述:append可能会导致白白浪费内存.</p>

<p>所以我们可以直接使用set命令来代替append和setrange修改字符串,来降低内存碎片化和浪费内存。</p>

<ol>
<li>keys,hgetall之类命令就不说了。其中hgetall的复杂度是o(n),n是filed的个数。在个数较少的情况下,还可用。</li>
</ol>

<h3 id="redis结构体选择判断">Redis结构体选择判断</h3>

<ol>
<li>如果hash使用ziplist编码的话。同样的数据,值越小,越应该选择hash而不是字符串.因为从内存的角度来说。此时的hash比string更节约内存。且值越小,内存的节约量更加明显。从写入的角度来说,hash的写入要比字符串慢,值越小,这个差距会越来越小,但是不可能持平。</li>
<li>从业务的角度来说:如果属性经常更改。那么存hash要方便得多且更改容易,反之,字符串就没有那么容易了。</li>
<li>如果hash使用的hashtable编码的话，那么可能不会节约内存反而可能会增加内存的消耗。</li>
<li>ziplist编码的hash更适合存储小对象，对于大对象不但内存优化效果不明显反而会增加命令操作的耗时。</li>
<li>ziplist在hash,list,zset下的耗时排序:list&lt;hash&lt;zset。</li>
</ol>

<p><strong>最后:推荐一本书《Redis开发与运维》</strong></p>

<h3 id="tinks">Tinks</h3>

<p>1、主动缓存（适用于更新可能小的情况，如一些设置或者配置）</p>

<pre><code>写场景：先将数据写入到数据库，写入成功后立即把数据同步到缓存。

或者写入到数据库后，把之前的缓存失效，下次请求的时候，加入到缓存中
</code></pre>

<p>2、直写(异步的方式)</p>

<pre><code>直接把Cache假装当成数据库，读写都针对Cache，然后Cache负责后面的数据同步问题,
以同步的方式 先写入数据库，写入成功后写入redis。
如果写入数据库一直没成功，采用日志记录的方式，单开个任务队列，读取日志，主动写入数据库并写入redis缓存
</code></pre>

<p>4、其他想法：(异步的方式)</p>

<pre><code>写入的时候，写入到redis,然后使用任务分发的方式，更新数据库，但是也需要和2一样，做数据库一致保证
</code></pre>

<p>5、京东做秒杀系统的做法：(异步的方式)</p>

<pre><code>直接对redis做操作，同时会记录redis的操作日志，然后另外的work服务主动读取日志，同步到数据库
</code></pre>
]]></content>
		</item>
		
		<item>
			<title>微服务设计与实践的经验总结</title>
			<link>https://yulibaozi.com/posts/design/knowledge/2018-03-09-microservices/</link>
			<pubDate>Fri, 09 Mar 2018 22:35:52 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/design/knowledge/2018-03-09-microservices/</guid>
			<description>对于项目中微服务的拆分方案:
自家项目中拆分的维度或者原则有哪些？ 在我们的项目中,拆分选取了多个维度。 1. 按照功能拆分 2. 按照业务拆分 3. 按照未来扩展度拆分 4. 按照平台拆分 5. 按照数据拆分。
我们在拆分设计过程中,其实还隐含了一个拆分原则:重要度拆分,并不是按照单一的拆分原则来拆分。而是多种原则并用,才会确定如何拆分。
具体的拆分服务以及为什么这样拆分? 我们拆分的部分服务有:
用户服务，计算服务,评论服务,用户服务,社区服务,微信小程序等。  大体示意图: 1. 主要按照功能拆分的实践&amp;lt;计算服务和评论服务&amp;gt;:
由于计算服务是一个底层服务,大多数业务(纯粹的业务,和不纯粹的业务(还包括了用户))都需要使用它,所以他的定位是:核心的,基本的,纯粹的,所以这个服务应该是按照功能拆分,能获取单一的特定的数据即可。 评论服务的拆分原则:评论系统的定位比较低,在数据上肯定是有帖子/主题才会有评论,而且有些游离于主要服务之外。这类服务主要是发评论和展示，对扩展而言,多个依附关系决定了他的扩张度不高,因为简单唯一,所以按照功能才分。   主要按照扩展度拆分的实践&amp;lt;用户服务&amp;gt;:
用户服务处于一个比较核心的地位,而且考虑到以后的项目开发能重用这一套用户系统，所以主要按照扩展度拆分，提供的方法不宜太多,提供登录注册，获取用户信息等方法就够了,主要是面向扩展的。这点是我们之前没有想到的。
 主要按照业务拆分的实践&amp;lt;社区服务&amp;gt;:
社区服务时一个大的需求服务，这个服务具有一定的时效性，因为需求存在时效性性,在未来看某些需求,可能就不那么合理,也是需求改动的重灾区,所以这个服务无论是代码量还是设计范围还是需求改动都比较大,而且可能存在被淘汰的可能,所以,我们就不对它再次拆分,而是注意这个服务的内部设计可以参考Go项目结构设计与实践(多存储下),尽量做到可变,灵活变即可。
 主要按照平台拆分的实践&amp;lt;小程序服务&amp;gt;:
小程序服务是为了照顾微信用户(微信的用户群体大),因为它的需求极有可能和PC,移动站的需求不太一样,这个和潮流和也公司的用户群体有关,公司的用户群体主要在那方面,可能就会侧重那方面,涉及到改动的侧重也就不一样,所以,按照平台拆分,可能是不二选择。
  实践中的好处与不足: 优点: 在实践中,我们享受到微服务设计的种种好处,比如说 1. 独立开发,熟悉开发
从效率的角度来说,当拆分和任务分配后,每个人独立完成某个模块,这样没有交叉,用自己熟悉的方式写自己的code,效率很灵活度都得到很大提升。   范围可控
从更改需求的角度来说,当更改需求时,只需要在其中一个微服务下修改代码,然后发布,这样涉及到的面很小,同时,如果改动出现了意外bug,受灾波及的范围也很小,能够把服务受灾粒度降到最低。
 通用约定，简单沟通
通用约定更少,除非是必要的通用约定(比如:Redis的Key,数据库状态值代表的意思,接口通用数据结构),在程序内部,我们可以按照我们自己的意愿在一个文件中统一声明,这样减少了沟通的成本。
 扩展容易，小巧轻便
当多个服务必须依赖某个服务时,需要扩展时,只需要对这个小的服务做负载均衡,这样服务的体积更小,更轻量。
 积木搭建,灵活多变。
由于是积木式搭建,灵活是微服务的核心，灵活多变是微服务的特质。在我们开发历程中,经历了无数次的需求改变或扩展,虽然有些时候精疲力竭,但是对于设计来说,改变却是轻而易举。虽然你可能庆幸于灵活多变，但是也要注意这次改变是否违背之前的约定,造成不必要的麻烦。
 通用的模型定义和约定,在设计之初就要想好模型定义的位置,多个服务之间公用同一模型,改动时,只需要一个地方改动,不存在双标(当然传统的一体化设计也不存在这种情况。)
  一些不足: 当然,好处多,但是我们使用过程中,也觉得有些不足: 1. 可能存在部分重复代码:
在多个微服务中,存在一定量的重复代码,当然,你可能会说,为什么不去调另外一个服务的接口,这样是能做到,同时没有重复代码,但是存在的问题是:平级服务之间相互依赖,这可不是一件好事,违背了微服务尽量独立的一贯原则,其二是:如果被依赖服务挂掉,那么受灾范围将会变大,这又违背了我们使用微服务的一个初衷。   可能存在通信问题
当的的确确存在上层服务依赖下层服务时,通讯和性能便成了一个主要问题,在我们项目中设计到一个期数计算服务,这是一个基础服务,上层服务需要调用,所有调用这个基础服务的接口都变得很慢,当时使用的GRPC,但是性能也不尽如人意或者说很不理想,性能波动很大,2倍,3倍,即使不波动,也会多出5ms+以上,最初查问题,根本没想到这个点上来,还是我们的C#在使用接口时,莫名的一句话,好像使用期数的接口都比较慢,突然,茅塞顿开,测试之后果然是这样,想了多种解决策略后,后面决定使用Redis把数据存起来(因为这个数据是通用的),用GRPC的数据保证正确性,用REDIS的数据保证效率,唯一难点就在时效性,因为行业的原因过了时间某个开奖时间点,数据就意味着错误,还好当初在设计接口时,数据返回得足够完整,测试的时候,还有些紧张,如果这种方案不能解决,我又将深受打击,而且已经困扰多时了, 打开浏览器,输入地址,回车。 这一刻,多天的抑郁一扫而空,性能提升10倍不止,比我预期的还要少的多, 这一刻我觉得我是个工程师的料，高兴过后,我决定优化这段代码,因为业务的关系,我优化的这小段代码满足90%的需求，变成了通用的获取期数代码,甚至可以说是公司最小巧实用而又有优雅的代码,比我当初写期数计算服务的成就还来的高。不仅减小了期数计算服务的压力,还获得了巨大的性能提升。</description>
			<content type="html"><![CDATA[

<p>对于项目中微服务的拆分方案:</p>

<h3 id="自家项目中拆分的维度或者原则有哪些">自家项目中拆分的维度或者原则有哪些？</h3>

<p>在我们的项目中,拆分选取了多个维度。
1. 按照功能拆分
2. 按照业务拆分
3. 按照未来扩展度拆分
4. 按照平台拆分
5. 按照数据拆分。</p>

<p>我们在拆分设计过程中,其实还隐含了一个拆分原则:<strong>重要度拆分</strong>,并不是按照单一的拆分原则来拆分。而是多种原则并用,才会确定如何拆分。</p>

<h3 id="具体的拆分服务以及为什么这样拆分">具体的拆分服务以及为什么这样拆分?</h3>

<p>我们拆分的部分服务有:</p>

<pre><code>用户服务，计算服务,评论服务,用户服务,社区服务,微信小程序等。
</code></pre>

<p>大体示意图:
<img src="http://index.yulibaozi.com/22505ce3-b161-4500-8697-26ae79740d92QQ20180309-0.jpg" alt="image" />
1. 主要按照功能拆分的实践&lt;计算服务和评论服务&gt;:</p>

<pre><code>由于计算服务是一个底层服务,大多数业务(纯粹的业务,和不纯粹的业务(还包括了用户))都需要使用它,所以他的定位是:核心的,基本的,纯粹的,所以这个服务应该是按照功能拆分,能获取单一的特定的数据即可。

评论服务的拆分原则:评论系统的定位比较低,在数据上肯定是有帖子/主题才会有评论,而且有些游离于主要服务之外。这类服务主要是发评论和展示，对扩展而言,多个依附关系决定了他的扩张度不高,因为简单唯一,所以按照功能才分。
</code></pre>

<ol>
<li><p>主要按照扩展度拆分的实践&lt;用户服务&gt;:</p>

<p>用户服务处于一个比较核心的地位,而且考虑到以后的项目开发能重用这一套用户系统，所以主要按照扩展度拆分，提供的方法不宜太多,提供登录注册，获取用户信息等方法就够了,主要是面向扩展的。这点是我们之前没有想到的。</p></li>

<li><p>主要按照业务拆分的实践&lt;社区服务&gt;:</p>

<p>社区服务时一个大的需求服务，这个服务具有一定的时效性，因为需求存在时效性性,在未来看某些需求,可能就不那么合理,也是需求改动的重灾区,所以这个服务无论是代码量还是设计范围还是需求改动都比较大,而且可能存在被淘汰的可能,所以,我们就不对它再次拆分,而是注意这个服务的内部设计可以参考<a href="http://yulibaozi.com/Article/34">Go项目结构设计与实践(多存储下)</a>,尽量做到可变,灵活变即可。</p></li>

<li><p>主要按照平台拆分的实践&lt;小程序服务&gt;:</p>

<p>小程序服务是为了照顾微信用户(微信的用户群体大),因为它的需求极有可能和PC,移动站的需求不太一样,这个和潮流和也公司的用户群体有关,公司的用户群体主要在那方面,可能就会侧重那方面,涉及到改动的侧重也就不一样,所以,按照平台拆分,可能是不二选择。</p></li>
</ol>

<h3 id="实践中的好处与不足">实践中的好处与不足:</h3>

<h4 id="优点">优点:</h4>

<p>在实践中,我们享受到微服务设计的种种好处,比如说
1. 独立开发,熟悉开发</p>

<pre><code>从效率的角度来说,当拆分和任务分配后,每个人独立完成某个模块,这样没有交叉,用自己熟悉的方式写自己的code,效率很灵活度都得到很大提升。
</code></pre>

<ol>
<li><p>范围可控</p>

<p>从更改需求的角度来说,当更改需求时,只需要在其中一个微服务下修改代码,然后发布,这样涉及到的面很小,同时,如果改动出现了意外bug,受灾波及的范围也很小,能够把服务受灾粒度降到最低。</p></li>

<li><p>通用约定，简单沟通</p>

<p>通用约定更少,除非是必要的通用约定(比如:Redis的Key,数据库状态值代表的意思,接口通用数据结构),在程序内部,我们可以按照我们自己的意愿在一个文件中统一声明,这样减少了沟通的成本。</p></li>

<li><p>扩展容易，小巧轻便</p>

<p>当多个服务必须依赖某个服务时,需要扩展时,只需要对这个小的服务做负载均衡,这样服务的体积更小,更轻量。</p></li>

<li><p>积木搭建,灵活多变。</p>

<p>由于是积木式搭建,灵活是微服务的核心，灵活多变是微服务的特质。在我们开发历程中,经历了无数次的需求改变或扩展,虽然有些时候精疲力竭,但是对于设计来说,改变却是轻而易举。虽然你可能庆幸于灵活多变，但是也要注意这次改变是否违背之前的约定,造成不必要的麻烦。</p></li>

<li><p>通用的模型定义和约定,在设计之初就要想好模型定义的位置,多个服务之间公用同一模型,改动时,只需要一个地方改动,不存在双标(当然传统的一体化设计也不存在这种情况。)</p></li>
</ol>

<h4 id="一些不足">一些不足:</h4>

<p>当然,好处多,但是我们使用过程中,也觉得有些不足:
1. 可能存在部分重复代码:</p>

<pre><code>在多个微服务中,存在一定量的重复代码,当然,你可能会说,为什么不去调另外一个服务的接口,这样是能做到,同时没有重复代码,但是存在的问题是:平级服务之间相互依赖,这可不是一件好事,违背了微服务尽量独立的一贯原则,其二是:如果被依赖服务挂掉,那么受灾范围将会变大,这又违背了我们使用微服务的一个初衷。
</code></pre>

<ol>
<li><p>可能存在通信问题</p>

<p>当的的确确存在上层服务依赖下层服务时,通讯和性能便成了一个主要问题,在我们项目中设计到一个期数计算服务,这是一个基础服务,上层服务需要调用,所有调用这个基础服务的接口都变得很慢,当时使用的GRPC,但是性能也不尽如人意或者说很不理想,性能波动很大,2倍,3倍,即使不波动,也会多出5ms+以上,最初查问题,根本没想到这个点上来,还是我们的C#在使用接口时,莫名的一句话,好像使用期数的接口都比较慢,突然,茅塞顿开,测试之后果然是这样,想了多种解决策略后,后面决定使用Redis把数据存起来(因为这个数据是通用的),用GRPC的数据保证正确性,用REDIS的数据保证效率,唯一难点就在时效性,因为行业的原因过了时间某个开奖时间点,数据就意味着错误,还好当初在设计接口时,数据返回得足够完整,测试的时候,还有些紧张,如果这种方案不能解决,我又将深受打击,而且已经困扰多时了,   打开浏览器,输入地址,回车。 这一刻,多天的抑郁一扫而空,性能提升10倍不止,比我预期的还要少的多, 这一刻我觉得我是个工程师的料，高兴过后,我决定优化这段代码,因为业务的关系,我优化的这小段代码满足90%的需求，变成了通用的获取期数代码,甚至可以说是公司最小巧实用而又有优雅的代码,比我当初写期数计算服务的成就还来的高。不仅减小了期数计算服务的压力,还获得了巨大的性能提升。</p></li>

<li><p>某接口压力集中</p>

<p>当确实存在依赖服务的时候,往往都是某一个或者少量的方法,这意味着被依赖的这几个方法压力很大,其他方法的压力基本很少或者没有。</p></li>

<li><p>铭记发布顺序</p>

<p>当发布服务时,由于是微服务设计,需要理清多个服务之间的发布顺序,发布服务多时,你需要细致的检查每个服务的配置信息,并且牢记发布的地址,否则的话,对导致上层接口不可用。</p></li>

<li><p>代码正确性不能把控</p>

<p>在进行开发时，由于是某个人独立开发,那么就意味着代码风格,注释不可控。因为时间的原因,你可能只来得及关心接口或者方法的数据是否正确,而来不及关心他是怎么实现的,所以需要把控着一个点。</p></li>
</ol>

<p>最后:</p>

<pre><code>任何一个看似就应该是这样的设计,都有着许多智慧。
</code></pre>
]]></content>
		</item>
		
		<item>
			<title>网站从单机到分布式的变迁</title>
			<link>https://yulibaozi.com/posts/design/knowledge/2018-03-09-single-to-many/</link>
			<pubDate>Sat, 24 Feb 2018 22:37:47 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/design/knowledge/2018-03-09-single-to-many/</guid>
			<description>变迁的过程分8步:
 单机 单机负载警告，数据与应用分离 应用服务器警告，应用服务器集群 数据库压力过大，读写分离 数据库再遇瓶颈，数据库垂直拆分à 单机数据库又遇瓶颈，水平拆分 数据库解决问后，应用面临新挑战，应用拆分 服务化结构  各个阶段遇到的问题和解决办法 1、 单机 只是在应用中把数据库的地址从本地改到另外一台机器上。
2、 单机负载警告，数据与应用分离 应用服务器之间没有交互，他们都是依赖数据库提供服务的。
3、 应用服务器警告，应用服务器集群
 3.1、出现的问题和解决办法
  用户对应用服务器访问的选择问题
问题描述：当用户访问的时候，应该选择那一台应用服务器响应用户请求。 解决办法： 1.1、通过DNS解决。 通过DNS服务器进行调度和控制，用户解析DNS的时候，就给用户一个应用服务器地址。 1.2、通过在应用服务器前增加负载均衡设备解决。 所有用户请求访问的时候都经过负载均衡设备来完成请求转发控制。  Session问题：
问题描述：当Http请求web服务器的时候，需要在http请求中找到会话数据（Session）,问题在于会话数据保存在单机上的，我第一次访问落在左边服务器（假设服务器有两台），这时候我的Session就创建在左边服务器，右边服务器就没有，我如何保证我每次访问都在同一台服务器或者说都能够拿到Session呢？ 解决办法： 2.1、添加负载均衡器，保证同样的Session请求都发送到同一个服务端处理。 优点:有利于针对Session进行服务器本地缓存。 缺点：如果一台Web服务器重启，这台机器的会话数据会丢失； 2.2、不要求负载均衡器保证同样的Session请求发送到同一服务端，而是在服务端进行Session数据同步。 缺点：同步Session数据会带来带宽开销，只要Session有变化就需要同步；每台Web服务器都需要保存所有的Session数据，性能下降厉害。 2.3、把Session数据集中处理，Web服务器需要使用Session数据时，去集中存储的地方读取就好。 缺点：读写Session数据引入了网络操作，存在时延和不稳定性；如果集中存储Session数据的地方出问题，影响巨大，但是对于Session数据很多的时候，优势巨大。 2.4、把Session数据放在Cookie中，每次Web服务器就从Cookie中生成对应的Session数据。 优点：同一会话的不同请求不限制具体处理机器。不依赖外部存储系统，不存在时延等问题。 缺点：Cookie长度限制，这会限制Session的长度；安全性，Session数据本来是服务端数据；性能影响，每次Http请求响应都带有Session数据。  数据库压力过大，读写分离
读写分离解决的是写数据读的压力。 带来的问题和解决办法： 数据复制问题 Master+Slave结构可以提供数据复制机制。（一主多从） 当主/从非对称且数据结构相同，多从对应一主的场景数据复制问题 解决办法： 应用通过数据访问层访问数据库，通过消息系统就数据库的通知发出消息通知，数据库同步服务器得到消息后进行数据复制，分库规则配置则复制在读数据以及数据同步服务器更新分库时让数据层知道分库规则，其中，数据同步服务器和DB主库得到交互主要是根据被修改或新增的数据主键来获取内容，他们采用的是行复制的方式。 第二钟基于数据库日志来进行数据的复制。 当主/从非对称且主/备分库方式不同时数据复制问题 非对称指的是：源数据和目标数据不是镜像关系。 应用数据源选择问题。 数据缓存:我们可以用Key-value进行缓存（例如：Redis），在缓存中，一般存放的是热数据而不是全部数据。应用先访问缓存，如果访问不存在再去访问数据库。 页面缓存。我们可以把静态页面，热页面，Js等进行缓存，例如CDN。  数据库再遇瓶颈，数据库垂直拆分
垂直拆分是指：专库专用，把数据库中不同的业务数据拆分到不同的数据库中。 遇到的问题: 应用需要配置多个数据源。 单机的ACID（原子性，一致性，隔离性，持久性）被打破。原单机事务控制的逻辑被打破。（见下） 靠外键进行约束的场景会受到影响。 Jion操作变得困难，因为数据可能已经在两个数据库中。（见下）  单机数据库又遇瓶颈，水平拆分</description>
			<content type="html"><![CDATA[

<p>变迁的过程分8步:</p>

<ol>
<li>单机</li>
<li>单机负载警告，数据与应用分离</li>
<li>应用服务器警告，应用服务器集群</li>
<li>数据库压力过大，读写分离</li>
<li>数据库再遇瓶颈，数据库垂直拆分à</li>
<li>单机数据库又遇瓶颈，水平拆分</li>
<li>数据库解决问后，应用面临新挑战，应用拆分</li>
<li>服务化结构</li>
</ol>

<h4 id="各个阶段遇到的问题和解决办法">各个阶段遇到的问题和解决办法</h4>

<p>1、  单机
只是在应用中把数据库的地址从本地改到另外一台机器上。</p>

<p>2、  单机负载警告，数据与应用分离
应用服务器之间没有交互，他们都是依赖数据库提供服务的。</p>

<p>3、  应用服务器警告，应用服务器集群</p>

<blockquote>
<p>3.1、出现的问题和解决办法</p>
</blockquote>

<ol>
<li><p>用户对应用服务器访问的选择问题</p>

<pre><code>问题描述：当用户访问的时候，应该选择那一台应用服务器响应用户请求。

解决办法：

1.1、通过DNS解决。

通过DNS服务器进行调度和控制，用户解析DNS的时候，就给用户一个应用服务器地址。

1.2、通过在应用服务器前增加负载均衡设备解决。

所有用户请求访问的时候都经过负载均衡设备来完成请求转发控制。
</code></pre></li>

<li><p>Session问题：</p>

<pre><code>问题描述：当Http请求web服务器的时候，需要在http请求中找到会话数据（Session）,问题在于会话数据保存在单机上的，我第一次访问落在左边服务器（假设服务器有两台），这时候我的Session就创建在左边服务器，右边服务器就没有，我如何保证我每次访问都在同一台服务器或者说都能够拿到Session呢？

解决办法：

2.1、添加负载均衡器，保证同样的Session请求都发送到同一个服务端处理。

优点:有利于针对Session进行服务器本地缓存。

缺点：如果一台Web服务器重启，这台机器的会话数据会丢失；

 

2.2、不要求负载均衡器保证同样的Session请求发送到同一服务端，而是在服务端进行Session数据同步。

缺点：同步Session数据会带来带宽开销，只要Session有变化就需要同步；每台Web服务器都需要保存所有的Session数据，性能下降厉害。

 

2.3、把Session数据集中处理，Web服务器需要使用Session数据时，去集中存储的地方读取就好。

缺点：读写Session数据引入了网络操作，存在时延和不稳定性；如果集中存储Session数据的地方出问题，影响巨大，但是对于Session数据很多的时候，优势巨大。

 

2.4、把Session数据放在Cookie中，每次Web服务器就从Cookie中生成对应的Session数据。

优点：同一会话的不同请求不限制具体处理机器。不依赖外部存储系统，不存在时延等问题。

缺点：Cookie长度限制，这会限制Session的长度；安全性，Session数据本来是服务端数据；性能影响，每次Http请求响应都带有Session数据。
</code></pre></li>

<li><p>数据库压力过大，读写分离</p>

<pre><code>读写分离解决的是写数据读的压力。

带来的问题和解决办法：

数据复制问题
Master+Slave结构可以提供数据复制机制。（一主多从）

当主/从非对称且数据结构相同，多从对应一主的场景数据复制问题
解决办法：

应用通过数据访问层访问数据库，通过消息系统就数据库的通知发出消息通知，数据库同步服务器得到消息后进行数据复制，分库规则配置则复制在读数据以及数据同步服务器更新分库时让数据层知道分库规则，其中，数据同步服务器和DB主库得到交互主要是根据被修改或新增的数据主键来获取内容，他们采用的是行复制的方式。
第二钟基于数据库日志来进行数据的复制。
当主/从非对称且主/备分库方式不同时数据复制问题
非对称指的是：源数据和目标数据不是镜像关系。

应用数据源选择问题。
数据缓存:我们可以用Key-value进行缓存（例如：Redis），在缓存中，一般存放的是热数据而不是全部数据。应用先访问缓存，如果访问不存在再去访问数据库。
页面缓存。我们可以把静态页面，热页面，Js等进行缓存，例如CDN。
</code></pre></li>

<li><p>数据库再遇瓶颈，数据库垂直拆分</p>

<pre><code>垂直拆分是指：专库专用，把数据库中不同的业务数据拆分到不同的数据库中。

遇到的问题:

应用需要配置多个数据源。
单机的ACID（原子性，一致性，隔离性，持久性）被打破。原单机事务控制的逻辑被打破。（见下）
靠外键进行约束的场景会受到影响。
Jion操作变得困难，因为数据可能已经在两个数据库中。（见下）
</code></pre></li>

<li><p>单机数据库又遇瓶颈，水平拆分</p>

<pre><code>水平拆分是指:把同一表中的数据拆分到两个数据库中。（例如按照地域拆分）

遇到的问题：
数据量太大进行分页处理的解决办法
非排序分页
同等步长地多个在多个数据源上分页处理。
分页的每页中来自不同数据源的记录数是一样的。

同等比例地分页处理。
分页的每页中，来自不同数据源的数据数占这个数据源符合条件的数据总数的比例是一样的。

排序后分页，这种情况是复杂的情况，假设我们有两个数据源
我们需要考虑极端情况就是 一页中数据可能来自同一个数据源，所以，假设我们一页需要5条数据，然后需要从两个数据源各取5条归并排序，对于第二页就需要把两个数据源的前10条（共计20条）进行归并，这意味着越往后翻，负担越大。
单机的ACID（原子性，一致性，隔离性，持久性）被打破。原单机事务控制的逻辑被打破。
靠外键进行约束的场景会受到影响。
依赖单库的自增序列生成唯一ID会受到影响。
</code></pre></li>
</ol>

<blockquote>
<p>唯一性问题</p>
</blockquote>

<p>如果只考虑Id的唯一性可以参考UUID的生成方式；根据业务情况使用各个种子（比如IP、MAC、时间等）生成唯一的Id。缺点是：连续性不好</p>

<blockquote>
<p>连续性问题</p>
</blockquote>

<p>把所有的Id放在同一个地方进行管理。对每个Id序列独立管理，每个机器使用Id时，都从这个Id生成器上取。缺点：性能问题，生成器的稳定问题，存储问题。</p>

<p>为了解决生成器的稳定问题，我们可以把Id生成器从机器和存储之间舍掉，在每个应用上完成生成器（生成Id）要做的工作，然后给应用请求使用。</p>

<p>单个逻辑意义上的表查询可能需要跨表查询。</p>

<blockquote>
<p>6.1、查询结果在应用上合并问题：</p>
</blockquote>

<p>1、排序问题：</p>

<p>如果查出来就是排好顺序，进行多路归并就好。</p>

<p>如果查出来的数据未排序，就要进行全排序。</p>

<p>2、函数处理问题</p>

<p>即使用Max、Min等对数据结果进行函数处理。</p>

<p>3、求平均值问题</p>

<p>需要把SQL改为查询SUM和Count 然后对多个数据来源的SUM求和，和Count求和，然后计算平均值。</p>

<p>Jion操作变得困难，因为数据可能已经在两个数据库中。（见下）</p>

<blockquote>
<p>水平拆分和垂直拆分遇到的共同问题解决:</p>
</blockquote>

<p>1、单机的ACID（原子性，一致性，隔离性，持久性）被打破。原单机事务控制的逻辑被打破。</p>

<p>解决办法：</p>

<p>两阶段提交。提交之前进行了准备阶段，如果在一个阶段失败，那么就在第二阶段的处理就是回滚所有资源。在必要的情况下才建议使用。
2、Paxos协议：</p>

<p>优点：比两阶段提交更轻量级。</p>

<p>前提:不存在拜占庭将军问题，也就是说需要保证一个可靠的安全的通信环境。</p>

<blockquote>
<p>2、Jion操作变得困难，因为数据可能已经在两个数据库中。</p>
</blockquote>

<p>解决办法：</p>

<p>7.1、把应用层原来数据库的Jion操作分成多个数据库操作。</p>

<p>7.2、数据冗余，对一些常用的信息进行数据冗余，这样就不需要Jion就变成单表操作。</p>

<p>7.3、借助外部系统（如搜素引擎）解决跨库问题。</p>

<blockquote>
<p>3、选择何种方式进行分库？</p>
</blockquote>

<p>1、根据某个字段(例如用户id)取模，然后将数据分散到不同的数据库和表中。分库的标准是尽可能的避免跨库查询。</p>

<p>2、除了id取模，还可以根据时间维度,好处在于数据备份、迁移和数据清空很方便。</p>

<p>4、如何进行分表？假设我们把同类数据放在两个数据库的四张表中</p>

<p>1、首先通过模2确定数据的分库，然后通过模4进行数据库内部分表。</p>

<p>2、3和4其实都是采用了固定哈希算法作为分库分表的规则。</p>

<p>5、一致性哈希的好处：</p>

<p>1、把节点对应的哈希值编程了一个范围，不再是离散的。</p>

<p>6、SQL的改写:</p>

<p>为什么要改写SQL？</p>

<p>分布在不同数据库的表结构一样，但表名未必一样，如果表名一样会产生误操作且不优雅。表名不一样进行路由和数据迁移时也比较便利。
SQL用到的索引名也需要修改，需要从逻辑上的名字变为对应数据库的物理名字。
在跨库计算平均值的时候也需要SQL为:获得数据后在计算。
如何选择数据源？
因为会为分库提供备库，分库把数据分到不同的数据分组中，根据执行SQL的特点（读，写），是否在事务中以及各库的权重规则计算得到SQL需要访问的数据库。</p>

<blockquote>
<p>8、如何做到数据平滑迁移？</p>
</blockquote>

<p>当在迁移的过程中又有数据的变化，可以考虑:</p>

<p>1、在开始进行数据迁移时，记录增量的日志，在迁移结束后，再对增量的变化进行处理，最后要把迁移的数据写暂停，保证增量日志处理完毕后，再切换规则，放开所有的写，完成迁移工作。</p>

<blockquote>
<p>7、  应用面临新挑战，应用拆分
为什么要拆分？</p>
</blockquote>

<p>业务越来越多，应用的功能也就越来越多，应用越来越大，我们需要考虑如何不让应用显得臃肿。</p>

<p>如何拆分：根据业务特性把应用拆分；根据功能拆分，但要解决代码复用的问题。</p>

<p>8、服务化结构
服务化好处：</p>

<p>应用立体化，可以把应用分为三层：Web系统，完成不同的业务处理，中间是多个服务中心，不同的服务中心提供不同的服务（例如：商品中心，交易中心，用户中心等），下层是数据库；
共享代码不在散落在不同的应用，而是放在各个服务中心。
业务功能访问不在受限于单机，引入了远程服务调用。
服务中心和数据库交互，前端web专注于和浏览器的交互。</p>
]]></content>
		</item>
		
		<item>
			<title>Golang使用jwt做用户身份验证</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-02-24-go-and-jwt/</link>
			<pubDate>Sat, 24 Feb 2018 22:18:08 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-02-24-go-and-jwt/</guid>
			<description>前要：在看本文前，请先看：什么是 JWT &amp;ndash; JSON WEB TOKEN 其中 StandardClaims的解释如下：
type StandardClaims struct { // 接收jwt的一方 Audience string `json:&amp;quot;aud,omitempty&amp;quot;` // jwt的过期时间，这个过期时间必须要大于签发时间 ExpiresAt int64 `json:&amp;quot;exp,omitempty&amp;quot;` // jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 Id string `json:&amp;quot;jti,omitempty&amp;quot;` // jwt的签发时间 IssuedAt int64 `json:&amp;quot;iat,omitempty&amp;quot;` //jwt签发者 Issuer string `json:&amp;quot;iss,omitempty&amp;quot;` // 定义在什么时间之前，该jwt都是不可用的. NotBefore int64 `json:&amp;quot;nbf,omitempty&amp;quot;` // jwt所面向的用户 Subject string `json:&amp;quot;sub,omitempty&amp;quot;` }  如果你要自己复制代码：请不要复制StandardClaims或者注释掉StandardClaims，以下才是完整代码：
一、使用jwt的代码： // 创建自己的Claims type JwtClaims struct { *jwt.StandardClaims //用户编号 Uid int64 Username string } var ( //盐 secret []byte = []byte(&amp;quot;yulibaozi.</description>
			<content type="html"><![CDATA[

<h4 id="前要-在看本文前-请先看-什么是-jwt-json-web-token-https-www-jianshu-com-p-576dbf44b2ae">前要：在看本文前，请先看：什么是 JWT &ndash; <a href="https://www.jianshu.com/p/576dbf44b2ae">JSON WEB TOKEN</a></h4>

<p>其中 StandardClaims的解释如下：</p>

<pre><code>type StandardClaims struct {
// 接收jwt的一方
Audience string `json:&quot;aud,omitempty&quot;`
// jwt的过期时间，这个过期时间必须要大于签发时间
ExpiresAt int64 `json:&quot;exp,omitempty&quot;`
// jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。
Id string `json:&quot;jti,omitempty&quot;`
// jwt的签发时间
IssuedAt int64 `json:&quot;iat,omitempty&quot;`
//jwt签发者
Issuer string `json:&quot;iss,omitempty&quot;`
// 定义在什么时间之前，该jwt都是不可用的.
NotBefore int64 `json:&quot;nbf,omitempty&quot;`
// jwt所面向的用户
Subject string `json:&quot;sub,omitempty&quot;`
}
</code></pre>

<p>如果你要自己复制代码：请不要复制StandardClaims或者注释掉StandardClaims，以下才是完整代码：</p>

<h4 id="一-使用jwt的代码">一、使用jwt的代码：</h4>

<pre><code>// 创建自己的Claims
type JwtClaims struct {
	*jwt.StandardClaims
	//用户编号
	Uid      int64
	Username string
}

var (
	//盐
	secret []byte = []byte(&quot;yulibaozi.com&quot;)
)

// CreateJwtToken 生成一个jwttoken
func CreateJwtToken(id int64, username string) (string, error) {
	// 设置token的到期时间
	expireToken := time.Now().Add(time.Hour * 24).Unix()
	//创建自定义claims
	claims := JwtClaims{
		&amp;jwt.StandardClaims{
			// 定义在什么时间之前，该jwt都是不可用的.
			NotBefore: time.Now().Unix(),
			// jwt的过期时间，这个过期时间必须要大于签发时间
			ExpiresAt: expireToken,
			//jwt签发者
			Issuer: &quot;yulibaozi&quot;,
		},
		id,
		username,
	}
	// 对自定义claims加密,jwt.SigningMethodHS256是加密算法得到第二部分
	token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
	// 给这个token盐加密 第三部分,得到一个完整的三段的加密
	signedToken, err := token.SignedString(secret)
	if err != nil {
		return &quot;&quot;, err
	}
	return signedToken, nil
}

// DestoryJwtToken 删除JwtToken
func DestoryJwtToken() (string, error) {
	claims := JwtClaims{
		&amp;jwt.StandardClaims{
			NotBefore: int64(time.Now().Unix() - 99998),
			ExpiresAt: int64(time.Now().Unix() - 99999),
			Issuer:    &quot;yulibaozi&quot;,
		},
		-1,
		&quot;&quot;,
	}
	// 对自定义claims加密,jwt.SigningMethodHS256是加密算法得到第二部分
	token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)
	// 给这个token盐加密 第三部分,得到一个完整的三段的加密
	signedToken, err := token.SignedString(secret)
	if err != nil {
		return &quot;&quot;, err
	}
	return signedToken, nil
}

// VerifyToken 得到一个JwtToken,然后验证是否合法,防止伪造
func VerifyJwtToken(jwtToken string) bool {
	// 解析、验证并返回一个令牌。keyFunc将收到解析令牌,应该返回验证的关键。如果一切都是干净的,error是空
	_, err := jwt.Parse(jwtToken, func(*jwt.Token) (interface{}, error) {
		return secret, nil
	})
	if err != nil {
		fmt.Println(&quot;解析jwtToken失败.&quot;, err)
		return false
	}
	return true
}

// ParseJwtToken 解析token得到是自己创建的Claims
func ParseJwtToken(jwtToken string) (*JwtClaims, error) {
	var jwtclaim = &amp;JwtClaims{}
	_, err := jwt.ParseWithClaims(jwtToken, jwtclaim, func(*jwt.Token) (interface{}, error) {
		//得到盐
		return secret, nil
	})
	if err != nil {
		fmt.Println(&quot;解析jwtToken失败.&quot;, err)
		return nil, errors.New(&quot;解析jwtToken失败&quot;)
	}
	return jwtclaim, nil
}

</code></pre>

<h4 id="二-在控制器中使用">二、在控制器中使用：</h4>

<ol>
<li><p>生成jwttoken:</p>

<pre><code>// 生成用户验证的jwttoken
jwttoken, err := ut.CreateJwtToken(id, username)
if err != nil {
	flash.Error(&quot;未知错误2&quot;)
	flash.Store(&amp;li.Controller)
	li.Redirect(&quot;/loginp&quot;, 302)
	return
}
</code></pre></li>

<li><p>删除jwttoken</p>

<pre><code>cookie := http.Cookie{Name: &quot;Authorization&quot;, Value: &quot;&quot;, Path: &quot;/&quot;, MaxAge: -1}
http.SetCookie(li.Ctx.ResponseWriter, &amp;cookie)
li.Redirect(&quot;/loginp&quot;, 302)
</code></pre></li>
</ol>
]]></content>
		</item>
		
		<item>
			<title>Golang 的值传递和引用传递以及何时使用指针</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-02-24-reference-and-value/</link>
			<pubDate>Sat, 24 Feb 2018 22:14:06 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-02-24-reference-and-value/</guid>
			<description>值传递、引用传递、指针传递的区别？  值传递： 形参是实参的拷贝，改变形参的值并不会影响外部实参的值。从被调用函数的角度来说，值传递是单向的（实参-&amp;gt;形参），参数的值只能传入，不能传出。当函数内部需要修改参数，并且不希望这个改变影响调用者时，采用值传递。
 指针传递： 一个变量（a）存储是另外一个变量（b）的地址，而获取另外一个变量的值需要用*当前变量名(*a)，需要注意是a有自己独立的地址，如果改变了这个*a的内容也会改变指向的变量的值。
 引用传递： 引用传递和值传递的共同点是：对形式变量的修改会影响到实际变量的值修改。一般适试用于：函数内部修改能够影响到外部调用者的修改。
 引用传递和值传递的区别是： 在引用传递中，形式变量的地址和实际变量的地址是一样的，而在go中不存在两个变量的地址一样的情况。
  Golang何时使用指针？ 一个函数何时该用指针类型做receiver对初学者而言一直是个头疼的问题。如果不知道该如何取舍，选择指针类型的receiver。但有些时候value receiver更加合适，比如对象是一些轻量级的不变的structs，使用value receiver会更加高效。下面是列举了一些常用的判断指导。
 如果receiver是map、func或者chan，不要使用指针 如果receiver是slice并且该函数并不会修改此slice，不要使用指针 如果该函数会修改receiver，此时一定要用指针 如果receiver是struct并且包含互斥类型sync.Mutex，或者是类似的同步变量，receiver必须是指针，这样可以避免对象拷贝 如果receiver是较大的struct或者array，使用指针则更加高效。多大才算大？假设struct内所有成员都要作为函数变量传进去，如果觉得这时数据太多，就是struct太大 如果receiver是struct，array或者slice，并且其中某个element指向了某个可变量，则这个时候receiver选指针会使代码的意图更加明显 如果receiver使较小的struct或者array，并且其变量都是些不变量、常量，例如time.Time，value receiver更加适合，因为value receiver可以减少需要回收的垃圾量。 最后，如果不确定用哪个，使用指针类的receiver  关于go的引用传递的问题？  It is not possible to create a Go program where two variables share the same storage location in memory. It is possible to create two variables whose contents point to the same storage location, but that is not the same thing as two variables who share the same storage location.</description>
			<content type="html"><![CDATA[

<h4 id="值传递-引用传递-指针传递的区别">值传递、引用传递、指针传递的区别？</h4>

<ol>
<li><p>值传递：
形参是实参的拷贝，改变形参的值并不会影响外部实参的值。从被调用函数的角度来说，值传递是单向的（实参-&gt;形参），参数的值只能传入，不能传出。当函数内部需要修改参数，并且不希望这个改变影响调用者时，采用值传递。</p></li>

<li><p>指针传递：
一个变量（a）存储是另外一个变量（b）的地址，而获取另外一个变量的值需要用*当前变量名(*a)，需要注意是a有自己独立的地址，如果改变了这个*a的内容也会改变指向的变量的值。</p></li>

<li><p>引用传递：
引用传递和值传递的共同点是：对形式变量的修改会影响到实际变量的值修改。一般适试用于：函数内部修改能够影响到外部调用者的修改。</p></li>

<li><p>引用传递和值传递的区别是：
在引用传递中，形式变量的地址和实际变量的地址是一样的，而在go中不存在两个变量的地址一样的情况。</p></li>
</ol>

<h4 id="golang何时使用指针">Golang何时使用指针？</h4>

<p>一个函数何时该用指针类型做receiver对初学者而言一直是个头疼的问题。如果不知道该如何取舍，选择指针类型的receiver。但有些时候value receiver更加合适，比如对象是一些轻量级的不变的structs，使用value receiver会更加高效。下面是列举了一些常用的判断指导。</p>

<ol>
<li>如果receiver是map、func或者chan，不要使用指针</li>
<li>如果receiver是slice并且该函数并不会修改此slice，不要使用指针</li>
<li>如果该函数会修改receiver，此时一定要用指针</li>
<li>如果receiver是struct并且包含互斥类型sync.Mutex，或者是类似的同步变量，receiver必须是指针，这样可以避免对象拷贝</li>
<li>如果receiver是较大的struct或者array，使用指针则更加高效。多大才算大？假设struct内所有成员都要作为函数变量传进去，如果觉得这时数据太多，就是struct太大</li>
<li>如果receiver是struct，array或者slice，并且其中某个element指向了某个可变量，则这个时候receiver选指针会使代码的意图更加明显</li>
<li>如果receiver使较小的struct或者array，并且其变量都是些不变量、常量，例如time.Time，value receiver更加适合，因为value receiver可以减少需要回收的垃圾量。</li>
<li>最后，如果不确定用哪个，使用指针类的receiver</li>
</ol>

<h4 id="关于go的引用传递的问题">关于go的引用传递的问题？</h4>

<blockquote>
<p>It is not possible to create a Go program where two variables share the same storage location in memory. It is possible to create two variables whose contents point to the same storage location, but that is not the same thing as two variables who share the same storage location.</p>
</blockquote>

<p>在Go中，是不可能存在两个变量有相同的内存地址（想想c里面的引用传递是不是两个变量可以出现相同的地址的情况）；但是可以存在两个变量同时指向同一个地址的情况，这两个指向同一个内存地址的变量并不是同一个变量，他们只是指向同一个内存地址（这个时候两个变量有自己独立的地址，想想C中的指针传递，是不是满足这点）。</p>

<p>本文属于转载和实践的总结，如果你需要通透的理解，建议按照先后顺序阅读，效果更佳：</p>

<p>1、<a href="http://www.cnblogs.com/yjkai/archive/2011/04/17/2018647.html">C++ 值传递、指针传递、引用传递的区别</a></p>

<p>2、<a href="https://dave.cheney.net/2017/04/29/there-is-no-pass-by-reference-in-go">Go没有引用传值</a></p>

<p>3、<a href="http://zpjiang.me/2016/08/22/when-to-use-pointer-in-golang/">Golang何时该使用指针</a></p>
]]></content>
		</item>
		
		<item>
			<title>Golang两种方式反转二叉树（镜像二叉树）</title>
			<link>https://yulibaozi.com/posts/go/algorithm/2018-02-24-mirror-binary-tree/</link>
			<pubDate>Sat, 24 Feb 2018 22:08:27 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/algorithm/2018-02-24-mirror-binary-tree/</guid>
			<description>递归的方式 // Mirror 反转(镜像)二叉树--递归的方式 func (t *TNode) Mirror(root *TNode) { if root == nil { return } if root.left == nil &amp;amp;&amp;amp; root.right == nil { return } tem := root.left root.left = root.right root.right = tem t.Mirror(root.left) t.Mirror(root.right) }  队列的方式 func (t *TNode) MirrorByQue(root *TNode) { if root == nil { return } list := list.New() list.PushBack(root) for list.Len() != 0 { // 删除队列中的第一个变量 // delnode:= list.Remove(list.Front()) delnode := (list.</description>
			<content type="html"><![CDATA[

<h3 id="递归的方式">递归的方式</h3>

<pre><code>// Mirror 反转(镜像)二叉树--递归的方式
func (t *TNode) Mirror(root *TNode) {
	if root == nil {
		return
	}
	if root.left == nil &amp;&amp; root.right == nil {
		return
	}
	tem := root.left
	root.left = root.right
	root.right = tem
	t.Mirror(root.left)
	t.Mirror(root.right)
}
</code></pre>

<h3 id="队列的方式">队列的方式</h3>

<pre><code>func (t *TNode) MirrorByQue(root *TNode) {
	if root == nil {
		return
	}
	list := list.New()
	list.PushBack(root)
	for list.Len() != 0 {
		// 删除队列中的第一个变量 
		// delnode:= list.Remove(list.Front())
		delnode := (list.Remove(list.Front())).(*TNode)
		if delnode == nil {
			fmt.Println(&quot;断言并转换失败&quot;)
		}
		tem := delnode.left
		delnode.left = delnode.right
		delnode.right = tem
		if delnode.right != nil {
			list.PushBack(delnode.right)
		}
		if delnode.left != nil {
			list.PushBack(delnode.left)
		}
	}
}
</code></pre>

<h3 id="完整代码">完整代码</h3>

<pre><code>
type TNode struct {
	//节点的值
	val int
	//左节点
	left *TNode
	//由节点
	right *TNode
	//根节点
	root *TNode
}

func main() {
	var tree = TNode{}
	val := []int{10, 6, 15, 4, 8, 11, 16, 3, 5}
	for _, v := range val {

		tree.CreateTree(v)

	}
	fmt.Println(&quot;反转前遍历：&quot;)
	fmt.Println()
	tree.ListTree(tree.root)
	fmt.Println()
	fmt.Println(&quot;反转后遍历&quot;)

	tree.Mirror(tree.root)
	tree.ListTree(tree.root)
	fmt.Println()
	fmt.Println(&quot;再次反转后遍历:&quot;)

	tree.MirrorByQue(tree.root)
	tree.ListTree(tree.root)

}

//CreateTree 创建二叉树
func (t *TNode) CreateTree(val int) {
	node := &amp;TNode{}
	node.val = val
	if t.root == nil {
		t.root = node
	} else {
		backNode := &amp;TNode{}
		currentNode := t.root
		for currentNode != nil {
			backNode = currentNode
			if currentNode.val &gt; val {
				currentNode = currentNode.left
			} else {
				currentNode = currentNode.right
			}
		}
		if backNode.val &gt; val {
			backNode.left = node
		} else {
			backNode.right = node
		}
	}
}

// ListTree 递归版前中后序遍历二叉树
func (t *TNode) ListTree(root *TNode) {
	if root != nil {
		fmt.Print(&quot; &quot;, root.val)
		t.ListTree(root.left)
		t.ListTree(root.right)
	}
}

// Mirror 反转(镜像)二叉树--递归的方式
func (t *TNode) Mirror(root *TNode) {
	if root == nil {
		return
	}
	if root.left == nil &amp;&amp; root.right == nil {
		return
	}
	tem := root.left
	root.left = root.right
	root.right = tem
	t.Mirror(root.left)
	t.Mirror(root.right)
}

// MirrorByQue 反转(镜像)二叉树--队列的方式
func (t *TNode) MirrorByQue(root *TNode) {
	if root == nil {
		return
	}
	list := list.New()
	list.PushBack(root)
	for list.Len() != 0 {
		// 删除队列中的第一个变量
		// delnode:= list.Remove(list.Front())
		delnode := (list.Remove(list.Front())).(*TNode)
		if delnode == nil {
			fmt.Println(&quot;断言并转换失败&quot;)
		}
		tem := delnode.left
		delnode.left = delnode.right
		delnode.right = tem
		if delnode.right != nil {
			list.PushBack(delnode.right)
		}
		if delnode.left != nil {
			list.PushBack(delnode.left)
		}
	}
}

</code></pre>

<h3 id="结果">结果</h3>

<pre><code>反转前遍历：
10 6 4 3 5 8 15 11 16
反转后遍历
10 15 16 11 6 8 4 5 3
再次反转后遍历:
10 6 4 3 5 8 15 11 16
</code></pre>

<p><img src="http://index.yulibaozi.com/a6fa6ddf-ee00-420a-8d1d-8d8a479b6af5erchash.jpg" alt="image" /></p>
]]></content>
		</item>
		
		<item>
			<title>invalid memory address or nil pointer dereference</title>
			<link>https://yulibaozi.com/posts/go/knowledge/2018-02-24-panic-by-return-funcs/</link>
			<pubDate>Sat, 24 Feb 2018 20:56:35 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/knowledge/2018-02-24-panic-by-return-funcs/</guid>
			<description>以下是出现问题的原因之一：返回值表现的方式理解不透彻导致了这个问题
返回值返回方式一：
func (h *HTTPInterface) Run(item *task.Task) (*task.TaskLog, error) {  返回值返回方式二：
func (f *FUNCInterface) Run(item *task.Task) (taskLog *task.TaskLog, err error) {    对于这两种返回方式来说，各有各的好处：   对于方式一来说，代码是不容易出现invalid memory address or nil pointer dereference错误的，因为你要返回，你肯定需要创建。 对于方式二来说，如果你不能充分理解返回字段名的意义，那么请你绝对不要这样用，因为即使代码没有报错，你也为后来的人留下了坑。当然，他的好处在于代码非常简洁。 两种方式代码的对比： 请仔细看上面的代码片段，这对你的理解非常重要。
代码片段中的第一个函数就是我们说的第一种方式，代码片段中的第二个函数就是我们说的第二种方式，你不必纠结代码很JAVA风，这不是重点。
  两者的区别：   对于第一个函数来说，如果调用方即使err!=nil 你仍然可以使用返回的对象，但是上面这个片段不行，因为返回了nil，如果要改上面的代码的话，你可以返回一个空对象（new创建的对象）。 对于第二个函数来说，如果err!=nil，那么你是绝对不能使用返回值的任何对象的，这个是约定好的，因为如果用了会报invalid memory address or nil pointer dereference，所以，当你在忽略错误，或者是说即使是出现错误也要操作返回对象的时候，第二种方式是不可取的， 最后：如果你不懂第二种的含义，而一直野蛮的使用，我很无语。
出现invalid memory address or nil pointer dereference的第二种情况：
不清楚 :=作用域的情况，可以参照：一个有关Golang变量作用域的坑
最后致谢：Golang隐修会(Go语言)的@wangsir和@K.O Angel™ 对问题的讨论。</description>
			<content type="html"><![CDATA[<p>以下是出现问题的原因之一：返回值表现的方式理解不透彻导致了这个问题</p>

<p>返回值返回方式一：</p>

<pre><code>func (h *HTTPInterface) Run(item *task.Task) (*task.TaskLog, error) {
</code></pre>

<p>返回值返回方式二：</p>

<pre><code>func (f *FUNCInterface) Run(item *task.Task) (taskLog *task.TaskLog, err error) {
</code></pre>

<blockquote>
<ol>
<li>对于这两种返回方式来说，各有各的好处：</li>
</ol>
</blockquote>

<p>对于方式一来说，代码是不容易出现invalid memory address or nil pointer dereference错误的，因为你要返回，你肯定需要创建。
对于方式二来说，如果你不能充分理解返回字段名的意义，那么请你绝对不要这样用，因为即使代码没有报错，你也为后来的人留下了坑。当然，他的好处在于代码非常简洁。
两种方式代码的对比：
<img src="http://index.yulibaozi.com/f6d5d61a-5146-4e42-81a3-61563090b98ede-1024x261.jpg" alt="image" /></p>

<p>请仔细看上面的代码片段，这对你的理解非常重要。</p>

<p>代码片段中的第一个函数就是我们说的第一种方式，代码片段中的第二个函数就是我们说的第二种方式，你不必纠结代码很JAVA风，这不是重点。</p>

<blockquote>
<ol>
<li>两者的区别：</li>
</ol>
</blockquote>

<p>对于第一个函数来说，如果调用方即使err!=nil 你仍然可以使用返回的对象，但是上面这个片段不行，因为返回了nil，如果要改上面的代码的话，你可以返回一个空对象（new创建的对象）。
对于第二个函数来说，如果err!=nil，那么你是绝对不能使用返回值的任何对象的，这个是约定好的，因为如果用了会报invalid memory address or nil pointer dereference，所以，当你在忽略错误，或者是说即使是出现错误也要操作返回对象的时候，第二种方式是不可取的，
最后：如果你不懂第二种的含义，而一直野蛮的使用，我很无语。</p>

<p>出现invalid memory address or nil pointer dereference的第二种情况：</p>

<p>不清楚 :=作用域的情况，可以参照：<a href="http://tonybai.com/2015/01/13/a-hole-about-variable-scope-in-golang/">一个有关Golang变量作用域的坑</a></p>

<p>最后致谢：Golang隐修会(Go语言)的@wangsir和@K.O Angel™ 对问题的讨论。</p>
]]></content>
		</item>
		
		<item>
			<title>三向切分快速排序</title>
			<link>https://yulibaozi.com/posts/go/algorithm/2018-02-23-three-way-split-quick-sort/</link>
			<pubDate>Fri, 23 Feb 2018 20:19:49 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/go/algorithm/2018-02-23-three-way-split-quick-sort/</guid>
			<description>封装成函数：
//三向切分快速排序 func ThreeWayQuickSort(s []int) { sort3way(s, 0, len(s)-1) } //在lt之前的(lo~lt-1)都小于中间值 //在gt之前的(gt+1~hi)都大于中间值 //在lt~i-1的都等于中间值 //在i~gt的都还不确定（最终i会大于gt，即不确定的将不复存在） func sort3way(s []int, lo, hi int) { if lo &amp;gt;= hi { return } v, lt, i, gt := s[lo], lo, lo+1, hi for i &amp;lt;= gt { if s[i] &amp;lt; v { swap(s, i, lt) lt++ i++ } else if s[i] &amp;gt; v { swap(s, i, gt) gt-- } else { i++ } } sort3way(s, lo, lt-1) sort3way(s, gt+1, hi) } func swap(s []int, i int, j int) { s[i], s[j] = s[j], s[i] } 测试：</description>
			<content type="html"><![CDATA[<p>封装成函数：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="c1">//三向切分快速排序
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">ThreeWayQuickSort</span><span class="p">(</span><span class="nx">s</span> <span class="p">[]</span><span class="kt">int</span><span class="p">)</span> <span class="p">{</span>
	<span class="nf">sort3way</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="p">}</span>
 
<span class="c1">//在lt之前的(lo~lt-1)都小于中间值
</span><span class="c1">//在gt之前的(gt+1~hi)都大于中间值
</span><span class="c1">//在lt~i-1的都等于中间值
</span><span class="c1">//在i~gt的都还不确定（最终i会大于gt，即不确定的将不复存在）
</span><span class="c1"></span><span class="kd">func</span> <span class="nf">sort3way</span><span class="p">(</span><span class="nx">s</span> <span class="p">[]</span><span class="kt">int</span><span class="p">,</span> <span class="nx">lo</span><span class="p">,</span> <span class="nx">hi</span> <span class="kt">int</span><span class="p">)</span> <span class="p">{</span>
	<span class="k">if</span> <span class="nx">lo</span> <span class="o">&gt;=</span> <span class="nx">hi</span> <span class="p">{</span>
		<span class="k">return</span>
	<span class="p">}</span>
	<span class="nx">v</span><span class="p">,</span> <span class="nx">lt</span><span class="p">,</span> <span class="nx">i</span><span class="p">,</span> <span class="nx">gt</span> <span class="o">:=</span> <span class="nx">s</span><span class="p">[</span><span class="nx">lo</span><span class="p">],</span> <span class="nx">lo</span><span class="p">,</span> <span class="nx">lo</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nx">hi</span>
	<span class="k">for</span> <span class="nx">i</span> <span class="o">&lt;=</span> <span class="nx">gt</span> <span class="p">{</span>
		<span class="k">if</span> <span class="nx">s</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">&lt;</span> <span class="nx">v</span> <span class="p">{</span>
			<span class="nf">swap</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span> <span class="nx">i</span><span class="p">,</span> <span class="nx">lt</span><span class="p">)</span>
			<span class="nx">lt</span><span class="o">++</span>
			<span class="nx">i</span><span class="o">++</span>
		<span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="nx">s</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">&gt;</span> <span class="nx">v</span> <span class="p">{</span>
			<span class="nf">swap</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span> <span class="nx">i</span><span class="p">,</span> <span class="nx">gt</span><span class="p">)</span>
			<span class="nx">gt</span><span class="o">--</span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="nx">i</span><span class="o">++</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="nf">sort3way</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span> <span class="nx">lo</span><span class="p">,</span> <span class="nx">lt</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
	<span class="nf">sort3way</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span> <span class="nx">gt</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="nx">hi</span><span class="p">)</span>
<span class="p">}</span>
 
<span class="kd">func</span> <span class="nf">swap</span><span class="p">(</span><span class="nx">s</span> <span class="p">[]</span><span class="kt">int</span><span class="p">,</span> <span class="nx">i</span> <span class="kt">int</span><span class="p">,</span> <span class="nx">j</span> <span class="kt">int</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">s</span><span class="p">[</span><span class="nx">i</span><span class="p">],</span> <span class="nx">s</span><span class="p">[</span><span class="nx">j</span><span class="p">]</span> <span class="p">=</span> <span class="nx">s</span><span class="p">[</span><span class="nx">j</span><span class="p">],</span> <span class="nx">s</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span>
<span class="p">}</span></code></pre></div>
<p>测试：</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="nx">s</span> <span class="o">:=</span> <span class="p">[]</span><span class="kt">int</span><span class="p">{</span><span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">}</span>
<span class="nx">fmt</span><span class="p">.</span><span class="nf">Println</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span>
<span class="nf">ThreeWayQuickSort</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span>
<span class="nx">fmt</span><span class="p">.</span><span class="nf">Println</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span></code></pre></div>
<p>输出：</p>

<p>[9 0 6 5 8 2 1 7 4 3]<br />
[0 1 2 3 4 5 6 7 8 9]</p>
]]></content>
		</item>
		
		<item>
			<title>Docker 使用手册</title>
			<link>https://yulibaozi.com/posts/docker/knowledge/2018-01-24-docker-use/</link>
			<pubDate>Wed, 24 Jan 2018 22:28:04 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/docker/knowledge/2018-01-24-docker-use/</guid>
			<description>docker使用笔记 1、基于dockerfile使用docker build 创建images  格式
 docker build . -t 需要打包成的镜像名称 备注：.代表dockerfile在当前目录,以dockerfile为准   示例
 docker build . -t liimages/spider1.0  2、基于image运行docker容器同时运行指定主机端口  格式
 docker run -d -p 主机端口:容器端口 镜像名称或者Id 备注：-d:后台运行,-p 暴露端口 4000:80的意思是容器的80的端口映射到主机的4000端口 外部以400端口访问   示例
 docker run -d -p 4000:80 liimages/spider1.0  3、基于image运行docker容器同时运行指定主机端口且挂载外部目录，且出错重启  格式
  -v 参数： 冒号前为宿主机目录，必须为绝对路径，冒号后为镜像内挂载的路径。   示例
 docker run --restart=always -itdp 1230:1235 -v /etc/localtime:/etc/localtime:ro liimages/spider2.0 ./run.sh  4、进入到容器内部可以修改相关配置,不使用attach，attach方式因为退出，导致整个容器退出。  格式</description>
			<content type="html"><![CDATA[

<h2 id="docker使用笔记">docker使用笔记</h2>

<h4 id="1-基于dockerfile使用docker-build-创建images">1、基于dockerfile使用docker build 创建images</h4>

<blockquote>
<p>格式</p>
</blockquote>

<pre><code>docker build . -t 需要打包成的镜像名称    备注：.代表dockerfile在当前目录,以dockerfile为准

</code></pre>

<blockquote>
<p>示例</p>
</blockquote>

<pre><code>docker build . -t liimages/spider1.0
</code></pre>

<h4 id="2-基于image运行docker容器同时运行指定主机端口">2、基于image运行docker容器同时运行指定主机端口</h4>

<blockquote>
<p>格式</p>
</blockquote>

<pre><code>docker run -d -p 主机端口:容器端口 镜像名称或者Id   
   备注：-d:后台运行,-p 暴露端口
   4000:80的意思是容器的80的端口映射到主机的4000端口 外部以400端口访问

</code></pre>

<blockquote>
<p>示例</p>
</blockquote>

<pre><code>docker run -d -p 4000:80 liimages/spider1.0
</code></pre>

<h4 id="3-基于image运行docker容器同时运行指定主机端口且挂载外部目录-且出错重启">3、基于image运行docker容器同时运行指定主机端口且挂载外部目录，且出错重启</h4>

<blockquote>
<p>格式</p>
</blockquote>

<pre><code> -v 参数：
 冒号前为宿主机目录，必须为绝对路径，冒号后为镜像内挂载的路径。

</code></pre>

<blockquote>
<p>示例</p>
</blockquote>

<pre><code>docker run --restart=always -itdp 1230:1235 -v /etc/localtime:/etc/localtime:ro liimages/spider2.0 ./run.sh
</code></pre>

<h4 id="4-进入到容器内部可以修改相关配置-不使用attach-attach方式因为退出-导致整个容器退出">4、进入到容器内部可以修改相关配置,不使用attach，attach方式因为退出，导致整个容器退出。</h4>

<blockquote>
<p>格式</p>
</blockquote>

<pre><code> docker exec -it 容器名称或者容器Id /bin/sh

</code></pre>

<blockquote>
<p>示例</p>
</blockquote>

<pre><code>docker exec -it bb2 /bin/sh

</code></pre>

<blockquote>
<p>相关连接：<a href="http://blog.csdn.net/halcyonbaby/article/details/46884605">CSDN追寻神迹</a></p>
</blockquote>

<h4 id="4-删除镜像-删除容器是rm">4、删除镜像.删除容器是rm</h4>

<blockquote>
<p>格式</p>
</blockquote>

<pre><code> 格式一：docker rmi 镜像名或者镜像Id
 格式二：docker rmi -f 镜像名或者镜像Id 备注：强制删除与该镜像有关的容器（包括启动中的）和该镜像

</code></pre>

<blockquote>
<p>示例</p>
</blockquote>

<pre><code>docker rmi liimages/spider2.0 
</code></pre>

<h4 id="5-把打包的镜像上传到仓库">5、把打包的镜像上传到仓库</h4>

<blockquote>
<p>格式</p>
</blockquote>

<pre><code>前提1：镜像的名字要符合官方标准
前提2：使用docker login 登录

格式：docker push 镜像名

</code></pre>

<blockquote>
<p>示例</p>

<pre><code>docker push 镜像名
</code></pre>

<p>相关连接</p>
</blockquote>

<p><a href="https://yq.aliyun.com/ziliao/101923?spm=5176.8246799.0.0.1BKYVo">云栖社区PUSH失败解决方案</a></p>

<p><a href="https://stackoverflow.com/questions/41984399/denied-requested-access-to-the-resource-is-denied-docker">stackoverflow错误解决方案</a></p>

<h2 id="docker容器间通信">Docker容器间通信</h2>

<ul>
<li><p>容器每次启动时会分配个一个IP地址，这个IP地址只在宿主主机内部有用，其它主机上的程序无法访问此IP</p></li>

<li><p>一台机器上的docker容器之间默认是可以通过分配的IP进行通信的，可以通过启动参数 -icc=false —iptables=true 来关闭互通，严格隔离以提高安全性</p></li>

<li><p>虽然每次启动分配的IP可能会变，但启动时加类似 —link redis:db 这样的参数给容器起别名，可以起到DNS作用，原理是在 /etc/hosts 里面映射IP到别名，这样你的程序和其他容器通信就可以不用管IP，用别名，IP变但它不会变</p></li>

<li><p>link只支持单主机，跨主机link最早的方案是Ambassador(docker远程代理)，每台主机启动一个Ambassador容器负责对主机上其它容器的网络转发(socket proxy)，后演变成 github.com/gliderlabs/connectable</p></li>

<li><p>手动处理容器间通信很麻烦，一般用编排工具：kubernetes、swarm&hellip;&hellip;</p></li>
</ul>

<h2 id="常用服务的启动命令">常用服务的启动命令</h2>

<h4 id="1-mysql的启动命令">1、mysql的启动命令</h4>

<pre><code>docker run --restart=always --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=mypassword -d docker.io/mysql 启动一个mysql

其中--restart=always是自动启动的意思
</code></pre>

<h3 id="2-redis的启动命令">2、redis的启动命令</h3>

<pre><code>docker run --restart=always  -p 6379:6379 --name redis -d redis redis-server --appendonly yes --requirepass mypassword

其中：--appendonly yes是持久化数据的意思
</code></pre>
]]></content>
		</item>
		
		<item>
			<title>Mysql不同数据类型的差异对比与取舍</title>
			<link>https://yulibaozi.com/posts/mysql/knowledge/2018-01-24-mysql/</link>
			<pubDate>Wed, 24 Jan 2018 20:45:09 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/mysql/knowledge/2018-01-24-mysql/</guid>
			<description>数据库设计  前要：字符串长度不是定义字节数，而是字符数，多字节字符集需要更多的空间存储单个字符。
 字段设计维度的思考 可以按照以下三个维度考虑设计： - 按照字段的使用频率设计，预判性的分为冷、温、热字段设计表 - 根据展示数据展示位置来设计，比如有些数据只在个人中心显示而不会在其他地方显示 - 根据字段的大小设计表，比如，有些大文本 text，blob的类型的可以考虑拆分。 - 按照读／写（更新）的比例来设计表
数据类型的选择  更小的通常更快，性能更佳原因在于占用更少的磁盘，内存，cpu缓存并且需要处理的cpu周期也更小。但是要确认你没有低估存储值的范围 尽量避免NULL，相对于mysql来说，他更难优化，因为NULL的列使得索引、索引统计和值比较都更为复杂。 在mysql中，整型的计算一般使用bigint（64位）来计算，即使是32位的环境也是如此。但是一些聚合函数例外，他们使用decimal和double来计算 decimal类型通常用来存储精确小数，因为他支持高精确计算（5.0及其以后）   索引的建立  尽量避免在可为NULL的列上创建索引  应该使用整型来存储IP地址？ 通常情况下，人们使用varchar（15）来存储IPv4的地址，事实上，他们是32位的无符号整型，而不是字符串，用小数点分割成四部分只是为了让人们简单易懂，所以应该用无符号整型来存储ip地址，mysql同时也提供了INET_ATON（）和INET_NTOA()函数在这两者之间转换。
如何在数据库中存储钱？ 将需要存储的货币单位更加小数的位数乘以相应的倍数即可，假设，我们要存储到万分之一分，那么我们就把所有的金额乘以一百万，将结果存储在bigint中，这样就可以避免浮点存储计算不精确和decimal精确计算代价高的问题。
数据库中DATETIME和TIMESAMP的区别？ 他们都可以存储相同的数据类型：时间和日期，精确到秒。 但是区别： - TIMESAMP只使用DATETIME一半的存储空间，并且会根据时区变化，具有特殊的自动更新能力。 - TIMESAMP允许的时间范围要小很多，所以有时候他上面的特殊能力可能为成为他的障碍。 - TIMESAMP能表示从1970年1月1日午夜以来的秒数到2038年,而DATETIME能保存1001年到9999年，精度为秒。
在数据库中如何保存到微秒级别的时间戳？ 答案是：使用bigint类型存储微秒级别的时间戳也可以切换到mariadb来代替mysql
varchar和char的区别？ varchar：
varchar类型存储的是可变长的字符串，他比定长类型的更节省空间，因为他只用必要的空间，字符越短使用越少的空间，但是有一种情况例外，就是使用ROW_FORMAT=FIXED创建的话，每一行都使用的定长存储，这很浪费空间，另外varchar需要一到两个字节额外记录字符串的长度，如果列小于255个字节，那么使用一个字节，否则使用两个。假设使用latin1字符集，则一个varchar（10）的列需要11个字节的存储空间，而varchar（20000）需要20002个字节，因为需要存储长度信息。
varchar虽然节省了空间，对性能有帮助，但由于是变长的，在update时可能会让行比原来更长，那么这就有额外的工作，这个行占用的空间增长，并且在此页内没有更多的空间存储，这种情况下，不同的存储引擎的处理方式不一样，比如：myisam将拆成不同的片段存储，innodb则需要分裂页使得行可以放在列内，总之，就是加重了碎片化。
char： char是定长的，mysql会根据定义的字符串长度来分配足够的空间，当存储char的时候，mysql会删除末尾的空格，且char值会根据采用空格进行填充
总结： varchar适合 - 字符串列的最大长度比平均长度大很多，列更新很少的情况，这样碎片化不是问题，就像使用utf-8这样复杂的字符集，每个字符度使用不同的字节数进行存储。 - 注意：innodb会把过长的varchar处理成blob
char适合 - 存储很短的字符串或者所有的值接近同一个长度，例如char比较适合存储密码md5值，因为这是一个定长的值。 - 对于经常变更的数据，char也比varchar更好，因为定长的不容易产生碎片。 - 对于非常短的列，char比varchar在存储空间上更有效率，因为varchar会需要额外字节来记录长度。
数据库索引 一、索引的特点  1.索引可以加快数据库的检索速度 2.索引降低了数据库插入、修改、删除等维护任务的速度 3.索引创建在表上，不能创建在视图上 4.索引既可以直接创建，也可以间接创建 5.可以在优化隐藏中，使用索引 6.</description>
			<content type="html"><![CDATA[

<h2 id="数据库设计">数据库设计</h2>

<blockquote>
<p>前要：字符串长度不是定义字节数，而是字符数，多字节字符集需要更多的空间存储单个字符。</p>
</blockquote>

<h4 id="字段设计维度的思考">字段设计维度的思考</h4>

<p>可以按照以下三个维度考虑设计：
- 按照字段的使用频率设计，预判性的分为冷、温、热字段设计表
- 根据展示数据展示位置来设计，比如有些数据只在个人中心显示而不会在其他地方显示
- 根据字段的大小设计表，比如，有些大文本 text，blob的类型的可以考虑拆分。
- 按照读／写（更新）的比例来设计表</p>

<h4 id="数据类型的选择">数据类型的选择</h4>

<ul>
<li>更小的通常更快，性能更佳原因在于占用更少的磁盘，内存，cpu缓存并且需要处理的cpu周期也更小。但是要确认你没有低估存储值的范围</li>
<li>尽量避免NULL，相对于mysql来说，他更难优化，因为NULL的列使得索引、索引统计和值比较都更为复杂。</li>
<li>在mysql中，整型的计算一般使用bigint（64位）来计算，即使是32位的环境也是如此。但是一些聚合函数例外，他们使用decimal和double来计算</li>
<li>decimal类型通常用来存储精确小数，因为他支持高精确计算（5.0及其以后）</li>
<li></li>
</ul>

<h4 id="索引的建立">索引的建立</h4>

<ul>
<li>尽量避免在可为NULL的列上创建索引</li>
</ul>

<h4 id="应该使用整型来存储ip地址">应该使用整型来存储IP地址？</h4>

<p>通常情况下，人们使用varchar（15）来存储IPv4的地址，事实上，他们是32位的无符号整型，而不是字符串，用小数点分割成四部分只是为了让人们简单易懂，所以应该用无符号整型来存储ip地址，mysql同时也提供了INET_ATON（）和INET_NTOA()函数在这两者之间转换。</p>

<h4 id="如何在数据库中存储钱">如何在数据库中存储钱？</h4>

<p>将需要存储的货币单位更加小数的位数乘以相应的倍数即可，假设，我们要存储到万分之一分，那么我们就把所有的金额乘以一百万，将结果存储在bigint中，这样就可以避免浮点存储计算不精确和decimal精确计算代价高的问题。</p>

<h4 id="数据库中datetime和timesamp的区别">数据库中DATETIME和TIMESAMP的区别？</h4>

<p>他们都可以存储相同的数据类型：时间和日期，精确到秒。
但是区别：
- TIMESAMP只使用DATETIME一半的存储空间，并且会根据时区变化，具有特殊的自动更新能力。
-  TIMESAMP允许的时间范围要小很多，所以有时候他上面的特殊能力可能为成为他的障碍。
-  TIMESAMP能表示从1970年1月1日午夜以来的秒数到2038年,而DATETIME能保存1001年到9999年，精度为秒。</p>

<h4 id="在数据库中如何保存到微秒级别的时间戳">在数据库中如何保存到微秒级别的时间戳？</h4>

<p>答案是：使用bigint类型存储微秒级别的时间戳也可以切换到mariadb来代替mysql</p>

<h3 id="varchar和char的区别">varchar和char的区别？</h3>

<p>varchar：</p>

<p>varchar类型存储的是可变长的字符串，他比定长类型的更节省空间，因为他只用必要的空间，字符越短使用越少的空间，但是有一种情况例外，就是使用ROW_FORMAT=FIXED创建的话，每一行都使用的定长存储，这很浪费空间，另外varchar需要一到两个字节额外记录字符串的长度，如果列小于255个字节，那么使用一个字节，否则使用两个。假设使用latin1字符集，则一个varchar（10）的列需要11个字节的存储空间，而varchar（20000）需要20002个字节，因为需要存储长度信息。</p>

<p>varchar虽然节省了空间，对性能有帮助，但由于是变长的，在update时可能会让行比原来更长，那么这就有额外的工作，这个行占用的空间增长，并且在此页内没有更多的空间存储，这种情况下，不同的存储引擎的处理方式不一样，比如：myisam将拆成不同的片段存储，innodb则需要分裂页使得行可以放在列内，总之，就是加重了碎片化。</p>

<p>char：
char是定长的，mysql会根据定义的字符串长度来分配足够的空间，当存储char的时候，mysql会删除末尾的空格，且char值会根据采用空格进行填充</p>

<p>总结：
varchar适合
- 字符串列的最大长度比平均长度大很多，列更新很少的情况，这样碎片化不是问题，就像使用utf-8这样复杂的字符集，每个字符度使用不同的字节数进行存储。
- 注意：innodb会把过长的varchar处理成blob</p>

<p>char适合
- 存储很短的字符串或者所有的值接近同一个长度，例如char比较适合存储密码md5值，因为这是一个定长的值。
- 对于经常变更的数据，char也比varchar更好，因为定长的不容易产生碎片。
- 对于非常短的列，char比varchar在存储空间上更有效率，因为varchar会需要额外字节来记录长度。</p>

<h2 id="数据库索引">数据库索引</h2>

<h3 id="一-索引的特点">一、索引的特点</h3>

<ul>
<li>1.索引可以加快数据库的检索速度</li>
<li>2.索引降低了数据库插入、修改、删除等维护任务的速度</li>
<li>3.索引创建在表上，不能创建在视图上</li>
<li>4.索引既可以直接创建，也可以间接创建</li>
<li>5.可以在优化隐藏中，使用索引</li>

<li><p>6.使用查询处理器执行SQL语句，在一个表上，一次只能使用一个索引</p>

<h3 id="二-使用策略">二、使用策略</h3></li>

<li><p>如果经常需要同时对两个字段进行AND查询,那么使用两个单独索引不如建立一个复合索引，因为两个单独索引通常数据库只能使用其中一个，而使用复合索引因为索引本身就对应到两个字段上的，效率会有很大提高。</p></li>

<li><p>聚集索引通常速度优于非聚集索引</p></li>

<li><p>建索引时应考虑是否有足够的空间。索引占据空间,平均约1.2倍数据库本身大小。</p></li>

<li><p>在经常用于查询或聚合条件的字段上建立聚集索引。这类查询条件包括 between, &gt;, &lt;,group by, max,min, count等。</p></li>

<li><p>在值高度的具有唯一性字段上建立索引。不能在诸如性别的字段上建立索引。</p></li>

<li><p>只有作为索引的第一个列包含在查询条件中，该索引才的作用。</p></li>

<li><p>每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。</p></li>

<li><p>删除一直不用的索引。特别是对于删除和修改比较频繁的数据表，必须考虑如何精华索引。</p></li>
</ul>

<h3 id="最后">最后：</h3>

<pre><code>B+树最常用，性能也不差，用于范围查询和单值查询都可以。特别是范围查询，非得用B+树这种顺序的才可以了。
HASH的如果只是对单值查询的话速度会比B+树快一点，但是ORACLE好像不支持HASH索引，只支持HASH表空间。
位图的使用情况很局限，只有很少的情况才能用，一定要确定真正适合使用这种索引才用（值的类型很少并且需要复合查询），否则建立一大堆位图就一点意义都没有了。
</code></pre>
]]></content>
		</item>
		
		<item>
			<title>Nginx日常配置</title>
			<link>https://yulibaozi.com/posts/nginx/2018-01-24-nginx/</link>
			<pubDate>Wed, 24 Jan 2018 20:42:38 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/nginx/2018-01-24-nginx/</guid>
			<description>性能部分 1、worker_processes  表示工作进程的数量，一般设置成CPU核的数量即可
 2、worker_connections  表示每个工作进程的并发连接数，默认设置为1024
 worker_processes 1; worker_connections 1024;  3、Buffers  如果Buffer太小，Nginx会不停的写一些临时文件； client_body_buffer_size 允许客户端请求单个文件字节数； client_header_buffer_size 用于设置客户请求的Header头缓冲区大小； client_max_body_size 设置客户端能上传文件的大小，默认为1m； large_client_header_buffers 设置客户端请求的Header头缓冲大小
 client_body_buffer_size 10K; client_header_buffer_size 1k; client_max_body_size 8m; large_client_header_buffers 2 1k;  4、Timeouts  client_header_timeout 和 client_body_timeout设置请求头和请求体(各自)的超时时间，如果没有发送请求头和请求体，Nginx服务器会返回408错误或者request time out。
keepalive_timeout 给客户端分配keep-alive链接超时时间。服务器将在这个超时时间过后关闭链接，我们将它设置低些可以让Nginx持续工作的时间更长。
send_timeout 指定客户端的响应超时时间。这个设置不会用于整个转发器，而是在两次客户端读取操作之间。如果在这段时间内，客户端没有读取任何数据，Nginx就会关闭连接
 client_body_timeout 12; client_header_timeout 12; keepalive_timeout 15; send_timeout 10;  5、Gzip Compression  开启Gzip,可以帮助nginx减少大量的网络传输工作，需要注意：gzip_comp_level的设置，如果太高，Nginx服务会浪费CPU的执行周期。
 gzip on; gzip_vary on; gzip_comp_level 2; gzip_buffers 16 8k; gzip_min_length 1000; gzip_proxied any; gzip_disable &amp;quot;msie6&amp;quot;; gzip_http_version 1.</description>
			<content type="html"><![CDATA[

<h3 id="性能部分">性能部分</h3>

<h4 id="1-worker-processes">1、worker_processes</h4>

<blockquote>
<p>表示工作进程的数量，一般设置成CPU核的数量即可</p>
</blockquote>

<h4 id="2-worker-connections">2、worker_connections</h4>

<blockquote>
<p>表示每个工作进程的并发连接数，默认设置为1024</p>
</blockquote>

<pre><code>worker_processes 1;
worker_connections 1024;
</code></pre>

<h4 id="3-buffers">3、Buffers</h4>

<blockquote>
<p>如果Buffer太小，Nginx会不停的写一些临时文件；
client_body_buffer_size 允许客户端请求单个文件字节数；
client_header_buffer_size 用于设置客户请求的Header头缓冲区大小；
client_max_body_size 设置客户端能上传文件的大小，默认为1m；
large_client_header_buffers 设置客户端请求的Header头缓冲大小</p>
</blockquote>

<pre><code>client_body_buffer_size 10K;
client_header_buffer_size 1k;
client_max_body_size 8m;
large_client_header_buffers 2 1k;
</code></pre>

<h4 id="4-timeouts">4、Timeouts</h4>

<blockquote>
<p>client_header_timeout 和 client_body_timeout设置请求头和请求体(各自)的超时时间，如果没有发送请求头和请求体，Nginx服务器会返回408错误或者request time out。<br />
keepalive_timeout 给客户端分配keep-alive链接超时时间。服务器将在这个超时时间过后关闭链接，我们将它设置低些可以让Nginx持续工作的时间更长。<br />
send_timeout 指定客户端的响应超时时间。这个设置不会用于整个转发器，而是在两次客户端读取操作之间。如果在这段时间内，客户端没有读取任何数据，Nginx就会关闭连接</p>
</blockquote>

<pre><code>client_body_timeout 12;
client_header_timeout 12;
keepalive_timeout 15;
send_timeout 10;
</code></pre>

<h4 id="5-gzip-compression">5、Gzip Compression</h4>

<blockquote>
<p>开启Gzip,可以帮助nginx减少大量的网络传输工作，需要注意：gzip_comp_level的设置，如果太高，Nginx服务会浪费CPU的执行周期。</p>
</blockquote>

<pre><code>gzip             on;
gzip_vary        on;             
gzip_comp_level  2;
gzip_buffers     16 8k;
gzip_min_length  1000;
gzip_proxied     any;
gzip_disable     &quot;msie6&quot;;
gzip_http_version  1.0;
gzip_proxied     expired no-cache no-store private auth;
gzip_types       text/plain application/x-javascript text/xml text/css application/xml text/javascript application/json application/javascript;
</code></pre>

<h4 id="6-static-file-caching">6、Static File Caching</h4>

<blockquote>
<p>静态文件的缓存</p>

<pre><code>location ~*\.(jpg|jpeg|png|gif|ico|css|js)$ {
    expires 365d;
}
</code></pre>
</blockquote>

<h4 id="7-日志">7、日志</h4>

<blockquote>
<p>access_log 设置Nginx是否将存储访问的日志，关闭这个选择可以让磁盘的IO操作更快。</p>

<pre><code>access_log off;
</code></pre>
</blockquote>

<h3 id="https部分">HTTPS部分</h3>

<h4 id="1-基本的https配置">1、基本的https配置</h4>

<blockquote>
<p>配置文件如下：</p>
</blockquote>

<pre><code>
listen  443;
ssl     on;

ssl_certificate  /etc/nginx/conf.d/cert/niu.pem;
ssl_certificate_key /etc/nginx/conf.d/cert/niu.key;

ssl_session_timeout 60m;
ssl_session_cache shared:SSL:10m;
ssl_buffer_size 8k;
ssl_session_tickets on;
ssl_stapling on;
ssl_stapling_verify on;


ssl_protocols   TLSv1 TLSv1.1 TLSv1.2;
ssl_ciphers ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-RC4-SHA:!ECDHE-RSA-RC4-SHA:ECDH-ECDSA-RC4-SHA:ECDH-RSA-RC4-SHA:ECDHE-RSA-AES256-SHA:!RC4-SHA:HIGH:!aNULL:!eNULL:!LOW:!3DES:!MD5:!EXP:!CBC:!EDH:!kEDH:!PSK:!SRP:!kECDH;
ssl_prefer_server_ciphers on;
fastcgi_param HTTPS on;
fastcgi_param HTTP_SCHEME https;



</code></pre>

<h4 id="2-http2配置">2、http2配置</h4>

<blockquote>
<p>配置http2，需要编译安装Nginx加上对应的支持模块</p>
</blockquote>

<pre><code>    listen             443 ssl http2;
    add_header Strict-Transport-Security max-age=15768000;
    add_header X-Content-Type-Options nosniff;

</code></pre>

<h3 id="proxy部分">Proxy部分</h3>

<blockquote>
<p>在location / {} 中配置以下代码
需要在服务器端获取用户IP时，可使用：X-Real-IP 或 REMOTE-HOST 或 X-Forwarded-For</p>
</blockquote>

<pre><code>proxy_pass http://127.0.0.1:1235
proxy_read_timeout 3600s;
proxy_send_timeout 12s;
proxy_set_header X-Real-IP $remote_addr
proxy_set_header REMOTE-HOST $remote_addr;
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
proxy_set_header Host $host;

</code></pre>

<h3 id="websocket">WebSocket</h3>

<blockquote>
<p>添加以下header实现WebSocket代理</p>
</blockquote>

<pre><code>proxy_http_version 1.1;
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection &quot;upgrade&quot;;

</code></pre>

<h3 id="301跳转">301跳转</h3>

<blockquote>
<p>如：<a href="http://www.niu.cn">http://www.niu.cn</a> 需要跳转到 <a href="https://www.niu.cn，可使用以下跳转">https://www.niu.cn，可使用以下跳转</a></p>
</blockquote>

<pre><code>server{
    server_name www.niu.cn;
    listen 80;
    add_header Strict-Transport-Security max-age=15768000;
    return 301 https://www.niu.cn$request_uri;
}
</code></pre>

<h3 id="禁止直接使用ip访问">禁止直接使用IP访问</h3>

<blockquote>
<p>如使用http(https)://114.215.65.149 访问到了虚拟主机，可使用以下配置</p>
</blockquote>

<pre><code>// http
server {
        listen 80 default_server;
        server_name _;
        return 403;
}

// https
server {
        listen 443 default_server;
        server_name _;
        ssl on;

        ssl_certificate  /etc/nginx/conf.d/cert/niu.pem;
        ssl_certificate_key /etc/nginx/conf.d/cert/niu.key;

        return 403;
}


</code></pre>
]]></content>
		</item>
		
		<item>
			<title>不同场景,两种不同的限流手法</title>
			<link>https://yulibaozi.com/posts/design/knowledge/2018-01-24-limiting/</link>
			<pubDate>Wed, 24 Jan 2018 20:08:01 +0800</pubDate>
			
			<guid>https://yulibaozi.com/posts/design/knowledge/2018-01-24-limiting/</guid>
			<description> 限流之洪峰限流 缘起:解决实际流量大于系统承载能力的问题，有三大关键因因子：允许访问的频率,爆发量和爆发周期。 爆发周期指的是:此次爆发到下次爆发的时间间隔 - 令牌桶算法
1、每秒会有r个令牌放入桶中，或者说没过1/r秒桶中增加一个令牌。 2、桶中最多存放b个令牌，如果桶满了，新放入的令牌会被丢弃或者自己不放。 3、如果一个n字节的数据包达到时，消耗n个令牌，请求通过。 4、如果桶中可用令牌小于n，则请求被拒绝。 how？如何让令匀速发放呢? 1、例如Guava,让每个请求到来的时候带上一个时间戳,然后比较下一个请求是否在1/r之内到达。如果是，不允许通过，反之通过。 2、使用积分中的&amp;quot;极限&amp;quot;的思想，把1秒分成更细的粒度，让这个时间粒度允许通过固定的数据， 从而让令牌更均匀地落在1秒这个时间单位。粒度越小，那么越均匀，but这也越费计算能力。so，我们需要做一个平衡。  场景：我们需求是系统的通过率是1000QPS,且假设我们的桶大小是300,并把1秒切分为10个格子，由计算得每个格子是100ms,每个格子发放1000/10个令牌，假设在双11 0点之前放满了桶，且0点到0点0分1秒,每秒会超过10000，最后限流的结果是一条平滑下降的曲线，当下降到100之后就是一条水平的曲线。
回调限流 场景：例如 物流系统需要接收交易成功的消息，回调交易系统的订单消息，这样才能处理订单并发货，特点是，请求是系统主动发起，调用量级波动大，会出现时间堆积，且能容忍小段时间的时间延迟，由于时间的堆积，不错任何限制的话，系统A向系统B有可能在短时间内将所有堆积的请求一次性发出去，这样会对系统B造成非常大的压力，所以回调限流专门为解决问题而设计。 - 漏桶算法
请求（水）先进入桶中，漏桶以一定的速度出水，当水流速度过大时会直接溢出，通过这种方式来调节请求的处理速度。通过这种方法，来避免这种回调的请求抢占珍贵的系统资源，从而又保证这些请求能在预期的时间被执行。 Q:溢出的请求如何处理？ 重新放入？还是抛弃？  </description>
			<content type="html"><![CDATA[

<h3 id="限流之洪峰限流">限流之洪峰限流</h3>

<p>缘起:解决实际流量大于系统承载能力的问题，有三大关键因因子：允许访问的频率,爆发量和爆发周期。
爆发周期指的是:此次爆发到下次爆发的时间间隔
- 令牌桶算法</p>

<pre><code>1、每秒会有r个令牌放入桶中，或者说没过1/r秒桶中增加一个令牌。
2、桶中最多存放b个令牌，如果桶满了，新放入的令牌会被丢弃或者自己不放。
3、如果一个n字节的数据包达到时，消耗n个令牌，请求通过。
4、如果桶中可用令牌小于n，则请求被拒绝。

how？如何让令匀速发放呢?
1、例如Guava,让每个请求到来的时候带上一个时间戳,然后比较下一个请求是否在1/r之内到达。如果是，不允许通过，反之通过。
2、使用积分中的&quot;极限&quot;的思想，把1秒分成更细的粒度，让这个时间粒度允许通过固定的数据， 从而让令牌更均匀地落在1秒这个时间单位。粒度越小，那么越均匀，but这也越费计算能力。so，我们需要做一个平衡。

</code></pre>

<p>场景：我们需求是系统的通过率是1000QPS,且假设我们的桶大小是300,并把1秒切分为10个格子，由计算得每个格子是100ms,每个格子发放1000/10个令牌，假设在双11 0点之前放满了桶，且0点到0点0分1秒,每秒会超过10000，最后限流的结果是一条平滑下降的曲线，当下降到100之后就是一条水平的曲线。</p>

<h3 id="回调限流">回调限流</h3>

<p>场景：例如 物流系统需要接收交易成功的消息，回调交易系统的订单消息，这样才能处理订单并发货，特点是，请求是系统主动发起，调用量级波动大，会出现时间堆积，且能容忍小段时间的时间延迟，由于时间的堆积，不错任何限制的话，系统A向系统B有可能在短时间内将所有堆积的请求一次性发出去，这样会对系统B造成非常大的压力，所以回调限流专门为解决问题而设计。
- 漏桶算法</p>

<pre><code>请求（水）先进入桶中，漏桶以一定的速度出水，当水流速度过大时会直接溢出，通过这种方式来调节请求的处理速度。通过这种方法，来避免这种回调的请求抢占珍贵的系统资源，从而又保证这些请求能在预期的时间被执行。
Q:溢出的请求如何处理？
重新放入？还是抛弃？
</code></pre>
]]></content>
		</item>
		
	</channel>
</rss>

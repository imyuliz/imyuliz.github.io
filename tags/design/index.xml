<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>design on yulibaozi</title>
    <link>https://yulibaozi.com/tags/design/</link>
    <description>Recent content in design on yulibaozi</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-hans</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Fri, 09 Mar 2018 22:35:52 +0800</lastBuildDate>
    
	<atom:link href="https://yulibaozi.com/tags/design/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>微服务设计与实践的经验总结</title>
      <link>https://yulibaozi.com/posts/design/knowledge/2018-03-09-microservices/</link>
      <pubDate>Fri, 09 Mar 2018 22:35:52 +0800</pubDate>
      
      <guid>https://yulibaozi.com/posts/design/knowledge/2018-03-09-microservices/</guid>
      <description>对于项目中微服务的拆分方案:
自家项目中拆分的维度或者原则有哪些？ 在我们的项目中,拆分选取了多个维度。 1. 按照功能拆分 2. 按照业务拆分 3. 按照未来扩展度拆分 4. 按照平台拆分 5. 按照数据拆分。
我们在拆分设计过程中,其实还隐含了一个拆分原则:重要度拆分,并不是按照单一的拆分原则来拆分。而是多种原则并用,才会确定如何拆分。
具体的拆分服务以及为什么这样拆分? 我们拆分的部分服务有:
用户服务，计算服务,评论服务,用户服务,社区服务,微信小程序等。  大体示意图: 1. 主要按照功能拆分的实践&amp;lt;计算服务和评论服务&amp;gt;:
由于计算服务是一个底层服务,大多数业务(纯粹的业务,和不纯粹的业务(还包括了用户))都需要使用它,所以他的定位是:核心的,基本的,纯粹的,所以这个服务应该是按照功能拆分,能获取单一的特定的数据即可。 评论服务的拆分原则:评论系统的定位比较低,在数据上肯定是有帖子/主题才会有评论,而且有些游离于主要服务之外。这类服务主要是发评论和展示，对扩展而言,多个依附关系决定了他的扩张度不高,因为简单唯一,所以按照功能才分。   主要按照扩展度拆分的实践&amp;lt;用户服务&amp;gt;:
用户服务处于一个比较核心的地位,而且考虑到以后的项目开发能重用这一套用户系统，所以主要按照扩展度拆分，提供的方法不宜太多,提供登录注册，获取用户信息等方法就够了,主要是面向扩展的。这点是我们之前没有想到的。
 主要按照业务拆分的实践&amp;lt;社区服务&amp;gt;:
社区服务时一个大的需求服务，这个服务具有一定的时效性，因为需求存在时效性性,在未来看某些需求,可能就不那么合理,也是需求改动的重灾区,所以这个服务无论是代码量还是设计范围还是需求改动都比较大,而且可能存在被淘汰的可能,所以,我们就不对它再次拆分,而是注意这个服务的内部设计可以参考Go项目结构设计与实践(多存储下),尽量做到可变,灵活变即可。
 主要按照平台拆分的实践&amp;lt;小程序服务&amp;gt;:
小程序服务是为了照顾微信用户(微信的用户群体大),因为它的需求极有可能和PC,移动站的需求不太一样,这个和潮流和也公司的用户群体有关,公司的用户群体主要在那方面,可能就会侧重那方面,涉及到改动的侧重也就不一样,所以,按照平台拆分,可能是不二选择。
  实践中的好处与不足: 优点: 在实践中,我们享受到微服务设计的种种好处,比如说 1. 独立开发,熟悉开发
从效率的角度来说,当拆分和任务分配后,每个人独立完成某个模块,这样没有交叉,用自己熟悉的方式写自己的code,效率很灵活度都得到很大提升。   范围可控
从更改需求的角度来说,当更改需求时,只需要在其中一个微服务下修改代码,然后发布,这样涉及到的面很小,同时,如果改动出现了意外bug,受灾波及的范围也很小,能够把服务受灾粒度降到最低。
 通用约定，简单沟通
通用约定更少,除非是必要的通用约定(比如:Redis的Key,数据库状态值代表的意思,接口通用数据结构),在程序内部,我们可以按照我们自己的意愿在一个文件中统一声明,这样减少了沟通的成本。
 扩展容易，小巧轻便
当多个服务必须依赖某个服务时,需要扩展时,只需要对这个小的服务做负载均衡,这样服务的体积更小,更轻量。
 积木搭建,灵活多变。
由于是积木式搭建,灵活是微服务的核心，灵活多变是微服务的特质。在我们开发历程中,经历了无数次的需求改变或扩展,虽然有些时候精疲力竭,但是对于设计来说,改变却是轻而易举。虽然你可能庆幸于灵活多变，但是也要注意这次改变是否违背之前的约定,造成不必要的麻烦。
 通用的模型定义和约定,在设计之初就要想好模型定义的位置,多个服务之间公用同一模型,改动时,只需要一个地方改动,不存在双标(当然传统的一体化设计也不存在这种情况。)
  一些不足: 当然,好处多,但是我们使用过程中,也觉得有些不足: 1. 可能存在部分重复代码:
在多个微服务中,存在一定量的重复代码,当然,你可能会说,为什么不去调另外一个服务的接口,这样是能做到,同时没有重复代码,但是存在的问题是:平级服务之间相互依赖,这可不是一件好事,违背了微服务尽量独立的一贯原则,其二是:如果被依赖服务挂掉,那么受灾范围将会变大,这又违背了我们使用微服务的一个初衷。   可能存在通信问题
当的的确确存在上层服务依赖下层服务时,通讯和性能便成了一个主要问题,在我们项目中设计到一个期数计算服务,这是一个基础服务,上层服务需要调用,所有调用这个基础服务的接口都变得很慢,当时使用的GRPC,但是性能也不尽如人意或者说很不理想,性能波动很大,2倍,3倍,即使不波动,也会多出5ms+以上,最初查问题,根本没想到这个点上来,还是我们的C#在使用接口时,莫名的一句话,好像使用期数的接口都比较慢,突然,茅塞顿开,测试之后果然是这样,想了多种解决策略后,后面决定使用Redis把数据存起来(因为这个数据是通用的),用GRPC的数据保证正确性,用REDIS的数据保证效率,唯一难点就在时效性,因为行业的原因过了时间某个开奖时间点,数据就意味着错误,还好当初在设计接口时,数据返回得足够完整,测试的时候,还有些紧张,如果这种方案不能解决,我又将深受打击,而且已经困扰多时了, 打开浏览器,输入地址,回车。 这一刻,多天的抑郁一扫而空,性能提升10倍不止,比我预期的还要少的多, 这一刻我觉得我是个工程师的料，高兴过后,我决定优化这段代码,因为业务的关系,我优化的这小段代码满足90%的需求，变成了通用的获取期数代码,甚至可以说是公司最小巧实用而又有优雅的代码,比我当初写期数计算服务的成就还来的高。不仅减小了期数计算服务的压力,还获得了巨大的性能提升。</description>
    </item>
    
    <item>
      <title>网站从单机到分布式的变迁</title>
      <link>https://yulibaozi.com/posts/design/knowledge/2018-03-09-single-to-many/</link>
      <pubDate>Sat, 24 Feb 2018 22:37:47 +0800</pubDate>
      
      <guid>https://yulibaozi.com/posts/design/knowledge/2018-03-09-single-to-many/</guid>
      <description>变迁的过程分8步:
 单机 单机负载警告，数据与应用分离 应用服务器警告，应用服务器集群 数据库压力过大，读写分离 数据库再遇瓶颈，数据库垂直拆分à 单机数据库又遇瓶颈，水平拆分 数据库解决问后，应用面临新挑战，应用拆分 服务化结构  各个阶段遇到的问题和解决办法 1、 单机 只是在应用中把数据库的地址从本地改到另外一台机器上。
2、 单机负载警告，数据与应用分离 应用服务器之间没有交互，他们都是依赖数据库提供服务的。
3、 应用服务器警告，应用服务器集群
 3.1、出现的问题和解决办法
  用户对应用服务器访问的选择问题
问题描述：当用户访问的时候，应该选择那一台应用服务器响应用户请求。 解决办法： 1.1、通过DNS解决。 通过DNS服务器进行调度和控制，用户解析DNS的时候，就给用户一个应用服务器地址。 1.2、通过在应用服务器前增加负载均衡设备解决。 所有用户请求访问的时候都经过负载均衡设备来完成请求转发控制。  Session问题：
问题描述：当Http请求web服务器的时候，需要在http请求中找到会话数据（Session）,问题在于会话数据保存在单机上的，我第一次访问落在左边服务器（假设服务器有两台），这时候我的Session就创建在左边服务器，右边服务器就没有，我如何保证我每次访问都在同一台服务器或者说都能够拿到Session呢？ 解决办法： 2.1、添加负载均衡器，保证同样的Session请求都发送到同一个服务端处理。 优点:有利于针对Session进行服务器本地缓存。 缺点：如果一台Web服务器重启，这台机器的会话数据会丢失； 2.2、不要求负载均衡器保证同样的Session请求发送到同一服务端，而是在服务端进行Session数据同步。 缺点：同步Session数据会带来带宽开销，只要Session有变化就需要同步；每台Web服务器都需要保存所有的Session数据，性能下降厉害。 2.3、把Session数据集中处理，Web服务器需要使用Session数据时，去集中存储的地方读取就好。 缺点：读写Session数据引入了网络操作，存在时延和不稳定性；如果集中存储Session数据的地方出问题，影响巨大，但是对于Session数据很多的时候，优势巨大。 2.4、把Session数据放在Cookie中，每次Web服务器就从Cookie中生成对应的Session数据。 优点：同一会话的不同请求不限制具体处理机器。不依赖外部存储系统，不存在时延等问题。 缺点：Cookie长度限制，这会限制Session的长度；安全性，Session数据本来是服务端数据；性能影响，每次Http请求响应都带有Session数据。  数据库压力过大，读写分离
读写分离解决的是写数据读的压力。 带来的问题和解决办法： 数据复制问题 Master+Slave结构可以提供数据复制机制。（一主多从） 当主/从非对称且数据结构相同，多从对应一主的场景数据复制问题 解决办法： 应用通过数据访问层访问数据库，通过消息系统就数据库的通知发出消息通知，数据库同步服务器得到消息后进行数据复制，分库规则配置则复制在读数据以及数据同步服务器更新分库时让数据层知道分库规则，其中，数据同步服务器和DB主库得到交互主要是根据被修改或新增的数据主键来获取内容，他们采用的是行复制的方式。 第二钟基于数据库日志来进行数据的复制。 当主/从非对称且主/备分库方式不同时数据复制问题 非对称指的是：源数据和目标数据不是镜像关系。 应用数据源选择问题。 数据缓存:我们可以用Key-value进行缓存（例如：Redis），在缓存中，一般存放的是热数据而不是全部数据。应用先访问缓存，如果访问不存在再去访问数据库。 页面缓存。我们可以把静态页面，热页面，Js等进行缓存，例如CDN。  数据库再遇瓶颈，数据库垂直拆分
垂直拆分是指：专库专用，把数据库中不同的业务数据拆分到不同的数据库中。 遇到的问题: 应用需要配置多个数据源。 单机的ACID（原子性，一致性，隔离性，持久性）被打破。原单机事务控制的逻辑被打破。（见下） 靠外键进行约束的场景会受到影响。 Jion操作变得困难，因为数据可能已经在两个数据库中。（见下）  单机数据库又遇瓶颈，水平拆分</description>
    </item>
    
    <item>
      <title>不同场景,两种不同的限流手法</title>
      <link>https://yulibaozi.com/posts/design/knowledge/2018-01-24-limiting/</link>
      <pubDate>Wed, 24 Jan 2018 20:08:01 +0800</pubDate>
      
      <guid>https://yulibaozi.com/posts/design/knowledge/2018-01-24-limiting/</guid>
      <description> 限流之洪峰限流 缘起:解决实际流量大于系统承载能力的问题，有三大关键因因子：允许访问的频率,爆发量和爆发周期。 爆发周期指的是:此次爆发到下次爆发的时间间隔 - 令牌桶算法
1、每秒会有r个令牌放入桶中，或者说没过1/r秒桶中增加一个令牌。 2、桶中最多存放b个令牌，如果桶满了，新放入的令牌会被丢弃或者自己不放。 3、如果一个n字节的数据包达到时，消耗n个令牌，请求通过。 4、如果桶中可用令牌小于n，则请求被拒绝。 how？如何让令匀速发放呢? 1、例如Guava,让每个请求到来的时候带上一个时间戳,然后比较下一个请求是否在1/r之内到达。如果是，不允许通过，反之通过。 2、使用积分中的&amp;quot;极限&amp;quot;的思想，把1秒分成更细的粒度，让这个时间粒度允许通过固定的数据， 从而让令牌更均匀地落在1秒这个时间单位。粒度越小，那么越均匀，but这也越费计算能力。so，我们需要做一个平衡。  场景：我们需求是系统的通过率是1000QPS,且假设我们的桶大小是300,并把1秒切分为10个格子，由计算得每个格子是100ms,每个格子发放1000/10个令牌，假设在双11 0点之前放满了桶，且0点到0点0分1秒,每秒会超过10000，最后限流的结果是一条平滑下降的曲线，当下降到100之后就是一条水平的曲线。
回调限流 场景：例如 物流系统需要接收交易成功的消息，回调交易系统的订单消息，这样才能处理订单并发货，特点是，请求是系统主动发起，调用量级波动大，会出现时间堆积，且能容忍小段时间的时间延迟，由于时间的堆积，不错任何限制的话，系统A向系统B有可能在短时间内将所有堆积的请求一次性发出去，这样会对系统B造成非常大的压力，所以回调限流专门为解决问题而设计。 - 漏桶算法
请求（水）先进入桶中，漏桶以一定的速度出水，当水流速度过大时会直接溢出，通过这种方式来调节请求的处理速度。通过这种方法，来避免这种回调的请求抢占珍贵的系统资源，从而又保证这些请求能在预期的时间被执行。 Q:溢出的请求如何处理？ 重新放入？还是抛弃？  </description>
    </item>
    
  </channel>
</rss>